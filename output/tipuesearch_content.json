{"pages":[{"url":"http://igormarfin.github.io/blog/2016/04/15/application-of-the-monte-carlo-integration-in-options-trading-(part-i)/","text":"/*! * * IPython notebook * */.ansibold{font-weight:bold}.ansiblack{color:black}.ansired{color:darkred}.ansigreen{color:darkgreen}.ansiyellow{color:#c4a000}.ansiblue{color:darkblue}.ansipurple{color:darkviolet}.ansicyan{color:steelblue}.ansigray{color:gray}.ansibgblack{background-color:black}.ansibgred{background-color:red}.ansibggreen{background-color:green}.ansibgyellow{background-color:yellow}.ansibgblue{background-color:blue}.ansibgpurple{background-color:magenta}.ansibgcyan{background-color:cyan}.ansibggray{background-color:gray}div.cell{border:1px solid transparent;display:-webkit-box;-webkit-box-orient:vertical;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:vertical;-moz-box-align:stretch;display:box;box-orient:vertical;box-align:stretch;display:flex;flex-direction:column;align-items:stretch;border-radius:2px;box-sizing:border-box;-moz-box-sizing:border-box;-webkit-box-sizing:border-box;border-width:thin;border-style:solid;width:100%;padding:5px;margin:0;outline:none}div.cell.selected{border-color:#ababab}@media print{div.cell.selected{border-color:transparent}}.edit_mode div.cell.selected{border-color:green}@media print{.edit_mode div.cell.selected{border-color:transparent}}.prompt{min-width:14ex;padding:.4em;margin:0;font-family:monospace;text-align:right;line-height:1.21429em}@media (max-width:540px){.prompt{text-align:left}}div.inner_cell{display:-webkit-box;-webkit-box-orient:vertical;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:vertical;-moz-box-align:stretch;display:box;box-orient:vertical;box-align:stretch;display:flex;flex-direction:column;align-items:stretch;-webkit-box-flex:1;-moz-box-flex:1;box-flex:1;flex:1}@-moz-document url-prefix(){div.inner_cell{overflow-x:hidden}}div.input_area{border:1px solid #cfcfcf;border-radius:2px;background:#f7f7f7;line-height:1.21429em}div.prompt:empty{padding-top:0;padding-bottom:0}div.unrecognized_cell{padding:5px 5px 5px 0;display:-webkit-box;-webkit-box-orient:horizontal;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:horizontal;-moz-box-align:stretch;display:box;box-orient:horizontal;box-align:stretch;display:flex;flex-direction:row;align-items:stretch}div.unrecognized_cell .inner_cell{border-radius:2px;padding:5px;font-weight:bold;color:red;border:1px solid #cfcfcf;background:#eaeaea}div.unrecognized_cell .inner_cell a{color:inherit;text-decoration:none}div.unrecognized_cell .inner_cell a:hover{color:inherit;text-decoration:none}@media (max-width:540px){div.unrecognized_cell>div.prompt{display:none}}@media print{div.code_cell{page-break-inside:avoid}}div.input{page-break-inside:avoid;display:-webkit-box;-webkit-box-orient:horizontal;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:horizontal;-moz-box-align:stretch;display:box;box-orient:horizontal;box-align:stretch;display:flex;flex-direction:row;align-items:stretch}@media (max-width:540px){div.input{display:-webkit-box;-webkit-box-orient:vertical;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:vertical;-moz-box-align:stretch;display:box;box-orient:vertical;box-align:stretch;display:flex;flex-direction:column;align-items:stretch}}div.input_prompt{color:navy;border-top:1px solid transparent}div.input_area>div.highlight{margin:.4em;border:none;padding:0;background-color:transparent}div.input_area>div.highlight>pre{margin:0;border:none;padding:0;background-color:transparent}.CodeMirror{line-height:1.21429em;font-size:14px;height:auto;background:none}.CodeMirror-scroll{overflow-y:hidden;overflow-x:auto}.CodeMirror-lines{padding:.4em}.CodeMirror-linenumber{padding:0 8px 0 4px}.CodeMirror-gutters{border-bottom-left-radius:2px;border-top-left-radius:2px}.CodeMirror pre{padding:0;border:0;border-radius:0}.highlight-base{}.highlight-variable{}.highlight-variable-2{color:#1a1a1a}.highlight-variable-3{color:#333}.highlight-string{color:#ba2121}.highlight-comment{color:#408080;font-style:italic}.highlight-number{80}.highlight-atom{color:#88f}.highlight-keyword{8000;font-weight:bold}.highlight-builtin{8000}.highlight-error{color:#f00}.highlight-operator{color:#a2f;font-weight:bold}.highlight-meta{color:#a2f}.highlight-def{f}.highlight-string-2{color:#f50}.highlight-qualifier{color:#555}.highlight-bracket{color:#997}.highlight-tag{color:#170}.highlight-attribute{c}.highlight-header{color:blue}.highlight-quote{90}.highlight-link{c}.cm-s-ipython span.cm-keyword{8000;font-weight:bold}.cm-s-ipython span.cm-atom{color:#88f}.cm-s-ipython span.cm-number{80}.cm-s-ipython span.cm-def{f}.cm-s-ipython span.cm-variable{}.cm-s-ipython span.cm-operator{color:#a2f;font-weight:bold}.cm-s-ipython span.cm-variable-2{color:#1a1a1a}.cm-s-ipython span.cm-variable-3{color:#333}.cm-s-ipython span.cm-comment{color:#408080;font-style:italic}.cm-s-ipython span.cm-string{color:#ba2121}.cm-s-ipython span.cm-string-2{color:#f50}.cm-s-ipython span.cm-meta{color:#a2f}.cm-s-ipython span.cm-qualifier{color:#555}.cm-s-ipython span.cm-builtin{8000}.cm-s-ipython span.cm-bracket{color:#997}.cm-s-ipython span.cm-tag{color:#170}.cm-s-ipython span.cm-attribute{c}.cm-s-ipython span.cm-header{color:blue}.cm-s-ipython span.cm-quote{90}.cm-s-ipython span.cm-link{c}.cm-s-ipython span.cm-error{color:#f00}.cm-s-ipython span.cm-tab{background:url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADAAAAAMCAYAAAAkuj5RAAAAAXNSR0IArs4c6QAAAGFJREFUSMft1LsRQFAQheHPowAKoACx3IgEKtaEHujDjORSgWTH/ZOdnZOcM/sgk/kFFWY0qV8foQwS4MKBCS3qR6ixBJvElOobYAtivseIE120FaowJPN75GMu8j/LfMwNjh4HUpwg4LUAAAAASUVORK5CYII=);background-position:right;background-repeat:no-repeat}div.output_wrapper{position:relative;display:-webkit-box;-webkit-box-orient:vertical;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:vertical;-moz-box-align:stretch;display:box;box-orient:vertical;box-align:stretch;display:flex;flex-direction:column;align-items:stretch}div.output_scroll{height:24em;width:100%;overflow:auto;border-radius:2px;-webkit-box-shadow:inset 0 2px 8px rgba(0,0,0,0.8);box-shadow:inset 0 2px 8px rgba(0,0,0,0.8);display:block}div.output_collapsed{margin:0;padding:0;display:-webkit-box;-webkit-box-orient:vertical;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:vertical;-moz-box-align:stretch;display:box;box-orient:vertical;box-align:stretch;display:flex;flex-direction:column;align-items:stretch}div.out_prompt_overlay{height:100%;padding:0 .4em;position:absolute;border-radius:2px}div.out_prompt_overlay:hover{-webkit-box-shadow:inset 0 0 1px #000;box-shadow:inset 0 0 1px #000;background:rgba(240,240,240,0.5)}div.output_prompt{color:darkred}div.output_area{padding:0;page-break-inside:avoid;display:-webkit-box;-webkit-box-orient:horizontal;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:horizontal;-moz-box-align:stretch;display:box;box-orient:horizontal;box-align:stretch;display:flex;flex-direction:row;align-items:stretch}div.output_area .MathJax_Display{text-align:left !important}div.output_area div.output_area .output{display:-webkit-box;-webkit-box-orient:vertical;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:vertical;-moz-box-align:stretch;display:box;box-orient:vertical;box-align:stretch;display:flex;flex-direction:column;align-items:stretch}@media (max-width:540px){div.output_area{display:-webkit-box;-webkit-box-orient:vertical;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:vertical;-moz-box-align:stretch;display:box;box-orient:vertical;box-align:stretch;display:flex;flex-direction:column;align-items:stretch}}div.output_area pre{margin:0;padding:0;border:0;vertical-align:baseline;color:black;background-color:transparent;border-radius:0}div.output_subarea{padding:.4em;-webkit-box-flex:1;-moz-box-flex:1;box-flex:1;flex:1}div.output_text{text-align:left;line-height:1.21429em}div.output_stderr{background:#fdd}div.output_latex{text-align:left}div.output_javascript:empty{padding:0}.js-error{color:darkred}div.raw_input_container{font-family:monospace;padding-top:5px}input.raw_input{font-family:inherit;font-size:inherit;color:inherit;width:auto;vertical-align:baseline;padding:0 .25em;margin:0 .25em}input.raw_input:focus{box-shadow:none}p.p-space{margin-bottom:10px}div.output_unrecognized{padding:5px;font-weight:bold;color:red}div.output_unrecognized a{color:inherit;text-decoration:none}div.output_unrecognized a:hover{color:inherit;text-decoration:none}.rendered_html{}.rendered_html :link{text-decoration:underline}.rendered_html :visited{text-decoration:underline}.rendered_html h1:first-child{margin-top:.538em}.rendered_html h2:first-child{margin-top:.636em}.rendered_html h3:first-child{margin-top:.777em}.rendered_html h4:first-child{margin-top:1em}.rendered_html h5:first-child{margin-top:1em}.rendered_html h6:first-child{margin-top:1em}.rendered_html *+ul{margin-top:1em}.rendered_html *+ol{margin-top:1em}.rendered_html *+table{margin-top:1em}.rendered_html *+p{margin-top:1em}.rendered_html *+img{margin-top:1em}div.text_cell{display:-webkit-box;-webkit-box-orient:horizontal;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:horizontal;-moz-box-align:stretch;display:box;box-orient:horizontal;box-align:stretch;display:flex;flex-direction:row;align-items:stretch}@media (max-width:540px){div.text_cell>div.prompt{display:none}}div.text_cell_render{outline:none;resize:none;width:inherit;border-style:none;padding:.5em .5em .5em .4em;box-sizing:border-box;-moz-box-sizing:border-box;-webkit-box-sizing:border-box}a.anchor-link:link{text-decoration:none;padding:0 20px;visibility:hidden}h1:hover .anchor-link,h2:hover .anchor-link,h3:hover .anchor-link,h4:hover .anchor-link,h5:hover .anchor-link,h6:hover .anchor-link{visibility:visible}.text_cell.rendered .input_area{display:none}.text_cell.unrendered .text_cell_render{display:none}.cm-header-1,.cm-header-2,.cm-header-3,.cm-header-4,.cm-header-5,.cm-header-6{font-weight:bold;font-family:\"Helvetica Neue\",Helvetica,Arial,sans-serif}.cm-header-1{font-size:185.7%}.cm-header-2{font-size:157.1%}.cm-header-3{font-size:128.6%}.cm-header-4{font-size:110%}.cm-header-5{font-size:100%;font-style:italic}.cm-header-6{font-size:100%;font-style:italic}.widget-interact>div,.widget-interact>input{padding:2.5px}.widget-area{page-break-inside:avoid;display:-webkit-box;-webkit-box-orient:horizontal;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:horizontal;-moz-box-align:stretch;display:box;box-orient:horizontal;box-align:stretch;display:flex;flex-direction:row;align-items:stretch}.widget-area .widget-subarea{padding:.44em .4em .4em 1px;margin-left:6px;box-sizing:border-box;-moz-box-sizing:border-box;-webkit-box-sizing:border-box;display:-webkit-box;-webkit-box-orient:vertical;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:vertical;-moz-box-align:stretch;display:box;box-orient:vertical;box-align:stretch;display:flex;flex-direction:column;align-items:stretch;-webkit-box-flex:2;-moz-box-flex:2;box-flex:2;flex:2;-webkit-box-align:start;-moz-box-align:start;box-align:start;align-items:flex-start}.widget-area.connection-problems .prompt:after{content:\"\\f127\";font-family:'FontAwesome';color:#d9534f;font-size:14px;top:3px;padding:3px}.slide-track{border:1px solid #ccc;background:#fff;border-radius:2px}.widget-hslider{padding-left:8px;padding-right:2px;overflow:visible;width:350px;height:5px;max-height:5px;margin-top:13px;margin-bottom:10px;border:1px solid #ccc;background:#fff;border-radius:2px;display:-webkit-box;-webkit-box-orient:horizontal;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:horizontal;-moz-box-align:stretch;display:box;box-orient:horizontal;box-align:stretch;display:flex;flex-direction:row;align-items:stretch}.widget-hslider .ui-slider{border:0;background:none;display:-webkit-box;-webkit-box-orient:horizontal;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:horizontal;-moz-box-align:stretch;display:box;box-orient:horizontal;box-align:stretch;display:flex;flex-direction:row;align-items:stretch;-webkit-box-flex:1;-moz-box-flex:1;box-flex:1;flex:1}.widget-hslider .ui-slider .ui-slider-handle{width:12px;height:28px;margin-top:-8px;border-radius:2px}.widget-hslider .ui-slider .ui-slider-range{height:12px;margin-top:-4px;background:#eee}.widget-vslider{padding-bottom:5px;overflow:visible;width:5px;max-width:5px;height:250px;margin-left:12px;border:1px solid #ccc;background:#fff;border-radius:2px;display:-webkit-box;-webkit-box-orient:vertical;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:vertical;-moz-box-align:stretch;display:box;box-orient:vertical;box-align:stretch;display:flex;flex-direction:column;align-items:stretch}.widget-vslider .ui-slider{border:0;background:none;margin-left:-4px;margin-top:5px;display:-webkit-box;-webkit-box-orient:vertical;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:vertical;-moz-box-align:stretch;display:box;box-orient:vertical;box-align:stretch;display:flex;flex-direction:column;align-items:stretch;-webkit-box-flex:1;-moz-box-flex:1;box-flex:1;flex:1}.widget-vslider .ui-slider .ui-slider-handle{width:28px;height:12px;margin-left:-9px;border-radius:2px}.widget-vslider .ui-slider .ui-slider-range{width:12px;margin-left:-1px;background:#eee}.widget-text{width:350px;margin:0}.widget-listbox{width:350px;margin-bottom:0}.widget-numeric-text{width:150px;margin:0}.widget-progress{margin-top:6px;min-width:350px}.widget-progress .progress-bar{-webkit-transition:none;-moz-transition:none;-ms-transition:none;-o-transition:none;transition:none}.widget-combo-btn{min-width:125px}.widget_item .dropdown-menu li a{color:inherit}.widget-hbox{display:-webkit-box;-webkit-box-orient:horizontal;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:horizontal;-moz-box-align:stretch;display:box;box-orient:horizontal;box-align:stretch;display:flex;flex-direction:row;align-items:stretch}.widget-hbox input[type=\"checkbox\"]{margin-top:9px;margin-bottom:10px}.widget-hbox .widget-label{min-width:10ex;padding-right:8px;padding-top:5px;text-align:right;vertical-align:text-top}.widget-hbox .widget-readout{padding-left:8px;padding-top:5px;text-align:left;vertical-align:text-top}.widget-vbox{display:-webkit-box;-webkit-box-orient:vertical;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:vertical;-moz-box-align:stretch;display:box;box-orient:vertical;box-align:stretch;display:flex;flex-direction:column;align-items:stretch}.widget-vbox .widget-label{padding-bottom:5px;text-align:center;vertical-align:text-bottom}.widget-vbox .widget-readout{padding-top:5px;text-align:center;vertical-align:text-top}.widget-box{box-sizing:border-box;-moz-box-sizing:border-box;-webkit-box-sizing:border-box;-webkit-box-align:start;-moz-box-align:start;box-align:start;align-items:flex-start}.widget-radio-box{display:-webkit-box;-webkit-box-orient:vertical;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:vertical;-moz-box-align:stretch;display:box;box-orient:vertical;box-align:stretch;display:flex;flex-direction:column;align-items:stretch;box-sizing:border-box;-moz-box-sizing:border-box;-webkit-box-sizing:border-box;padding-top:4px}.widget-radio-box label{margin-top:0}.widget-radio{margin-left:20px} .highlight .hll { background-color: #ffffcc } .highlight { background: #f8f8f8; } .highlight .c { color: #408080; font-style: italic } /* Comment */ .highlight .err { border: 1px solid #FF0000 } /* Error */ .highlight .k { color: #008000; font-weight: bold } /* Keyword */ .highlight .o { color: #666666 } /* Operator */ .highlight .cm { color: #408080; font-style: italic } /* Comment.Multiline */ .highlight .cp { color: #BC7A00 } /* Comment.Preproc */ .highlight .c1 { color: #408080; font-style: italic } /* Comment.Single */ .highlight .cs { color: #408080; font-style: italic } /* Comment.Special */ .highlight .gd { color: #A00000 } /* Generic.Deleted */ .highlight .ge { font-style: italic } /* Generic.Emph */ .highlight .gr { color: #FF0000 } /* Generic.Error */ .highlight .gh { color: #000080; font-weight: bold } /* Generic.Heading */ .highlight .gi { color: #00A000 } /* Generic.Inserted */ .highlight .go { color: #888888 } /* Generic.Output */ .highlight .gp { color: #000080; font-weight: bold } /* Generic.Prompt */ .highlight .gs { font-weight: bold } /* Generic.Strong */ .highlight .gu { color: #800080; font-weight: bold } /* Generic.Subheading */ .highlight .gt { color: #0044DD } /* Generic.Traceback */ .highlight .kc { color: #008000; font-weight: bold } /* Keyword.Constant */ .highlight .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */ .highlight .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */ .highlight .kp { color: #008000 } /* Keyword.Pseudo */ .highlight .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */ .highlight .kt { color: #B00040 } /* Keyword.Type */ .highlight .m { color: #666666 } /* Literal.Number */ .highlight .s { color: #BA2121 } /* Literal.String */ .highlight .na { color: #7D9029 } /* Name.Attribute */ .highlight .nb { color: #008000 } /* Name.Builtin */ .highlight .nc { color: #0000FF; font-weight: bold } /* Name.Class */ .highlight .no { color: #880000 } /* Name.Constant */ .highlight .nd { color: #AA22FF } /* Name.Decorator */ .highlight .ni { color: #999999; font-weight: bold } /* Name.Entity */ .highlight .ne { color: #D2413A; font-weight: bold } /* Name.Exception */ .highlight .nf { color: #0000FF } /* Name.Function */ .highlight .nl { color: #A0A000 } /* Name.Label */ .highlight .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */ .highlight .nt { color: #008000; font-weight: bold } /* Name.Tag */ .highlight .nv { color: #19177C } /* Name.Variable */ .highlight .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */ .highlight .w { color: #bbbbbb } /* Text.Whitespace */ .highlight .mb { color: #666666 } /* Literal.Number.Bin */ .highlight .mf { color: #666666 } /* Literal.Number.Float */ .highlight .mh { color: #666666 } /* Literal.Number.Hex */ .highlight .mi { color: #666666 } /* Literal.Number.Integer */ .highlight .mo { color: #666666 } /* Literal.Number.Oct */ .highlight .sb { color: #BA2121 } /* Literal.String.Backtick */ .highlight .sc { color: #BA2121 } /* Literal.String.Char */ .highlight .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */ .highlight .s2 { color: #BA2121 } /* Literal.String.Double */ .highlight .se { color: #BB6622; font-weight: bold } /* Literal.String.Escape */ .highlight .sh { color: #BA2121 } /* Literal.String.Heredoc */ .highlight .si { color: #BB6688; font-weight: bold } /* Literal.String.Interpol */ .highlight .sx { color: #008000 } /* Literal.String.Other */ .highlight .sr { color: #BB6688 } /* Literal.String.Regex */ .highlight .s1 { color: #BA2121 } /* Literal.String.Single */ .highlight .ss { color: #19177C } /* Literal.String.Symbol */ .highlight .bp { color: #008000 } /* Name.Builtin.Pseudo */ .highlight .vc { color: #19177C } /* Name.Variable.Class */ .highlight .vg { color: #19177C } /* Name.Variable.Global */ .highlight .vi { color: #19177C } /* Name.Variable.Instance */ .highlight .il { color: #666666 } /* Literal.Number.Integer.Long */ Application of the Monte Carlo integration in Options trading (part I) $$\\\\[2pt]$$ Igor Marfin [Unister Gmb@2014] < igor.marfin@unister.de > $$\\\\[40pt]$$ Table of Contents 0.1 Abstract 0.2 Initialization 0.3 Introduction 0.3.1 The Black–Scholes model of the option value 0.4 Integration methods (schemes) for the stock prices 0.4.1 Convergence of the Monte Carlo schemes 0.4.2 Quantile of the distribution of stochastic variables 0.5 Machinery to operate the Option trading strategies 0.5.1 Stock volatility 0.5.1.1 Historical volatility calculator 0.5.1.2 Instantaneous volatility calculator 0.5.1.3 Implied ( the future realized) volatility 0.5.2 Monte Carlo calculation of the Option's Payoff 0.6 Monte-Carlo Estimation of integrals 0.6.1 Monte-Carlo Importance Sampling 0.6.1.1 Reject/Accept method for the Monte-Carlo Importance Sampling 0.6.2 Monte-Carlo with Stratified Sampling 0.1 Abstract Today I will start a series of tutorials on numerical methods useful in solving Stochastic Differential Equations (SDE). The stochastic differential equation is usually governing the price evolution of a Option call or Option put. I am going to introduce you into the modeling the option tradings using python and Monte Carlo techniques. Honestly saying, I got an idea to investigate the topic of the modeling stochastic processes in the Finance a long time ago. Recently, I have bumped into a small pdf file (CamDavidson-Pilon, 2012) at the Cam Davidson-Pilon's github page with an Assignment on the stochastic processes in Economics. And I have decided to understand and learn basic points given in the tasks. Therefore, I target two aims, first one is a step-by-step academical understanding of methods to work with stochastic processes in the Finance and second one is development of numerical methods to model stochastic processes in the stock/option tradings. $$\\\\[5pt]$$ 0.2 Initialization To set up the python environment for the data analysis and make a nicer style of the notebook, one can run the following commands in the beginning of our modeling: In [1]: import sys sys . path = [ '/usr/local/lib/python2.7/dist-packages' ] + sys . path # to fix the problem with numpy: this replaces 1.6 version by 1.9 % matplotlib inline % pylab inline ion () import os import matplotlib import numpy as np import matplotlib.pyplot as pl import matplotlib as mpl import logging import pymc as pm # a plotter and dataframe modules import seaborn as sns # seaborn to make a nice plots of the data import pandas as pd import scipy.stats as stats # Set up logging. logger = logging . getLogger () logger . setLevel ( logging . INFO ) from book_format import load_style , figsize , set_figsize load_style () Populating the interactive namespace from numpy and matplotlib Out[1]: @import url('http://fonts.googleapis.com/css?family=Source+Code+Pro'); @import url('http://fonts.googleapis.com/css?family=Vollkorn'); @import url('http://fonts.googleapis.com/css?family=Arimo'); div.cell{ width: 1200px; margin-left: 0% !important; margin-right: auto; } div.text_cell code { background: transparent; color: #000000; font-weight: 600; font-size: 11pt; font-style: bold; font-family: 'Source Code Pro', Consolas, monocco, monospace; } h1 { font-family: 'Open sans',verdana,arial,sans-serif; } div.input_area { background: #F6F6F9; border: 1px solid #586e75; } .text_cell_render h1 { font-weight: 200; font-size: 30pt; line-height: 100%; color:#c76c0c; margin-bottom: 0.5em; margin-top: 1em; display: block; white-space: wrap; } h2 { font-family: 'Open sans',verdana,arial,sans-serif; } .text_cell_render h2 { font-weight: 200; font-size: 16pt; font-style: italic; line-height: 100%; color:#c76c0c; margin-bottom: 0.5em; margin-top: 1.5em; display: inline; white-space: wrap; } h3 { font-family: 'Open sans',verdana,arial,sans-serif; } .text_cell_render h3 { font-weight: 200; font-size: 14pt; line-height: 100%; color:#d77c0c; margin-bottom: 0.5em; margin-top: 2em; display: block; white-space: nowrap; } h4 { font-family: 'Open sans',verdana,arial,sans-serif; } .text_cell_render h4 { font-weight: 100; font-size: 14pt; color:#d77c0c; margin-bottom: 0.5em; margin-top: 0.5em; display: block; white-space: nowrap; } h5 { font-family: 'Open sans',verdana,arial,sans-serif; } .text_cell_render h5 { font-weight: 200; font-style: normal; color: #1d3b84; font-size: 16pt; margin-bottom: 0em; margin-top: 0.5em; display: block; white-space: nowrap; } div.text_cell_render{ font-family: 'Arimo',verdana,arial,sans-serif; line-height: 125%; font-size: 120%; text-align:justify; text-justify:inter-word; } div.output_subarea.output_text.output_pyout { overflow-x: auto; overflow-y: scroll; max-height: 50000px; } div.output_subarea.output_stream.output_stdout.output_text { overflow-x: auto; overflow-y: scroll; max-height: 50000px; } div.output_wrapper{ margin-top:0.2em; margin-bottom:0.2em; } code{ font-size: 70%; } .rendered_html code{ background-color: transparent; } ul{ margin: 2em; } ul li{ padding-left: 0.5em; margin-bottom: 0.5em; margin-top: 0.5em; } ul li li{ padding-left: 0.2em; margin-bottom: 0.2em; margin-top: 0.2em; } ol{ margin: 2em; } ol li{ padding-left: 0.5em; margin-bottom: 0.5em; margin-top: 0.5em; } ul li{ padding-left: 0.5em; margin-bottom: 0.5em; margin-top: 0.2em; } a:link{ font-weight: bold; color:#447adb; } a:visited{ font-weight: bold; color: #1d3b84; } a:hover{ font-weight: bold; color: #1d3b84; } a:focus{ font-weight: bold; color:#447adb; } a:active{ font-weight: bold; color:#447adb; } .rendered_html :link { text-decoration: underline; } .rendered_html :hover { text-decoration: none; } .rendered_html :visited { text-decoration: none; } .rendered_html :focus { text-decoration: none; } .rendered_html :active { text-decoration: none; } .warning{ color: rgb( 240, 20, 20 ) } hr { color: #f3f3f3; background-color: #f3f3f3; height: 1px; } blockquote{ display:block; background: #fcfcfc; border-left: 5px solid #c76c0c; font-family: 'Open sans',verdana,arial,sans-serif; width:1000px; padding: 10px 10px 10px 10px; text-align:justify; text-justify:inter-word; } blockquote p { margin-bottom: 0; line-height: 125%; font-size: 100%; } MathJax.Hub.Config({ TeX: { extensions: [\"AMSmath.js\"] }, tex2jax: { inlineMath: [ ['$','$'], [\"\\\\(\",\"\\\\)\"] ], displayMath: [ ['$$','$$'], [\"\\\\[\",\"\\\\]\"] ] }, displayAlign: 'center', // Change this to 'center' to center equations. \"HTML-CSS\": { scale:100, availableFonts: [\"Neo-Euler\"], preferredFont: \"Neo-Euler\", webFont: \"Neo-Euler\", styles: {'.MathJax_Display': {\"margin\": 4}} } }); In [2]: %% javascript IPython . load_extensions ( \"calico-spell-check\" , \"calico-document-tools\" , \"calico-cell-tools\" ); <IPython.core.display.Javascript object> In [3]: %% javascript // to use latex environment, please install the plugin first: // sudo ipython install-nbextension https://rawgit.com/jfbercher/latex_envs/master/latex_envs.zip require ( \"base/js/utils\" ). load_extensions ( \"latex_envs/latex_envs\" ) <IPython.core.display.Javascript object> $$\\\\[5pt]$$ 0.3 Introduction A typical stochastic differential equation is of the form (Wiki:SDE, 2016) \\begin{definition}\\label{def:SDE} \\begin{eqnarray} \\mathrm{d} X_t = \\mu(X_t,t)\\, \\mathrm{d} t + \\sigma(X_t,t)\\, \\mathrm{d} B_t , \\end{eqnarray} \\end{definition}$$\\\\[1pt]$$ where $X_t$ is a price of the asset (option),which follows to the stochastic process, $\\mu(X_t,t)$ is a so-called trend function and $\\sigma(X_t,t) \\mathrm{d} B_t$ is a variation defined by the Wiener process $B_t$. Before we go further and start a simple analysis, we want to understand the basics of the stochastic calculations. There is an important Itô's lemma (Wiki:Ito_Lemmma, 2016) , which is used in to find the differential of a time-dependent function of a stochastic process. This is an informal derivation of the lemma. Assume $X_t$ is a Ito drift-diffusion process that satisfies the stochastic differential equation \\ref{def:SDE}, where $\\mu(X_t,t)=\\mu_t$ and $\\sigma(X_t,t) = \\sigma_t$ functions are constans. If f(t,x) is a twice-differentiable scalar function, its expansion in a Taylor series is \\begin{eqnarray} df = \\frac{\\partial f}{\\partial t}\\,dt + \\frac{\\partial f}{\\partial x}\\,dx + \\frac{1}{2}\\frac{\\partial&#94;2 f}{\\partial x&#94;2}\\,dx&#94;2 + \\cdots , \\end{eqnarray}\\begin{eqnarray} df = \\frac{\\partial f}{\\partial t}\\,dt + \\frac{\\partial f}{\\partial x}(\\mu_t\\,dt + \\sigma_t\\,dB_t) + \\frac{1}{2}\\frac{\\partial&#94;2 f}{\\partial x&#94;2} \\left (\\mu_t&#94;2\\,dt&#94;2 + 2\\mu_t\\sigma_t\\,dt\\,dB_t + \\sigma_t&#94;2\\,dB_t&#94;2 \\right ) + \\cdots. \\end{eqnarray} In the limit as $dt \\rightarrow 0$, the terms $dt&#94;2$ and $dtdBt$ tend to zero faster than $dB&#94;2$, which is $O(dt)$. Setting them to zero, substituting dt for $dB&#94;2$, and collecting the $dt$ and $dB$ terms, we obtain \\begin{lemma}\\label{def:ito} \\begin{eqnarray} df = \\left(\\frac{\\partial f}{\\partial t} + \\mu_t\\frac{\\partial f}{\\partial x} + \\frac{\\sigma_t&#94;2}{2}\\frac{\\partial&#94;2 f}{\\partial x&#94;2}\\right)dt + \\sigma_t\\frac{\\partial f}{\\partial x}\\,dB_t. \\end{eqnarray} \\end{lemma} The equation \\ref{def:ito} has an important meaning: the two-differential function of the stochastic process is also a stochastic process. $$\\\\[5pt]$$ 0.3.1 The Black–Scholes model of the option value Itô's lemma is used to model the behavior of the option values. This model is called the the Black–Scholes model for options. What is an option and what does the value of the option mean? To answer the questions, I address you to the wiki page (Wiki:option, 2016) on the option trading. The important term of the option trading is the value $V_t$ of the option. Basically, the value of the option is just the profit which would be obtained from selling the option at the moment of the expiry date, i.e. \\begin{eqnarray} V_t = (S_t - C_t)\\cdot N_s, \\end{eqnarray} where $S_t$, $C_t$ and $N_s$ are the stock price at the moment $t$ (before or at day of the expirity), the strike price and the number of the stock shares in the option. Suppose a stock price follows a geometric Brownian motion given by the stochastic differential equation \\cite{def:SDE}. Then, for the value of an option at time $t$ $V(t, S_t)$, Itô's lemma gives \\begin{eqnarray} dV(t,S_t) = \\left(\\frac{\\partial V}{\\partial t} + \\frac{1}{2}\\left(S_t\\sigma\\right)&#94;2\\frac{\\partial&#94;2 V}{\\partial S&#94;2}\\right)\\,dt +\\frac{\\partial V}{\\partial S}\\,dS_t. \\end{eqnarray} This formula has a simple explanation: Increase $dV(t,S_t)$ of the option value is proportional to the profit of having an amount $\\frac{\\partial V}{\\partial S}$ of the stock with rising prise $S_t$ and increasing process behavior because of its stochastic nature. The last point is proportional to the magnitude of the volatility of the process. The larger variation the stock price shows, the more profit can be achieved by buying the option in the case when the tendency $$ \\frac{\\partial&#94;2 V}{\\partial S&#94;2} >0 $$ is fulfilled. If we buy the option, and our left cash held is assumed to grow at the risk free rate $r$, then the increase of the value $V_t$ at the time $t$ of our portfolio satisfies the following equation \\begin{eqnarray} dV_t = r\\left(V_t-\\frac{\\partial V}{\\partial S}S_t\\right)\\,dt + \\frac{\\partial V}{\\partial S}\\,dS_t, \\end{eqnarray} where the first term describes the profit we can obtain from putting our money into the risk-free investment fonds. Here I use the the notation of the option value $V_t$ for total portfolio value. The above formula of the total portfolio increase only makes a sense in the case when you have already bought the option. Combining the two equations gives the celebrated Black–Scholes equation \\begin{theorem}\\label{def:bs} \\begin{eqnarray} \\frac{\\partial V}{\\partial t} + \\frac{\\sigma&#94;2S&#94;2}{2}\\frac{\\partial&#94;2 V}{\\partial S&#94;2} + rS\\frac{\\partial V}{\\partial S}-rV = 0. \\end{eqnarray} \\end{theorem} If we could integrate \\cite{def:bs} to predict the value of the option in the future time $t$, we would be able to minimize the risk by skipping from consideration such options which values will be predicted to be lowering. $$\\\\[5pt]$$ 0.4 Integration methods (schemes) for the stock prices To introduce integration techniques for solving different SDE like the Black–Scholes model \\cite{def:bs}, we start our study from solving the first tasks of the Cam Davidson-Pilson assignment (CamDavidson-Pilon, 2012) . Consider the following class of interest rate models dr_t = k(b − r_t )dt + sigma r_t&#94;y dWt , t ≥ 0. For your selection of the parameters r0 , k, b, sigma, and y (but y should be different > from 1), use the Euler scheme, the Milstein scheme, and the second order approximation scheme to generate paths of this process. (a) Using the three schemes to simulate paths, propose a Monte Carlo simulation method > to find q that satisfies P{r3 > q} ≤ 0.1. By considering the time step ∆ equal either to 0.004 or to 0.1, discuss accuracy of each method. First we write a simple simulator of the stochastic process using Euler, Milstein and Runge-Kutta schemes. In [5]: ''' An example of the stochastic simulation ''' from numpy.random import standard_normal from numpy import array , zeros , sqrt , shape from pylab import * import numpy as np # Let's consider this function # dr_t = k(b − r_t )dt + sigma r_t&#94;y dWt , t ≥ 0. # It exactly matches to the Euler recurrent scheme # our parameters k = 2e-1 b = 7e0 sigma = 5e-1 y = 1.1 # an Euler Scheme def rt_Euler ( rt_prev , k , b , dt , sigma , y , dWt ): ''' rate simulator: Euler scheme ''' return rt_prev + k * ( b - rt_prev ) * dt + sigma * pow (( rt_prev ), y ) * dWt # a scheme functor depends on r, delta_t, delta_W stochastic_Euler = lambda r , dt , dWt : rt_Euler ( r , k , b , dt , sigma , y , dWt ) def Path ( func , x_0 = 1e-9 , T = 100 , dt = 1e0 , N_Sim = 3 ): ''' generates the path of the stochastic process using schema ''' Steps = round ( T / dt ); #Steps in years S = zeros ([ N_Sim , Steps ], dtype = float ) x = range ( 0 , int ( Steps ), 1 ) for j in range ( 0 , N_Sim , 1 ): S [ j , 0 ] = x_0 for i in x [: - 1 ]: dWt = sqrt ( dt ) * standard_normal () val = func ( S [ j , i ], dt , dWt ) S [ j , i + 1 ] = x_0 if np . isnan ( val ) else val plot ( x , S [ j ], label = 'asset %i ' % j ) title ( ' %d Simulations of %d Days' % ( int ( N_Sim ), int ( Steps ))) xlabel ( 'time (days)' ) ylabel ( 'stock price' ) legend ( loc = \"upper left\" ) show () Path ( stochastic_Euler ) # a Milstein Scheme # Y_{n + 1} = Y_n + a(Y_n) \\Delta t + b(Y_n) \\Delta W_n + # \\frac{1}{2} b(Y_n) b'(Y_n) \\left( (\\Delta W_n)&#94;2 - \\Delta t \\right), def afunc ( rt_prev , k , b ): return k * ( b - rt_prev ) def bfunc ( rt_prev , sigma , y ): return sigma * pow (( rt_prev ), y ) def bfuncprime ( rt_prev , sigma , y ): return sigma * y * pow (( rt_prev ), y - 1 ) def rt_Milstein ( rt_prev , k , b , dt , sigma , y , dWt ): ''' rate simulator: Milstein scheme ''' return rt_prev + afunc ( rt_prev , k , b ) * dt + bfunc ( rt_prev , sigma , y ) * dWt + 0.5 * bfunc ( rt_prev , sigma , y ) * bfuncprime ( rt_prev , sigma , y ) * ( dWt * dWt - dt ) stochastic_Milstein = lambda r , dt , dWt : rt_Milstein ( r , k , b , dt , sigma , y , dWt ) Path ( stochastic_Milstein ) # Runge–Kutta method or the second order approximation scheme def rt_Middle ( rt_prev , k , b , dt , sigma , y ): return rt_prev + afunc ( rt_prev , k , b ) * dt + bfunc ( rt_prev , sigma , y ) * pow ( dt , 0.5 ) def rt_RG ( rt_prev , k , b , dt , sigma , y , dWt ): _rt = rt_Middle ( rt_prev , k , b , dt , sigma , y ) return rt_prev + afunc ( rt_prev , k , b ) * dt + bfunc ( rt_prev , sigma , y ) * dWt + 0.5 * ( bfunc ( _rt , sigma , y ) - bfunc ( rt_prev , sigma , y )) * ( dWt * dWt - dt ) * pow ( dt , - 0.5 ) stochastic_RG = lambda r , dt , dWt : rt_RG ( r , k , b , dt , sigma , y , dWt ) Path ( stochastic_RG ) def PathComparison ( funcs , x_0 = 1e-9 , T = 100 , dt = 1e0 ): ''' generates the path of the stochastic process using schema ''' Steps = round ( T / dt ); #Steps in years N_Sim = len ( funcs ) S = zeros ([ N_Sim , Steps ], dtype = float ) x = range ( 0 , int ( Steps ), 1 ) for j in range ( 0 , N_Sim , 1 ): S [ j , 0 ] = x_0 for i in x [: - 1 ]: dWt = sqrt ( dt ) * standard_normal () for j in range ( 0 , N_Sim , 1 ): val = funcs [ j ]( S [ j , i ], dt , dWt ) S [ j , i + 1 ] = x_0 if np . isnan ( val ) else val for j in range ( 0 , N_Sim , 1 ): plot ( x , S [ j ], label = 'scheme %s ' % funcs [j].func_code.co_names[0]) title ( ' %d Simulations of %d Days' % ( int ( N_Sim ), int ( Steps ))) xlabel ( 'time (days)' ) ylabel ( 'stock price' ) legend ( loc = \"upper left\" ) show () PathComparison ([ stochastic_Euler , stochastic_Milstein , stochastic_RG ]) /usr/local/lib/python2.7/dist-packages/IPython/kernel/__main__.py:35: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future /usr/local/lib/python2.7/dist-packages/IPython/kernel/__main__.py:93: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future Our generators look good. 0.4.1 Convergence of the Monte Carlo schemes How good is the convergence of each scheme? To answer this question, I will use a method proposed in this lecture (JanPalcyewski, 2016) . In [6]: ''' Estimation of the Convergence for different schemes of SDE simulations ''' def X_t_dt ( t_n , dt , X_n , X_n_1 ): t = np . random . uniform ( t_n , dt ) return X_n + ( t - t_n ) * ( X_n_1 - X_n ) / dt def PathForConvergenceHighPrec ( func , x_0 = 1e-9 , T = 100 , dt = 1e0 , N_Sim = 300 ): ''' generates the path of the stochastic process using some schema with High Precision ''' Steps = round ( T / dt ); #Steps in years S = zeros ([ N_Sim , Steps ], dtype = float ) x = range ( 0 , int ( Steps ), 1 ) for j in range ( 0 , N_Sim , 1 ): S [ j , 0 ] = x_0 for i in x [: - 1 ]: dWt = sqrt ( dt ) * standard_normal () val = func ( S [ j , i ], dt , dWt ) S [ j , i + 1 ] = x_0 if np . isnan ( val ) else val mean = np . mean ( S , axis = 0 ) ts = np . empty ( Steps ) ts . fill ( dt ) return mean , np . cumsum ( ts ) path_high_prec , x = PathForConvergenceHighPrec ( stochastic_Euler , dt = 1e-3 ) def ReplicatePathShape ( path1 , path2 ): return np . repeat ( path1 , path2 . shape [ 0 ] / path1 . shape [ 0 ]) def PathToTestConvergence ( func , x_0 = 1e-9 , T = 100 , dt = 1e0 , N_Sim = 300 ): ''' generates the path of the stochastic process using some schema ''' Steps = round ( T / dt ); #Steps in years S = zeros ([ N_Sim , Steps ], dtype = float ) x = range ( 0 , int ( Steps ), 1 ) for j in range ( 0 , N_Sim , 1 ): S [ j , 0 ] = x_0 for i in x [: - 1 ]: dWt = sqrt ( dt ) * standard_normal () val = func ( S [ j , i ], dt , dWt ) S [ j , i + 1 ] = x_0 if np . isnan ( val ) else val mean = np . mean ( S , axis = 0 ) return mean /usr/local/lib/python2.7/dist-packages/IPython/kernel/__main__.py:14: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future /usr/local/lib/python2.7/dist-packages/IPython/kernel/__main__.py:24: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future Then I define the estimator of the strong convergence as it was supposed in the lecture. Also I illustrate the convergence of the Euler scheme and estimate the convergence limit parameters. In [12]: def ConvergenceStrong ( func , dt , path_ref , N_Sim = 100 ): path = PathToTestConvergence ( func , dt = dt , N_Sim = N_Sim ) path = ReplicatePathShape ( path , path_ref ) return np . mean ( np . absolute ( path - path_ref )) deltaT = [ 5e0 , 1e0 , 5e-1 , 4e-1 , 2e-1 , 1e-1 , 5e-2 , 2e-2 ] def ConvergenceStrongSeries ( func , path_ref , deltaT ): return [ ConvergenceStrong ( func , dt , path_ref ) for dt in deltaT ] # for dt in deltaT: # yield ConvergenceStrong(func,dt,path_ref) # to produce some ensemble of the different convergences how_many = 10 ConvergenceStrongSeriesMany = [ ConvergenceStrongSeries ( stochastic_Euler , path_high_prec , deltaT ) for i in range ( how_many )] ConvergenceSeries = ConvergenceStrongSeries ( stochastic_Euler , path_high_prec , deltaT ) scheme = stochastic_Euler . func_code . co_names [ 0 ] def plotConvergengeSingle ( deltaT , ConvergenceSeries , scheme ): fig , ( ax1 , ax2 ) = plt . subplots ( 2 , 1 , figsize = ( 12 , 12 )) # a first plot: improving convergence with descreasing deltaT ax1 . plot ( range ( len ( deltaT )), ConvergenceSeries ) ax1 . set_title ( \"Convergence of the %s \" % scheme ) # a second plot: convergence as a function of the deltaT ax2 . plot ( deltaT , ConvergenceSeries , label = 'emperical' ) ax2 . plot ( deltaT , deltaT , label = 'theoretical' ) ax2 . set_title ( \"Convergence of the %s \" % sc heme ) ax2 . legend ( loc = \"upper left\" ) show () def plotConvergengeMultiple ( deltaT , ConvergenceStrongSeriesMany , scheme ): ''' it is as plotConvergengeSingle but it plots multiple stochastic series of the convergencies ''' fig , ( ax1 , ax2 ) = plt . subplots ( 2 , 1 , figsize = ( 12 , 12 )) for ConvergenceSeries in ConvergenceStrongSeriesMany : ax1 . plot ( range ( len ( deltaT )), ConvergenceSeries ) ax1 . set_title ( \"Convergence of the %s \" % scheme ) # for i , ConvergenceSeries in enumerate ( ConvergenceStrongSeriesMany ): ax2 . plot ( deltaT , ConvergenceSeries , label = 'emperical %d ' % i ) ax2 . plot ( deltaT , deltaT , label = 'theoretical' ) ax2 . set_title ( \"Convergence of the %s \" % sc heme ) ax2 . legend ( loc = \"upper left\" ) show () from scipy import optimize def findConvergenceParams ( deltaT , ConvergenceSeries ): # fit our result to find convergence parameters # method 1 #fitfunc = lambda p, x: p[0]*pow(x,p[1])# Target function #errfunc = lambda p, x, y: fitfunc(p, x) - y # Distance to the target function #p0 = [1., 1.] # Initial guess for the parameters #p1, success = optimize.leastsq(errfunc, p0[:], args=(np.array(deltaT), np.array(ConvergenceSeries))) # method 2 def f ( x , a , b ): return a * pow ( x , b ) p1 = optimize . curve_fit ( f , np . array ( deltaT ), np . array ( ConvergenceSeries ))[ 0 ] return p1 def findConvergenceParamsFromMany ( deltaT , ConvergenceStrongSeriesMany ): ''' fit parameters of the convergence limits from several stochastic series and return the average values of the parameters ''' res = [] for ConvergenceSeries in ConvergenceStrongSeriesMany : res += [ findConvergenceParams ( deltaT , ConvergenceSeries )] #print res return np . mean ( np . array ( res ), axis = 0 ) . tolist () p1 = findConvergenceParams ( deltaT , ConvergenceSeries ) p2 = findConvergenceParamsFromMany ( deltaT , ConvergenceStrongSeriesMany ) #print p2 print \"A Convergence limit obtained from a random path: {0:.2f}*deltaT&#94;{1:.2f}\" . format ( * p1 ) print \"A Convergence limit obtained from an ensemble of paths: {0:.2f}*deltaT&#94;{1:.2f}\" . format ( * p2 ) plotConvergengeMultiple ( deltaT , ConvergenceStrongSeriesMany , scheme ) A Convergence limit obtained from a random path: 1.57*deltaT&#94;0.21 A Convergence limit obtained from an ensemble of paths: 2.42*deltaT&#94;0.78 /usr/local/lib/python2.7/dist-packages/IPython/kernel/__main__.py:10: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future From previous estimations, we get that the strong convergence for the Euler scheme is about $$\\lim_{\\delta \\to 0}E(|S_t&#94;T - S_t&#94;{Euler}|)\\sim 1.79*{\\delta t}&#94;{0.45},$$ where $S_t&#94;T$ is an exact solution of the SDE. 0.4.2 Quantile of the distribution of stochastic variables Let's try to answer the question (a): (a) Using the three schemes to simulate paths, propose a Monte Carlo simulation method > to find q that satisfies P{r3 > q} ≤ 0.1. In [13]: def PathGenerator ( func , x_0 = 1e-9 , T = 100 , dt = 1e0 , N_Sim = 1000 ): ''' generates the path of the stochastic process using some schema ''' Steps = round ( T / dt ); #Steps in years S = zeros ([ N_Sim , Steps ], dtype = float ) x = range ( 0 , int ( Steps ), 1 ) for j in range ( 0 , N_Sim , 1 ): S [ j , 0 ] = x_0 for i in x [: - 1 ]: dWt = sqrt ( dt ) * standard_normal () val = func ( S [ j , i ], dt , dWt ) S [ j , i + 1 ] = x_0 if np . isnan ( val ) else val return S distr3elem = PathGenerator ( stochastic_Euler )[:, 2 ] hist ( distr3elem ) # get a r3 distributions (3rd step of the monte carlo simulation) title ( \"A distribution of the 3rd element from Euler scheme\" ) # calculate the quantile of 0.1 quantile = 0.1 # method 1 from scipy.stats.mstats import mquantiles print \"Method #1: Pr(r3>{0:.2f})<=0.1\" . format ( * mquantiles ( distr3elem , prob = [ 1. - quantile ])) # method 2 using the normal approximation import scipy mean = np . mean ( distr3elem ) var = np . std ( distr3elem ) print \"Method #2: Pr(r3>{0:.2f})<=0.1\" . format ( * scipy . stats . norm . ppf ( [ 1. - quantile ], mean , var )) Method #1: Pr(r3>3.47)<=0.1 Method #2: Pr(r3>3.46)<=0.1 /usr/local/lib/python2.7/dist-packages/IPython/kernel/__main__.py:6: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future Here I repeat all previous exercises for the Milstein scheme. In [14]: # let's get our theoretical solution of the stochastic differential equation :-) path_high_prec , x = PathForConvergenceHighPrec ( stochastic_Milstein , dt = 1e-3 ) /usr/local/lib/python2.7/dist-packages/IPython/kernel/__main__.py:14: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future /usr/local/lib/python2.7/dist-packages/IPython/kernel/__main__.py:24: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future In [15]: # now produce a set of the convergence series ConvergenceStrongSeriesMany = [ ConvergenceStrongSeries ( stochastic_Milstein , path_high_prec , deltaT ) for i in range ( how_many )] ConvergenceSeries = ConvergenceStrongSeries ( stochastic_Milstein , path_high_prec , deltaT ) scheme = stochastic_Milstein . func_code . co_names [ 0 ] /usr/local/lib/python2.7/dist-packages/IPython/kernel/__main__.py:10: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future In [16]: # here we find parameters of the convergence limit p1 = findConvergenceParams ( deltaT , ConvergenceSeries ) p2 = findConvergenceParamsFromMany ( deltaT , ConvergenceStrongSeriesMany ) In [17]: # plot obtained results print \"A Convergence limit obtained from a random path: {0:.2f}*deltaT&#94;{1:.2f}\" . format ( * p1 ) print \"A Convergence limit obtained from an ensemble of paths: {0:.2f}*deltaT&#94;{1:.2f}\" . format ( * p2 ) plotConvergengeMultiple ( deltaT , ConvergenceStrongSeriesMany , scheme ) A Convergence limit obtained from a random path: 1.48*deltaT&#94;2.30 A Convergence limit obtained from an ensemble of paths: 1.74*deltaT&#94;1.06 In [18]: # a quantile study distr3elem = PathGenerator ( stochastic_Milstein )[:, 2 ] hist ( distr3elem ) # get a r3 distributions (3rd step of the monte carlo simulation) title ( \"A distribution of the 3rd element from Milstein scheme\" ) print \"Method #1: Pr(r3>{0:.2f})<=0.1\" . format ( * mquantiles ( distr3elem , prob = [ 1. - quantile ])) Method #1: Pr(r3>3.54)<=0.1 /usr/local/lib/python2.7/dist-packages/IPython/kernel/__main__.py:6: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future Here is a summary on the obtained results for the Milstein scheme the strong convergence is about $$\\lim_{\\delta \\to 0}E(|S_t&#94;T - S_t&#94;{Milstein}|)\\sim 1.74*{\\delta t}&#94;{1.06} $$ Pr(r3>3.60)<=0.1 $$\\\\[5pt]$$ 0.5 Machinery to operate the Option trading strategies Now it is a time when we can introduce a few useful terms needed to understand what the European vanilla option is. From the wiki about Options (Wiki:option, 2016) , we can conclude that an option is a contract which gives the buyer (the owner or holder of the option) the right, but not the obligation, to buy or sell an underlying asset or instrument at a specified strike price on or before a specified date; there are the Call Option, which is a contract to buy stock shares and the Put Option, which guarantees you to sell stocks at the strike price. We restrict ourselves to consideration of the Call Options . Call Options have the following properties (I have just copied-pasted the wiki text here): Strike price : this is the price at which you can buy the stock (if you have bought a call option) or the price at which you must sell your stock (if you have sold a call option). Expiry date : this is the date on which the option expires, or becomes worthless, if the buyer doesn't exercise it. Premium : this is the price you pay when you buy an option and the price you receive when you sell an option. The current price of the Call Option on the market $S_t&#94;{Option}$ is defined by the formula \\begin{definition}\\label{def:priceOptionCall} \\begin{eqnarray} S_t&#94;{Option} = S_t&#94;{stock} + Premium - S&#94;{Strike}, \\end{eqnarray} \\end{definition}$$\\\\[1pt]$$ where $S_t&#94;{stock}$ is the current stock price, $Premium$ is the price you have to pay to the option holder because of the volatility of the stock prices. The call premium tends to go down as the option gets closer to the call date. And it goes down as the option price rises relative to the stock price. Also it is worth to mention that there are Bid/Ask prices of the option which are is more relevant in ascertaining the value of the option than the last price since options are not frequently traded. Meaning the value is usually the Ask/Bid Price . I want to clarify the Bid/Ask price definitions a little bit. As Investopedia (Investopedia, 2016) states, the Bid/Ask/Spread terms on the market are fully determined by the concept of the Supply and Demand . Supply refers to the volume or abundance of a particular item in the marketplace, such as the supply of stock for sale. Demand refers to an individual's willingness to pay a particular price for an item or stock. Suppose that a one-of-a-kind diamond is found in the remote countryside of Africa by a miner. Two potential buyers make themselves known about the diamond and submit bids for \\$1.2 million and \\$1.3 million dollars ( the Bid prices ), respectively. The asking ( Ask ) price of that diamond will be \\$1.3 million dollars more likely. The spread is the difference between the bid and asking prices for a particular security. An option usually covers 100 shares. So the bid/ask price should be multiplied by 100 to get the total cost . Let`s consider a realistic example for trading option strategy from the wiki: Let's say we bought 3 PNC strike \\$45, January 2012 call options in August for \\$11.75. That means we paid \\$3,525 (11.75 \\* 3 options for 100 shares each) for the right to buy 300 (3\\*100) PNC shares for \\$45 per share between now and January 2012. The stock at that time traded at \\$50.65 meaning the theoretical call premium was \\$6.1 as shown by our formula: (current price + theoretical time/volatility premium) – strike price, (50.65 + 6.1 – 45 = 11.75). Today the stock is trading at \\$64 making the call option worth \\$19.45 with a theoretical call premium now of 45 cents above its in-the-money intrinsic value \\$19 (the \\$64 market price minus the call option \\$45 strike price). First, we want to develop a simple scrambler to get data from Yahoo Finance. The pandas framework has an experimental tool pandas.io.data.Options which directly operates with Yahoo Finance!. This tool requires lxml library to be installed in the system. To install lxml support for the python in the Debian , we can apt-get like it is shown below: sudo apt-get install python-lxml In [19]: \"\"\" A simple Scrapper of the Option data \"\"\" from pandas.io.data import Options import datetime def get_option_data ( ticker , exp_date , strike_price_slice , type = \"call\" ): \"\"\" return call and put options \"\"\" x = Options ( ticker , 'yahoo' ) index = ( strike_price_slice , slice ( None ), type ) return x . get_options_data ( expiry = exp_date ) . loc [ index ,:] # our test parameters of the option market # I want to get information on options with # 1) expiry--> 1st October of 2016 expiry = datetime . date ( 2016 , 10 , 1 ) # 2) stock name (underlying) ticker = 'aapl' # 3) slice on the strike price strike_price_slice = slice ( 100. , 110. , 1 ) # 4) type of the option: 'call' option_type = \"call\" options_data = get_option_data ( ticker , expiry , strike_price_slice , option_type ) options_data Out[19]: Last Bid Ask Chg PctChg Vol Open_Int IV Root IsNonstandard Underlying Underlying_Price Quote_Time Strike Expiry Type Symbol 100 2016-10-21 call AAPL161021C00100000 14.95 14.85 15.00 -0.31 -2.03% 65 11208 25.91% AAPL False AAPL 111.58 2016-04-14 10:25:00 105 2016-10-21 call AAPL161021C00105000 11.55 11.45 11.55 -0.25 -2.12% 18 7428 25.07% AAPL False AAPL 111.58 2016-04-14 10:25:00 110 2016-10-21 call AAPL161021C00110000 8.60 8.55 8.65 -0.28 -3.15% 183 13655 24.52% AAPL False AAPL 111.58 2016-04-14 10:25:00 OK. What can be done else? We will need to have a calculator of the stock volatility. $$\\\\[5pt]$$ 0.5.1 Stock volatility There are many definitions of the Stock Volatility. I will try to undercover all of the them. First, let's recover the difinion of the stock return. Suppose $S_t$ is the price of the stock on day $t$, then the daily return on day $t$ is: \\begin{definition}\\label{def:return} \\begin{eqnarray} r_t = \\frac{ S_t - S_{t-1} }{ S_{t-1} }. \\end{eqnarray} \\end{definition}$$\\\\[1pt]$$ Also logarithmic returns are defined according to the Financial Mathematics (WikiVolatility, 2016) . The logarithmic returns are determined as follows \\begin{definition}\\label{def:logreturn} \\begin{eqnarray} logr_t = log(r_t+1). \\end{eqnarray} \\end{definition}$$\\\\[1pt]$$ Let's implement the calculation of returns for stocks. I will copy/paste a few code from my previous study \"Money Management System: Portfolio Optimization Approaches or How to Become a Millionaire from stock tradings\" . In [20]: ''' Igor Marfin <Unister Gmb, 2014> igor.marfin@unister.de Extraction the financial data from Yahoo! Finance Returns calculation ''' from pandas.io.data import DataReader from datetime import datetime instruments = [ 'AAPL' ] def getStockPrices ( startdate = datetime ( 2016 , 3 , 1 ), enddate = datetime . now (), stocks = instruments ): '''returns the stock prices''' data = map ( lambda x : DataReader ( x , \"yahoo\" , startdate , enddate ), stocks ) for i , symbol in enumerate ( stocks ): data [ i ][ 'Symbol' ] = stocks [ i ] return data def addReturns ( dataframe , price = 'Close' ): '''adds the returns on the price''' # get Series on the interested column _series = dataframe [ price ] _series = _series . pct_change () dataframe [ price + '_return' ] = _series return dataframe def addLogReturns ( dataframe , price = 'Close' ): '''adds the logarithmic returns on the price''' # get Series on the interested column _series = dataframe [ price ] _series = _series . pct_change () _series = _series . apply ( lambda x : np . log ( x + 1 )) dataframe [ price + '_logreturn' ] = _series return dataframe # test of the reader and 'PctChange' adder data = getStockPrices () index = 0 # get data of the 1st instrument # check the difference between two definitions pd . Series ( addLogReturns ( addReturns ( data [ index ])) . dropna ()[ \"Close_return\" ] - addLogReturns ( addReturns ( data [ index ])) . dropna ()[ \"Close_logreturn\" ]) . plot () Out[20]: <matplotlib.axes._subplots.AxesSubplot at 0xd23b5ac> In [21]: # a plot of the logarithmic returns addLogReturns ( addReturns ( data [ index ])) . dropna ()[ \"Close_logreturn\" ] . plot () Out[21]: <matplotlib.axes._subplots.AxesSubplot at 0xd22f76c> So, the difference in results is marginal. $$\\\\[5pt]$$ 0.5.1.1 Historical volatility calculator $$\\\\[5pt]$$ Let's add several calculators of the stock volatility. In [22]: ''' Igor Marfin <Unister Gmb, 2014> igor.marfin@unister.de Volatility calculators ''' def VolatilityPriceStdDev ( dataframe , price = 'Close' ): ''' It returns Std Deviation of the price as its Volatility. More details can be found here http://stockcharts.com/school/doku.php?id=chart_school:technical_indicators:standard_deviation_volatility ''' #print dataframe[price].describe() return dataframe [ price ] . std () # test of the Std Deviation for price print 'Std Deviation for the Close price ' , VolatilityPriceStdDev ( data [ index ]) def VolatilityReturnsStdDev ( dataframe , price = 'Close' , type = \"return\" ): ''' calculate returns of the prices. More details https://gist.github.com/johntyree/4587049 http://www.arngarden.com/2013/06/02/calculating-volatility-of-multi-asset-portfolio-example-using-python/ ''' return dataframe [ price + '_' + type ] . std () # test of the Std Deviation for returns print 'Std Deviation for the return ' , VolatilityReturnsStdDev ( data [ index ]) print 'Std Deviation for the logarithmic return ' , VolatilityReturnsStdDev ( data [ index ], type = \"logreturn\" ) ''' Realizations of the Volatility given at https://en.wikipedia.org/wiki/Volatility_%28finance%29 They relates to so-called \"actual historical volatility\" ''' import math import datetime def annualized_historical_volatility ( past_rollback_in_days = 30 , stocks = instruments ): ''' returns the annualized volatility https://gist.github.com/johntyree/4587049 ''' enddate = datetime . datetime . now () startdate = enddate - datetime . timedelta ( days = past_rollback_in_days ) data = getStockPrices ( startdate = startdate , enddate = enddate , stocks = stocks ) data = [ addLogReturns ( addReturns ( data [ i ])) for i , elem in enumerate ( data )] return [ math . sqrt ( 252. ) * VolatilityReturnsStdDev ( elem , price = 'Close' , type = \"logreturn\" ) for _ , elem in enumerate ( data )] print 'Historical annualized volatility for last 30 days ' , annualized_historical_volatility () print 'Historical annualized volatility for last 60 days ' , annualized_historical_volatility ( past_rollback_in_days = 60 ) def historical_volatility ( period_fraction , past_rollback_in_days = 30 , stocks = instruments ): ''' return a generalized volatility for some period: day,monthly ''' vol = annualized_historical_volatility ( past_rollback_in_days = past_rollback_in_days , stocks = stocks ) return map ( lambda x : math . sqrt ( period_fraction ) * x , vol ) period_frac = 1. / 12. # to calculate monthly volatility: 12 months in 1 year print 'Historical generalized volatility for the period %.3f (in the fraction of year) %r ' % ( period_frac , historical_volatility ( period_frac )) period_frac = 1. / 252. # to calculate daily volatility: 252 working days in 1 year print 'Historical generalized volatility for the period %.3f (in the fraction of year) %r ' % ( period_frac , historical_volatility ( period_frac )) Std Deviation for the Close price 3.60323590933 Std Deviation for the return 0.0103423551266 Std Deviation for the logarithmic return 0.0103178135132 Historical annualized volatility for last 30 days [0.17287079679166056] Historical annualized volatility for last 60 days [0.19563449702221694] Historical generalized volatility for the period 0.083 (in the fraction of year) [0.049903500531345159] Historical generalized volatility for the period 0.004 (in the fraction of year) [0.010889836601340866] $$\\\\[5pt]$$ 0.5.1.2 Instantaneous volatility calculator $$\\\\[2pt]$$ Also sometimes it is useful to calculate so-called instantaneous volatility. I address you to these slides https://www.google.de/url?sa=t&rct=j&q=&esrc=s&source=web&cd=6&cad=rja&uact=8&ved=0ahUKEwiUoKCKtvTLAhVElw8KHT4uBlUQFgg2MAU&url=http%3A%2F%2Farchive.euroscipy.org%2Ffile%2F6368%2Fraw%2FESP11-Fast_MonteCarlo_Slides.pdf&usg=AFQjCNEZ8ZuJvqfRdgU_Buxm_Aji1d3zHA&sig2=eVWUafGBNSSS4f9Ymk0z9Q&bvm=bv.118443451,d.bGg about Monte Carlo simulations of the stock volatility where you can find the definition of the instantaneous volatility. Because the instantaneous volatility has a theoretical meaning (it is a parameter of the particular SDE), I can only approximate its value using intra-day data of the stock prices. All steps below will show how I calculate the instantaneous volatility. First, download the api to work out intra-day data from GOOGLE. In [58]: ! git clone https://github.com/maxvitek/intradata Cloning into 'intradata'... remote: Counting objects: 54, done.[K remote: Compressing objects: 100% (31/31), done.[K remote: Total 54 (delta 20), reused 54 (delta 20), pack-reused 0[K Unpacking objects: 100% (54/54), done. In [23]: # %load intradata/intradata.py import time import datetime import pandas import requests import csv import io import pytz PROTOCOL = 'http://' BASE_URL = 'www.google.com/finance/getprices' def get_google_data ( symbol , interval = 60 , lookback = 1 , end_time = time . time ()): \"\"\" Get intraday data for the symbol from google finance and return a pandas DataFrame :param symbol (str) :param interval (int) :param lookback (int) :param end_time (unix timestamp) :returns pandas.DataFrame \"\"\" resource_url = PROTOCOL + BASE_URL payload = { 'q' : symbol , 'i' : str ( interval ), 'p' : str ( lookback ) + 'd' , 'ts' : str ( int ( end_time * 1000 )), 'f' : 'd,o,h,l,c,v' } r = requests . get ( resource_url , params = payload ) quotes = [] with io . BytesIO ( r . content ) as csvfile : quote_reader = csv . reader ( csvfile ) timestamp_start = None timestamp_offset = None timezone_offset = 0 for row in quote_reader : if row [ 0 ][: 16 ] == 'TIMEZONE_OFFSET=' : timezone_offset = - 1 * int ( row [ 0 ][ 16 :]) elif row [ 0 ][ 0 ] not in 'a1234567890' : # discard headers continue elif row [ 0 ][ 0 ] == 'a' : # 'a' prepended to the timestamp that starts each day timestamp_start = pytz . utc . localize ( datetime . datetime . fromtimestamp ( float ( row [ 0 ][ 1 :])) + datetime . timedelta ( minutes = timezone_offset )) timestamp_offset = 0 elif timestamp_start : timestamp_offset = int ( row [ 0 ]) if not timestamp_start and not timestamp_offset : continue timestamp = timestamp_start + datetime . timedelta ( seconds = timestamp_offset * interval ) closing_price = float ( row [ 1 ]) high_price = float ( row [ 2 ]) low_price = float ( row [ 3 ]) open_price = float ( row [ 4 ]) volume = float ( row [ 5 ]) quotes . append (( timestamp , closing_price , high_price , low_price , open_price , volume )) df = pandas . DataFrame ( quotes , columns = [ 'datetime' , 'close' , 'high' , 'low' , 'open' , 'volume' ]) df = df . set_index ( 'datetime' ) return df Afterwards I implement the calculation of the instantaneous volatility using the following function. In [24]: ''' Igor Marfin <Unister Gmb, 2014> igor.marfin@unister.de Instantaneous Volatility Calculator ''' import time def instantaneous_volatility ( hours_ago = 20 , stocks = instruments ): ''' returns instantaneous volatility for one day ''' end_time = datetime . datetime . now () - datetime . timedelta ( hours = hours_ago ) data = map ( lambda x : get_google_data ( x , interval = 60 , lookback = 30 )[ end_time : datetime . datetime . now ()] , instruments ) data = [ addLogReturns ( addReturns ( data [ i ], price = 'close' ), price = 'close' ) for i , elem in enumerate ( data )] # math.sqrt(24.*60) is because we want to calculate the # instantaneous_volatility for one day using intra data with 1 minute resolution! #vol = lambda x: math.sqrt(24.*60.)*VolatilityReturnsStdDev(x,price='close',type=\"logreturn\") vol = lambda x : VolatilityReturnsStdDev ( x , price = 'close' , type = \"logreturn\" ) return map ( lambda x : math . sqrt ( float ( len ( x ))) * vol ( x ), data ) instantaneous_volatility ( hours_ago = 20 ) INFO:requests.packages.urllib3.connectionpool:Starting new HTTP connection (1): www.google.com Out[24]: [0.0070833300621983919] $$\\\\[5pt]$$ 0.5.1.3 Implied ( the future realized) volatility $$\\\\[2pt]$$ Black-Scholes model describes the option prices as a function of the underlying price, strike, risk-free interest rate, time to expiry and volatility: $$ V=BS(S,K,r,T, \\sigma), $$ where $V$ is a solution of the \\cite{def:bs}. The volatility value $\\sigma$ used here is an estimate of the future realized price volatility or so-called implied volatility. Given that the stock price $S$, the strike $K$, risk-free interest rate $r$, and time to expiry $T$ are all known and easily found, we can actually think of a price for an option in the market as a function of $\\sigma$ only. What we want to find is a root of the following equation, where $V$ is known $$V=BS(\\sigma).$$ Where can we get all this information on $S$, $K$, $r$, $T$ and $V$? All data can be found at https://www.google.com/finance/ . For example for 'AAPL', check this link https://www.google.com/finance/option_chain?q=NASDAQ:AAPL or http://www.google.com/finance/option_chain?q=AAPL&output=json . Also you can use the Yahoo Finance, i.e. https://finance.yahoo.com/q/op?s=AAPL . We have already built a scrapper to get these data from Yahoo Finance get_option_data(ticker,expiry,strike_price_slice,option_type) Where someone can get $r$,risk-free interest rate? I recommend to visit this page https://www.treasury.gov/resource-center/data-chart-center/interest-rates/Pages/TextView.aspx?data=longtermrate where I have found for \"ten year US Treasury rate value $r$ as 2.41%. Ok. Let's extract needed data. Some parameters should be provided by us: Strike price (K) Time to maturity (T) Ticker (Underlying) We want to get $V$ (the call option prise) when K=100 T=30 days Ticker = 'AAPL' In [25]: until_days = 30. expiry = datetime . datetime . now () + datetime . timedelta ( days = until_days ) T = until_days / 252. K = 110. ticker = 'aapl' r = 0.0241 S = getStockPrices ( datetime . datetime . now () - datetime . timedelta ( days = 1 ), enddate = datetime . datetime . now (), stocks = [ ticker ])[ 0 ][ 'Close' ] option_type = \"call\" options_data = get_option_data ( ticker , expiry , K , option_type ) V = options_data [ 'Last' ] . values [ 0 ] option_data = pd . DataFrame ({ 'T' :[ T ], 'expiry' :[ expiry ], 'K' :[ K ], 'ticker' :[ ticker ], 'V' :[ V ], 'r' :[ r ], 'S' : S }) option_data Out[25]: K S T V expiry r ticker Date 2016-04-13 110 112.040001 0.119048 4.6 2016-05-14 16:26:42.737908 0.0241 aapl We need numerically to solve this equation $$ 3.22\\$ = BS(109.01\\$,110\\$,0.024,0.12,\\sigma) ,$$ where $BS$ is the Black–Scholes formula (WikiBlack–Scholesformula, 2016) . In [26]: ''' Igor Marfin <Unister Gmb, 2014> igor.marfin@unister.de Implied Volatility Calculator ''' import scipy N = scipy . stats . norm . cdf def bs_price ( sigma , cp_flag , S , K , T , r ): ''' returns Black–Scholes formula \\begin{align} C(S, t) &= N(d_1)S - N(d_2) Ke&#94;{-r(T - t)} \\\\ d_1 &= \\frac{1}{\\sigma\\sqrt{T - t}}\\left[\\ln\\left(\\frac{S}{K}\\right) + \\left(r + \\frac{\\sigma&#94;2}{2}\\right)(T - t)\\right] \\\\ d_2 &= d_1 - \\sigma\\sqrt{T - t} \\\\ \\end{align} ''' d1 = ( np . log ( S / K ) + ( r + sigma * sigma / 2. ) * T ) / ( sigma * sqrt ( T )) d2 = d1 - sigma * np . sqrt ( T ) if cp_flag == 'call' : price = S * N ( d1 , 0.0 , 1.0 ) - K * exp ( - r * T ) * N ( d2 , 0.0 , 1.0 ) else : price = K * exp ( - r * T ) * N ( - d2 ) - S * N ( - d1 ) return price args = [ 0.01 , # fake sigma 'call' , option_data [ 'S' ] . values [ 0 ], option_data [ 'K' ] . values [ 0 ], option_data [ 'T' ] . values [ 0 ], option_data [ 'r' ] . values [ 0 ], ] print \"Option Price at the fake sigma %.2f is %.2f \" % ( 0.01 , bs_price ( * args )) args = [ 'call' , option_data [ 'S' ] . values [ 0 ], option_data [ 'K' ] . values [ 0 ], option_data [ 'T' ] . values [ 0 ], option_data [ 'r' ] . values [ 0 ], ] print args , option_data [ 'V' ] . values [ 0 ] def implied_volatility ( option_params , V ): ''' numerically solves the BS equation ''' # the secant method solver = scipy . optimize . newton _target_fun = lambda x : bs_price ( x , * option_params ) - V #print scipy.optimize.brentq(_target_fun, 0., 1.) return solver ( _target_fun , 1. ) impliedSigma = implied_volatility ( args , option_data [ 'V' ] . values [ 0 ]) args = [ impliedSigma ] + args [ 0 :] print \"Calculated %.2f and Real %.2f Option Prices at sigma %.2f \" % ( bs_price ( * args ), V , impliedSigma ) Option Price at the fake sigma 0.01 is 2.36 ['call', 112.040001, 110.0, 0.11904761904761904, 0.0241] 4.6 Calculated 4.60 and Real 4.60 Option Prices at sigma 0.22 In [27]: \"\"\" Newton's method of the solving BS formula. To validate the previous result \"\"\" def find_vol ( target_value , call_put , S , K , T , r ): MAX_ITERATIONS = 100 PRECISION = 1.0e-5 sigma = 0.5 for i in xrange ( 0 , MAX_ITERATIONS ): price = bs_price ( call_put , S , K , T , r , sigma ) vega = bs_vega ( call_put , S , K , T , r , sigma ) price = price diff = target_value - price # our root print i , sigma , diff if ( abs ( diff ) < PRECISION ): return sigma sigma = sigma + diff / vega # f(x) / f'(x) # value wasn't found, return best guess so far return sigma n = scipy . stats . norm . pdf N = scipy . stats . norm . cdf def bs_price ( cp_flag , S , K , T , r , v , q = 0.0 ): d1 = ( log ( S / K ) + ( r + v * v / 2. ) * T ) / ( v * sqrt ( T )) d2 = d1 - v * sqrt ( T ) if cp_flag == 'c' : price = S * exp ( - q * T ) * N ( d1 ) - K * exp ( - r * T ) * N ( d2 ) else : price = K * exp ( - r * T ) * N ( - d2 ) - S * exp ( - q * T ) * N ( - d1 ) return price def bs_vega ( cp_flag , S , K , T , r , v , q = 0.0 ): d1 = ( log ( S / K ) + ( r + v * v / 2. ) * T ) / ( v * sqrt ( T )) return S * sqrt ( T ) * n ( d1 ) V_market = option_data [ 'V' ] . values [ 0 ] K = option_data [ 'K' ] . values [ 0 ] T = option_data [ 'T' ] . values [ 0 ] S = option_data [ 'S' ] . values [ 0 ] r = option_data [ 'r' ] . values [ 0 ] cp = 'c' # call option implied_vol = find_vol ( V_market , cp , S , K , T , r ) print 'Implied vol: %.2f%% ' % ( implied_vol * 100 ) 0 0.5 -4.25568928009 1 0.217935595223 -0.0342444025965 2 0.215598376654 -1.45533695619e-05 3 0.215597382521 -2.69473332537e-12 Implied vol: 21.56% Finally, to close the topic on the implied volatility, we can compare obtained our last result with one obtained from Yahoo Finance!: In [28]: until_days = 30. expiry = datetime . datetime . now () + datetime . timedelta ( days = until_days ) T = until_days / 252. K = 110. ticker = 'aapl' r = 0.0241 option_type = \"call\" options_data = get_option_data ( ticker , expiry , K , option_type ) print \"Calculated implied volatility %.2f and Implied Volatility %.6f from Yahoo Finance!\" % ( impliedSigma , float ( options_data [ \"IV\" ] . values [ 0 ] . replace ( \"%\" , \"\" )) * 1e-2 ) Calculated implied volatility 0.22 and Implied Volatility 0.265400 from Yahoo Finance! Unfortunately, I can't explain the difference in results, I address you to this discussion where this question was arisen http://quant.stackexchange.com/questions/21744/formula-behind-pandas-options-implied-volatility . $$\\\\[5pt]$$ 0.5.2 Monte Carlo calculation of the Option's Payoff $$\\\\[5pt]$$ We have reached the point when we would like to estimate the profit from buying the call options. This option's profit is called the option's payoff. The formula of the payoff is quite simple \\begin{definition}\\label{def:payoff} \\begin{eqnarray} P(S(t)) = Max (S(t)-K,0.), \\end{eqnarray} \\end{definition}$$\\\\[1pt]$$ where $P(t)$, $S(t)$ and $K$ are the payoff, stock and strike prices of the option by the time $t$. To estimate the $P(t)$ \\ref{def:payoff}, we assume that the stock prices follows the Ito drift-diffusion process, namely the Geometric Brownian Motion, which is defined as \\begin{definition}\\label{def:GBM} \\begin{eqnarray} \\mathrm{d}S(t)=rS(t)\\mathrm{d}t+\\sigma S(t)\\mathrm{d}B(t). \\end{eqnarray} \\end{definition}$$\\\\[1pt]$$ Recalling the Ito lemma \\ref{def:ito}, we can rewrite the equation \\ref{def:GBM} in the form \\begin{definition}\\label{def:GBM2} \\begin{eqnarray} \\mathrm{d}log(S(t)) = (r−\\frac{1}{2}\\sigma&#94;2)\\mathrm{d}t+\\sigma \\mathrm{d}B(t). \\end{eqnarray} \\end{definition}$$\\\\[1pt]$$ The SDE \\ref{def:GMB2} has an analytical solution, \\begin{theorem}\\label{def:GBM3} \\begin{eqnarray} S(t)=S(0)e&#94;{(r−\\frac{1}{2}\\sigma&#94;2)T+\\sigma\\sqrt{T}N(0,1)}, \\end{eqnarray} \\end{theorem}$$\\\\[1pt]$$ where we've used the fact that since $B(t)$ is a Brownian motion, it has the distribution as a normal distribution with variance $T$. Our aim is the developing the Monte Carlo estimator of the P(S(t)), i.e. $\\hat{E}(P(S(0)e&#94;{(r−\\frac{1}{2}\\sigma&#94;2)T+\\sigma\\sqrt{T}N(0,1)}))$ \\begin{definition}\\label{def:MCpayoff} \\begin{eqnarray} \\hat{E}(P(S(t)) = \\frac{1}{I}\\sum_i&#94;I P(S_i(t)). \\end{eqnarray} \\end{definition}$$\\\\[1pt]$$ Usually, the risk-neutral pay-off for a call option, $e&#94;{-rT}\\cdot E(P(S(t)$, is considered. In [29]: ''' Igor Marfin <Unister Gmb, 2014> igor.marfin@unister.de The Option Payoff Estimator ''' import math def StockPriceGBM ( r , sigma , dt , z ): ''' return the exp part the GBM analytical solution ''' #print \"1)\", sigma * math.sqrt(dt)*z #print \"2)\",(r-0.5*sigma**2)*dt #print \"3)\",np.exp((r-0.5*sigma**2)*dt + sigma * math.sqrt(dt)*z) return np . exp (( r - 0.5 * sigma ** 2 ) * dt + sigma * math . sqrt ( dt ) * z ) def MonteCarloPriceGenerator ( func , S_0 = 100. , T = 100 , dt = 1e0 , N_Sim = 300 ): ''' generates the MC paths of the func (either S(t) or P(t)) N_sim times ''' Steps = round ( T / dt ); #Steps in one MC chain (path) S = zeros ([ Steps + 1 , N_Sim ], dtype = float ) S [ 0 ] = S_0 # first element is S(0) for t in range ( 1 , int ( Steps ) + 1 ): z = standard_normal ( N_Sim ) S [ t ] = S [ t - 1 ] * func ( dt , z ) return S # parameters of the MC simulations until_days = 90. # 3 month forecast expiry = datetime . datetime . now () + datetime . timedelta ( days = until_days ) T = until_days / 252. dt = 1. / 252. K = 110. ticker = 'aapl' r = 0.0241 # a free-risk rate #r=0.0541 # a free-risk rate S_0 = getStockPrices ( datetime . datetime . now () - datetime . timedelta ( days = 1 ), enddate = datetime . datetime . now (), stocks = [ ticker ])[ 0 ][ 'Close' ] N_Sim = 10000 option_type = \"call\" options_data = get_option_data ( ticker , expiry , K , option_type ) V = options_data [ 'Last' ] . values [ 0 ] option_data = pd . DataFrame ({ 'T' :[ T ], 'expiry' :[ expiry ], 'K' :[ K ], 'ticker' :[ ticker ], 'V' :[ V ], 'r' :[ r ], 'S' : S }) # a volatility for dt period (daily) args = [ 'call' , option_data [ 'S' ] . values [ 0 ], option_data [ 'K' ] . values [ 0 ], option_data [ 'T' ] . values [ 0 ], option_data [ 'r' ] . values [ 0 ], ] #It can be either implied volatility or historical volatility #sigma = implied_volatility(args, option_data['V'].values[0]) sigma = historical_volatility ( dt )[ 0 ] print \"Sigma is \" , sigma stockprice_func = lambda dt , z : StockPriceGBM ( r , sigma , dt , z ) paths = MonteCarloPriceGenerator ( stockprice_func , S_0 = S_0 , T = T , dt = dt , N_Sim = N_Sim ) Sigma is 0.0108898366013 /usr/local/lib/python2.7/dist-packages/IPython/kernel/__main__.py:20: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future In [30]: # Our MC estimator def PayOffEstimator ( paths , r , T , K ): ''' returns Payoff estimation''' #N_Sim = paths.shape[1] #profit_trials=np.where(paths[-1,:]>=K) #zero_trials=np.where(paths[-1,:]<K) payoffs = np . maximum ( paths [ - 1 ,:] - K , 0. ) return payoffs * np . exp ( - r * T ), np . average ( payoffs ) * np . exp ( - r * T ) payoffs , avg_payoff = PayOffEstimator ( paths , r , T , K ) print \"On average, we earn about %.4f $ if we are buying %d -day-option (100 shares) of the ' %s ' now\" % ( avg_payoff * 100 , until_days , ticker ) zero_trials = np . where ( payoffs == 0 )[ 0 ] zero_trials_pct = float ( len ( zero_trials )) / len ( payoffs ) * 100. print \"Percentage of the zero-trials: %.2f%% \" % zero_trials_pct import seaborn as sns #hist(payoffs) # get a r3 distributions (3rd step of the monte carlo simulation) selection = np . where ( payoffs < 1.0 )[ 0 ] sns . distplot ( payoffs [ selection ]) #sns.distplot(payoffs) title ( \"A distribution of the payoff of %d -day-option of the ' %s '\" % ( until_days , ticker )) #pd.Series(payoffs).plot() On average, we earn about 298.0684$ if we are buying 90-day-option (100 shares) of the 'aapl' now Percentage of the zero-trials: 0.02% /usr/local/lib/python2.7/dist-packages/numpy/lib/function_base.py:564: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future n = np.zeros(bins, ntype) /usr/local/lib/python2.7/dist-packages/numpy/lib/function_base.py:600: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future n += np.bincount(indices, weights=tmp_w, minlength=bins).astype(ntype) Out[30]: <matplotlib.text.Text at 0xca4670c> It is important that your GBM model of the $S(t)$ has a right estimation of the daily drift term $\\mu$, which is determined by $r$ and $\\sigma$, \\begin{definition}\\label{def:drift_term} \\begin{eqnarray} \\mu =r−\\frac{1}{2}\\sigma&#94;2. \\end{eqnarray} \\end{definition}$$\\\\[1pt]$$ What happens when we increase the free-risk rate $r$ in two times, and hence the daily trend term $\\mu$ takes a larger value? The answer is given below. In [31]: #we have artificially increased the drift term r = 0.0241 # a free-risk rate r = 2.0 * r # let's double itb stockprice_func = lambda dt , z : StockPriceGBM ( r , sigma , dt , z ) paths = MonteCarloPriceGenerator ( stockprice_func , S_0 = S_0 , T = T , dt = dt , N_Sim = N_Sim ) payoffs , avg_payoff = PayOffEstimator ( paths , r , T , K ) print \"On average, we earn about %.4f $ if we are buying %d -day-option (100 shares) of the ' %s ' now\" % ( avg_payoff * 100 , until_days , ticker ) zero_trials = np . where ( payoffs == 0 )[ 0 ] zero_trials_pct = float ( len ( zero_trials )) / len ( payoffs ) * 100. print \"Percentage of the zero-trials: %.2f%% \" % zero_trials_pct import seaborn as sns #hist(payoffs) # get a r3 distributions (3rd step of the monte carlo simulation) selection = np . where ( payoffs < 4.0 )[ 0 ] sns . distplot ( payoffs [ selection ]) #sns.distplot(payoffs) title ( \"A distribution of the payoff of %d -day-option of the ' %s when the r is doubled'\" % ( until_days , ticker )) On average, we earn about 391.4910$ if we are buying 90-day-option (100 shares) of the 'aapl' now Percentage of the zero-trials: 0.00% /usr/local/lib/python2.7/dist-packages/IPython/kernel/__main__.py:20: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future Out[31]: <matplotlib.text.Text at 0xca3724c> $$\\\\[5pt]$$ 0.6 Monte-Carlo Estimation of integrals $$\\\\[5pt]$$ In the end of my tutorial, I want to solve numerically a few integrals presented in the assignment (CamDavidson-Pilon, 2012) . Let's consider the exercise Nr. 2 The next four questions deal with the problem of finding the integral $$I=\\int_0&#94;2e&#94;{-x&#94;2}dx$$ Use both crude and antithetic random numbers to estimate the value of the integral. How large a sample size should be, for antithetic and random numbers, in order to estimate correctly I up to three decimal places with probability at least 95%? $$\\\\[5pt]$$ 0.6.1 Monte-Carlo Importance Sampling $$\\\\[5pt]$$ I will use a MC integrator proposed by Cam Davidson-Pilon, and slightly modified by me. The technology of the importance sampling is very important in the world of the Monte Carlo integration. I'd like to refer a nice lecture (JessiCisewski, 2014) from Jessie Cisewski where you can find simple explanation of the technology. In [32]: # %load MCIntegrator.py ''' Taken from https://github.com/CamDavidsonPilon/Python-Numerics/blob/master/MonteCarlo/Integration/MonteCarloIntegrator.py Author CamDavidsonPilon Modified by Igor Marfin ''' import numpy as np import scipy.stats as stats import time class MCIntegrator ( object ): \"\"\" target_function: a function that accepts a n-D array, and returns an n-D array. interval: the interval of the integration b_antithetic: whether to use antithesis variables. Much quicker, but only useful on monotonic target_functions sampling_dist: a scipy frozen distribution with support equal to the interval N: number of variables to use in the initial estimate. control_variates = a list of function that accepts a nD array, and return an nD array \"\"\" def __init__ ( self , target_function , interval = ( 0 , 1 ), N = 10000 , b_antithetic = False , sampling_dist = stats . uniform (), verbose = False , control_variates = []): self . target_function = target_function self . min_interval , self . max_interval = interval self . N_ = N self . N = 0 self . sampling_dist = sampling_dist self . value = 0 self . var = np . nan self . b_antithetic = b_antithetic self . verbose = verbose self . control_variates = control_variates self . sample = np . array ([]) def estimate_N ( self , N ): self . N += N return self . _estimate ( N ) def _estimate ( self , N ): #generate N values from sampling_dist if not self . b_antithetic : U = self . sampling_dist . rvs ( size = N ) Y = self . target_function ( U ) for func in self . control_variates : X = func ( U ) Y += X if self . verbose : print Y . var () self . value += Y . sum () else : U_ = self . sampling_dist . rvs ( size = N / 2 ) antiU_ = self . min_interval + ( self . max_interval - U_ ) Y = ( self . target_function ( U_ ) + self . target_function ( antiU_ ) ) if self . verbose : print Y . var () self . value += Y . sum () self . sample = np . concatenate (( self . sample , Y )) self . var = self . sample . var ( ddof = 1 ) return self . value / self . N def estimate ( self ): self . N += self . N_ return self . _estimate ( self . N_ ) def SE ( self ): return np . sqrt ( self . var / self . N ) if self . var is not np . nan else np . nan If we use uniform random numbers, we utilize a basic Monte-Carlo estimator to evaluate the value of the integral. In [33]: ''' Monte Carlo estimation of the integral from the assignment ''' import time import scipy.stats as stats def target ( u ): return np . exp ( - u ** 2 ) * 2 mci = MCIntegrator ( target , interval = ( 0 , 2 ), b_antithetic = False , sampling_dist = stats . uniform ( 0 , 2 ), verbose = True ) N = 1e6 start = time . clock () print \"Using %d samples,\" % N print \"Non-antithetic: %.5f . ( %.5f .)\" % ( mci . estimate_N ( N ), mci . SE ()) print \"Duration: %.3f s.\" % ( time . clock () - start ) print /usr/local/lib/python2.7/dist-packages/scipy/stats/_continuous_distns.py:3909: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future return mtrand.uniform(0.0, 1.0, self._size) /usr/local/lib/python2.7/dist-packages/numpy/core/fromnumeric.py:225: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future return reshape(newshape, order=order) Using 1000000 samples, 0.475396645407 Non-antithetic: 0.88140. (0.00069.) Duration: 0.140 s. If we apply the sampling different from the uniform distribution in MC, i.e. we replace uniform distribution by some distribution close to the target function, then we can expect to increase the performance of the Monte Carlo. In [34]: ''' Importance Sampling Monte Carlo estimation of the integral from the assignment ''' import math def target ( u , interval ): return (( u >= interval [ 0 ]) & ( u <= interval [ 1 ])) * np . sqrt ( math . pi ) targetPrime = lambda u : target ( u ,( 0 , 2 )) mci = MCIntegrator ( targetPrime , interval = ( 0 , 2 ), b_antithetic = False , sampling_dist = stats . norm ( loc = 0. , scale = 1. ), verbose = True ) N = 1e6 start = time . clock () print \"Using %d samples,\" % N print \"Non-antithetic: %.5f . ( %.5f .)\" % ( mci . estimate_N ( N ), mci . SE ()) print \"Duration: %.3f s.\" % ( time . clock () - start ) print Using 1000000 samples, 0.783959574083 Non-antithetic: 0.84830. (0.00089.) Duration: 0.130 s. /usr/local/lib/python2.7/dist-packages/scipy/stats/_continuous_distns.py:127: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future return mtrand.standard_normal(self._size) The importance sampling improves the MC efficiency, i.e. achieving high accuracy of the integration requires less computing time. Let's check another importance sampling functions. In [35]: x = np . linspace ( 0 , 2 , 100 ) def target ( u ): return np . exp ( - u ** 2 ) * 2 def sampler1 ( u ): ''' a linear approximation of the exponent tail ''' return ( -. 5 * u + 1 ) * 2 def sampler2 ( u ): ''' Taylor Series Expansions of Exponential Functions http://www.efunda.com/math/taylor_series/exponential.cfm ''' #return (1-u**2+u**4/2.-u**6/2./3.+u**8/2./3./4.)*2 a = np . array ([ 0. if b < 1. else round ( b ) - 1 for b in u ]) # a regularization term return np . abs ( 1. - ( u - a ) ** 2 ) * 2 plot ( x , target ( x ), label = \"original\" ) plot ( x , sampler1 ( x ), label = \"sampler1\" ) plot ( x , sampler2 ( x ), label = \"sampler2\" ) legend ( loc = \"upper right\" ) Out[35]: <matplotlib.legend.Legend at 0xca5420c> $$\\\\[5pt]$$ 0.6.1.1 Reject/Accept method for the Monte-Carlo Importance Sampling $$\\\\[5pt]$$ The code, shown below, is a simple Reject/Accept method to generate samples according some analytical functions. In [37]: ''' A realization of the importance sampler for any sampling distribution See http://people.duke.edu/~ccc14/sta-663/MonteCarlo.html ''' import scipy.stats as st # Reject/Accept method to generate samples according the target function class Importance ( object ): def __init__ ( self , target , interval ): self . target = target self . interval = interval def rvs ( self , size ): u = np . random . uniform ( * self . interval , size = size ) # accept-reject criterion for each point in sampling distribution # what is the best interval of the randomly generated numbers? # this one ? r = np . random . uniform ( np . min ( self . target ( u )), np . max ( self . target ( u )), size ) # or this one? #r = np.random.uniform(0, np.max(self.target(u)), size) # or this one? #r = np.random.uniform(*self.interval, size=size) # accepted points will come from target distribution v = u [ r <= self . target ( u )] return v # An alternative approach. # This calculator is technically hard to control: rv_continuous can produce # negative values of the pdf. #class Importance(st.rv_continuous): # def _pdf(self,x): # pass #Importance._pdf = lambda self,x: sampler1(x) #mysampler = Importance(name='mysampler') interval = ( 0 , 2 ) x = np . linspace ( * interval , num = 100 ) def target ( u ): ''' a function to be integrated''' return np . exp ( - u ** 2 ) * 2 # An Example of the importance sampling with the Sampler1 mysampler1 = Importance ( sampler1 , interval ) hist ( mysampler1 . rvs ( 10000 ), label = \"sampler1\" ) # to plot distribution # normalization of the sampler normalization1 = MCIntegrator ( sampler1 , interval = interval , b_antithetic = False , sampling_dist = stats . uniform ( * interval )) . estimate_N ( 1e6 ) # target function should be modified accordingly! def targetPrime ( x , target , sampler , interval , normalization ): return target ( x ) / ( sampler ( x ) / ( interval [ 1 ] - interval [ 0 ])) * normalization _targetPrime = lambda x : targetPrime ( x , target , sampler1 , interval , normalization1 ) mci = MCIntegrator ( _targetPrime , interval = interval , b_antithetic = False , sampling_dist = mysampler1 , verbose = True ) N = 1e6 start = time . clock () print \"Sampler1\" print \"Using %d samples,\" % N print \"Non-antithetic: %.5f . ( %.5f .)\" % ( mci . estimate_N ( N ), mci . SE ()) print \"Duration: %.3f s.\" % ( time . clock () - start ) print # An Example of the importance sampling with the Sampler2 mysampler2 = Importance ( sampler2 , interval ) hist ( mysampler2 . rvs ( 10000 ), label = \"sampler2\" ) # to plot distribution # normalization of the sampler normalization2 = MCIntegrator ( sampler2 , interval = interval , b_antithetic = False , sampling_dist = stats . uniform ( * interval )) . estimate_N ( 1e6 ) _targetPrime2 = lambda x : targetPrime ( x , target , sampler2 , interval , normalization2 ) mci = MCIntegrator ( _targetPrime2 , interval = interval , b_antithetic = False , sampling_dist = mysampler2 , verbose = True ) N = 1e6 start = time . clock () print \"Sampler2\" print \"Using %d samples,\" % N print \"Non-antithetic: %.5f . ( %.5f .)\" % ( mci . estimate_N ( N ), mci . SE ()) print \"Duration: %.3f s.\" % ( time . clock () - start ) print legend ( loc = \"upper right\" ) /usr/local/lib/python2.7/dist-packages/IPython/kernel/__main__.py:15: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future /usr/local/lib/python2.7/dist-packages/IPython/kernel/__main__.py:19: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future Sampler1 Using 1000000 samples, 0.21254036049 Non-antithetic: 0.88171. (0.00046.) Duration: 0.170 s. Sampler2 Using 1000000 samples, 5.44010673961 Non-antithetic: 0.82234. (0.00233.) Duration: 2.520 s. Out[37]: <matplotlib.legend.Legend at 0xd1b97ac> In [38]: # An Example of the importance sampling with the original target mysampler3 = Importance ( target , interval ) hist ( mysampler3 . rvs ( 10000 ), label = \"original\" ) # to plot distribution # normalization of the sampler normalization3 = MCIntegrator ( target , interval = interval , b_antithetic = False , sampling_dist = stats . uniform ( * interval )) . estimate_N ( 1e6 ) _targetPrime3 = lambda x : targetPrime ( x , target , target , interval , normalization3 ) print normalization3 mci = MCIntegrator ( _targetPrime3 , interval = interval , b_antithetic = False , sampling_dist = mysampler3 , verbose = True ) N = 1e7 start = time . clock () print \"Original Target\" print \"Using %d samples,\" % N print \"Non-antithetic: %.5f . ( %.5f .)\" % ( mci . estimate_N ( N ), mci . SE ()) print \"Duration: %.3f s.\" % ( time . clock () - start ) print legend ( loc = \"upper right\" ) /usr/local/lib/python2.7/dist-packages/IPython/kernel/__main__.py:15: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future /usr/local/lib/python2.7/dist-packages/IPython/kernel/__main__.py:19: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future 0.881972798098 Original Target Using 10000000 samples, 3.86541843558e-29 Non-antithetic: 0.75968. (0.00000.) Duration: 3.480 s. Out[38]: <matplotlib.legend.Legend at 0xd1b9b4c> As it was expected, different samplers give different results. The obtained results show that the simpler sampler is, the better performance can be achieved. $$\\\\[5pt]$$ 0.6.2 Monte-Carlo with Stratified Sampling $$\\\\[5pt]$$ Next our task is to implement the stratified sampling (WikiStratifiedSampling, 2016) in the Monte Carlo estimator. Why do we need to stratify sample at all? This can improve the accuracy of the Monte Carlo estimation. Let's integrate our target function in the 5 regions as $0 In [39]: from operator import isub import scipy.integrate as integrate def target ( u ): return np . exp ( - u ** 2 ) * 2 def validation_of_integration ( target , interval , val ): ''' compare results obtained by MC and using a technique from the Fortran library QUADPACK. ''' result_quad = integrate . quad ( target , * interval )[ 0 ] print \"QUADPACK: %.5f and MC: %r \" % ( result_quad , val [ 0 ]) print \"MC Error: %r \" % ( val [ 1 ]) return val [ 0 ], result_quad def MCEstimation ( target , interval , N = 1e6 ): mci = MCIntegrator ( target , interval = interval , b_antithetic = False , sampling_dist = stats . uniform ( * interval ), verbose = True ) return - mci . estimate_N ( N ) * isub ( * interval ), mci . SE () # Integrate the target function in the 3 sub-regions print \"3 sub-regions\" intervals = [( 0 , 0.5 ),( 0.5 , 1.5 ),( 1.5 , 2 )] integralMC = 0 integralQUAD = 0 for interval in intervals : print _tmp1 , _tmp2 = validation_of_integration ( target , interval , MCEstimation ( target , interval )) integralMC += _tmp1 integralQUAD += _tmp2 print print \"Result MC is \" , integralMC print \"Result QUAD is \" , integralQUAD print print \"2 sub-regions\" intervals = [( 0 , 1 ),( 1 , 2 )] integralMC = 0 integralQUAD = 0 for interval in intervals : print _tmp1 , _tmp2 = validation_of_integration ( target , interval , MCEstimation ( target , interval )) integralMC += _tmp1 integralQUAD += _tmp2 print print \"Result MC is \" , integralMC print \"Result QUAD is \" , integralQUAD print print \"1 region\" intervals = [( 0 , 2 )] integralMC = 0 integralQUAD = 0 for interval in intervals : print _tmp1 , _tmp2 = validation_of_integration ( target , interval , MCEstimation ( target , interval )) integralMC += _tmp1 integralQUAD += _tmp2 print print \"Result MC is \" , integralMC print \"Result QUAD is \" , integralQUAD 3 sub-regions 0.018022824605 QUADPACK: 0.92256 and MC: 0.92266051398506843 MC Error: 0.00013424918110660584 0.215369406245 QUADPACK: 0.78981 and MC: 0.56114633640158273 MC Error: 0.00046407932685548262 0.00248147688842 QUADPACK: 0.05179 and MC: 0.015017947367964572 MC Error: 4.9814449408738103e-05 Result MC is 1.49882479775 Result QUAD is 1.76416278152 2 sub-regions 0.161585774809 QUADPACK: 1.49365 and MC: 1.4939922544849218 MC Error: 0.00040197753220198699 0.0377347962115 QUADPACK: 0.27051 and MC: 0.1397071083441512 MC Error: 0.00019425455965400439 Result MC is 1.63369936283 Result QUAD is 1.76416278152 1 region 0.474715539751 QUADPACK: 1.76416 and MC: 1.7633504652270953 MC Error: 0.00068899638204205304 Result MC is 1.76335046523 Result QUAD is 1.76416278152 It is clearly visible that Monte Carlo is inaccurate in the region of $0.5 stratified.py, for this purpose. In [169]: ! wget https://gist.githubusercontent.com/spacelis/6088623/raw/1c71eeb3ab0548e33bae99afacaacd9963438664/stratified.py --2016-04-13 16:54:41-- https://gist.githubusercontent.com/spacelis/6088623/raw/1c71eeb3ab0548e33bae99afacaacd9963438664/stratified.py Resolving gist.githubusercontent.com (gist.githubusercontent.com)... 23.235.43.133 Connecting to gist.githubusercontent.com (gist.githubusercontent.com)|23.235.43.133|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 2186 (2.1K) [text/plain] Saving to: `stratified.py' 100%[======================================>] 2,186 --.-K/s in 0s 2016-04-13 16:54:43 (31.3 MB/s) - `stratified.py' saved [2186/2186] In [51]: # %load stratified.py #!/usr/bin/env python \"\"\" File: stratified.py Author: SpaceLis Email: Wen.Li@tudelft.nl Github: http://github.com/spacelis Description: Sampling in a stratified way. That is sampling from each subpopulation to make the sample set more representative than simple random sampling. For example, a population of places from each category is not uniform, it is needed to insure each category has a place sampled and the number of the samples from each category should be propotional to its size. \"\"\" import numpy as np from itertools import tee , izip , chain from collections import Counter def pairwise ( iterable ): \"s -> (s0,s1), (s1,s2), (s2, s3), ...\" a , b = tee ( iterable ) next ( b , None ) return izip ( a , b ) def int_partition ( K , percentages , minimum = 1 ): \"\"\" Scale the percentages up K times \"\"\" assert K > minimum * len ( percentages ), 'K is too small for partitioning' p = np . array ( percentages , dtype = np . float ) dist = np . ones ( len ( p ), dtype = np . float ) * minimum dist += ( K - len ( p )) * p left = K - dist . sum () if left > 0 : while left > 0 : diff = p * K - dist dist [ np . argmax ( diff )] += 1 left -= 1 elif left < 0 : # FIXME seems never having chance to run actually while left < 0 : diff = p * K - dist dist [ np . argmin ( diff )] -= 1 left += 1 return dist def stratified_samples ( iterable , percentages , size , prop_sizes = True , replace = False ): \"\"\" Sampling the data with stratified sampling \"\"\" partitions = len ( percentages ) dist = sorted ( Counter ( iterable ) . iteritems (), key = lambda x : x [ 1 ], reverse = True ) if prop_sizes : samplesize = int_partition ( float ( size ), percentages ) else : samplesize = int_partition ( size , [ 1. / partitions ] * partitions ) pivots = list ( np . cumsum ( int_partition ( len ( dist ), percentages ))) chozen_idx = [ np . random . choice ( range ( int ( s ), int ( t )), n , replace = replace ) for n , ( s , t ) in zip ( samplesize , pairwise ([ 0 ] + pivots ))] chozen = [[ dist [ i ][ 0 ] for i in ig ] for ig in chozen_idx ] return chozen In [54]: ''' Stratified Sampling Igor Marfin @2014 <Unister Gmbh> ''' interval = ( 0 , 2 ) intervals = [( 0 , 0.5 ),( 0.5 , 1.5 ),( 1.5 , 2 )] x = stats . uniform ( * interval ) . rvs ( 10000 ) subsamples = [] func_vals = [] for interval in intervals : condition = ( x >= interval [ 0 ]) & ( x <= interval [ 1 ]) subsamples += [ x [ condition ] . tolist ()] func_vals += [ target ( x [ condition ]) . tolist ()] #func_vals+=target(x[condition]).tolist() import itertools import operator subsamplesmerged = list ( itertools . chain . from_iterable ([ zip ( b , a ) for a , b in zip ( subsamples , func_vals )])) a = stratified_samples ( subsamplesmerged ,[ 0.8 , 0.1 , 0.1 ], 1000 ) for i , item in enumerate ( a ): print \"Average in %d region is %f \" % ( i , np . average ( np . array ( map ( operator . itemgetter ( 0 ), item )))) Average in 0 region is 0.863028 Average in 1 region is 0.874898 Average in 2 region is 0.813675 /usr/local/lib/python2.7/dist-packages/IPython/kernel/__main__.py:64: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future and it is obvious that it only works for categorical values and not numbers. $$\\\\[5pt]$$ The adaptive Monte Carlo VEGAS algorithm $$\\\\[5pt]$$ There is a method implementing the adaptive stratified sampling for Monte Carlo simulations. This method is called VEGAS algorithm. You can install python version of the VEGAS algorithm (G.P.Lepage, 2015) . In [221]: ! sudo pip install vegas [sudo] password for debian: If we repeat the MC integration of the target function with VEGAS, we can build the adaptive grid of the stratified sampling used in this MC. The example below shows that adaptive stratified sampling improves Monte-Carlo estimation dramatically. In [62]: ''' An example of the application of the VEGAS algorithm ''' import vegas # a problematic interval interval = ( 0.5 , 1.5 ) def target ( u ): return np . exp ( - u ** 2 ) * 2 integ = vegas . Integrator ([ interval ]) result = integ ( target , nitn = 5 , neval = 1000 ) print ( result . summary ()) #print('result = %s Q = %.2f' % (result, result.Q)) validation_of_integration ( target , interval ,( result , result . Q )) # show the adaptive grid integ . map . show_grid ( 40 ) itn integral wgt average chi2/dof Q ------------------------------------------------------- 1 0.789791(70) 0.789791(70) 0.00 1.00 2 0.789910(65) 0.789855(48) 1.55 0.21 3 0.789973(66) 0.789895(39) 1.82 0.16 4 0.789858(68) 0.789886(34) 1.29 0.28 5 0.789829(72) 0.789876(30) 1.10 0.35 QUADPACK: 0.78981 and MC: RAvgArray([0.789876(30)], dtype=object) MC Error: 0.35462157595551524 That's it for this lecture. $$\\\\[5pt]$$ Plans for next tutorials $$\\\\[5pt]$$ A short `to-do list for my future lectures is the following: Introduction to Strategies of the option trading like Covered Call Protective Put Option Strangle Butterfly spread etc Developing the self-learned machinery to make decisions following some option strategy Miscellaneous In [198]: ### Do not delete the following Markdown section! ### This is the BibTeX references! References &#94; &#94; &#94; Cam Davidson-Pilon,. 2012. STAT906 - Assignemtn Nr.3 . URL &#94; Wiki:SDE,. 2016. Stochastic differential equation . URL &#94; Wiki:Ito Lemmma,. 2016. _Itô's lemma . URL &#94; &#94; Wiki:option,. 2016. Call Option . URL &#94; Jan Palcyewski,. 2016. Lecture5: Numerical schemes for SDEs . URL &#94; Investopedia,. 2016. The Basics Of The Bid-Ask Spread . URL &#94; WikiVolatility,. 2016. Volatility in finance . URL &#94; Wiki Black–Scholes formula,. 2016. Black–Scholes formula . URL &#94; Jessi Cisewski,. 2014. Importance Sampling . URL &#94; Wiki Stratified Sampling,. 2016. Stratified Sampling . URL &#94; G.P.Lepage,. 2015. Adaptive Monte Carlo vegas algorithm . URL if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML'; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: 'center',\" + \" displayIndent: '0em',\" + \" showMathMenu: true,\" + \" tex2jax: { \" + \" inlineMath: [ ['$','$'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }\" + \" } \" + \"}); \"; (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Statistics","title":"Application of the Monte Carlo integration in Options trading (part I)"},{"url":"http://igormarfin.github.io/blog/2016/02/07/a-post-with-an-image/","text":"Sometimes you would like to post articles with images. What you can do is the following make post_with_image # to test the html output make html make serve # to publish to github.com make publish make post_with_image command. Basicaly, it performs make add_entry_rst A post with an Image,blog \\\\\\, post \\\\\\, image,BuildingThisBlog cat how-to-post-with-image.rst >> $( BLOG_SOURCE ) /content/*a-post-with-an-image.rst mkdir -p $( BLOG_SOURCE ) /content/images cp pelican-plugin-post-stats-medium-example.png $( BLOG_SOURCE ) /content/images/ echo \" Here is my image .. figure:: /images/pelican-plugin-post-stats-medium-example.png :align: right This is the caption of the figure. \" >> $( BLOG_SOURCE ) /content/*a-post-with-an-image.rst Also it is important that your pelicanconf.py has images in the static path: STATIC_PATHS =[ 'images' ] Here is my image This is the caption of the figure. That's it.","tags":"BuildingThisBlog","title":"A post with an Image"},{"url":"http://igormarfin.github.io/blog/2016/02/07/probabilistic-matrix-factorization-for-making-personalized-recommendations/","text":"/*! * * IPython notebook * */.ansibold{font-weight:bold}.ansiblack{color:black}.ansired{color:darkred}.ansigreen{color:darkgreen}.ansiyellow{color:#c4a000}.ansiblue{color:darkblue}.ansipurple{color:darkviolet}.ansicyan{color:steelblue}.ansigray{color:gray}.ansibgblack{background-color:black}.ansibgred{background-color:red}.ansibggreen{background-color:green}.ansibgyellow{background-color:yellow}.ansibgblue{background-color:blue}.ansibgpurple{background-color:magenta}.ansibgcyan{background-color:cyan}.ansibggray{background-color:gray}div.cell{border:1px solid transparent;display:-webkit-box;-webkit-box-orient:vertical;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:vertical;-moz-box-align:stretch;display:box;box-orient:vertical;box-align:stretch;display:flex;flex-direction:column;align-items:stretch;border-radius:2px;box-sizing:border-box;-moz-box-sizing:border-box;-webkit-box-sizing:border-box;border-width:thin;border-style:solid;width:100%;padding:5px;margin:0;outline:none}div.cell.selected{border-color:#ababab}@media print{div.cell.selected{border-color:transparent}}.edit_mode div.cell.selected{border-color:green}@media print{.edit_mode div.cell.selected{border-color:transparent}}.prompt{min-width:14ex;padding:.4em;margin:0;font-family:monospace;text-align:right;line-height:1.21429em}@media (max-width:540px){.prompt{text-align:left}}div.inner_cell{display:-webkit-box;-webkit-box-orient:vertical;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:vertical;-moz-box-align:stretch;display:box;box-orient:vertical;box-align:stretch;display:flex;flex-direction:column;align-items:stretch;-webkit-box-flex:1;-moz-box-flex:1;box-flex:1;flex:1}@-moz-document url-prefix(){div.inner_cell{overflow-x:hidden}}div.input_area{border:1px solid #cfcfcf;border-radius:2px;background:#f7f7f7;line-height:1.21429em}div.prompt:empty{padding-top:0;padding-bottom:0}div.unrecognized_cell{padding:5px 5px 5px 0;display:-webkit-box;-webkit-box-orient:horizontal;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:horizontal;-moz-box-align:stretch;display:box;box-orient:horizontal;box-align:stretch;display:flex;flex-direction:row;align-items:stretch}div.unrecognized_cell .inner_cell{border-radius:2px;padding:5px;font-weight:bold;color:red;border:1px solid #cfcfcf;background:#eaeaea}div.unrecognized_cell .inner_cell a{color:inherit;text-decoration:none}div.unrecognized_cell .inner_cell a:hover{color:inherit;text-decoration:none}@media (max-width:540px){div.unrecognized_cell>div.prompt{display:none}}@media print{div.code_cell{page-break-inside:avoid}}div.input{page-break-inside:avoid;display:-webkit-box;-webkit-box-orient:horizontal;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:horizontal;-moz-box-align:stretch;display:box;box-orient:horizontal;box-align:stretch;display:flex;flex-direction:row;align-items:stretch}@media (max-width:540px){div.input{display:-webkit-box;-webkit-box-orient:vertical;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:vertical;-moz-box-align:stretch;display:box;box-orient:vertical;box-align:stretch;display:flex;flex-direction:column;align-items:stretch}}div.input_prompt{color:navy;border-top:1px solid transparent}div.input_area>div.highlight{margin:.4em;border:none;padding:0;background-color:transparent}div.input_area>div.highlight>pre{margin:0;border:none;padding:0;background-color:transparent}.CodeMirror{line-height:1.21429em;font-size:14px;height:auto;background:none}.CodeMirror-scroll{overflow-y:hidden;overflow-x:auto}.CodeMirror-lines{padding:.4em}.CodeMirror-linenumber{padding:0 8px 0 4px}.CodeMirror-gutters{border-bottom-left-radius:2px;border-top-left-radius:2px}.CodeMirror pre{padding:0;border:0;border-radius:0}.highlight-base{}.highlight-variable{}.highlight-variable-2{color:#1a1a1a}.highlight-variable-3{color:#333}.highlight-string{color:#ba2121}.highlight-comment{color:#408080;font-style:italic}.highlight-number{80}.highlight-atom{color:#88f}.highlight-keyword{8000;font-weight:bold}.highlight-builtin{8000}.highlight-error{color:#f00}.highlight-operator{color:#a2f;font-weight:bold}.highlight-meta{color:#a2f}.highlight-def{f}.highlight-string-2{color:#f50}.highlight-qualifier{color:#555}.highlight-bracket{color:#997}.highlight-tag{color:#170}.highlight-attribute{c}.highlight-header{color:blue}.highlight-quote{90}.highlight-link{c}.cm-s-ipython span.cm-keyword{8000;font-weight:bold}.cm-s-ipython span.cm-atom{color:#88f}.cm-s-ipython span.cm-number{80}.cm-s-ipython span.cm-def{f}.cm-s-ipython span.cm-variable{}.cm-s-ipython span.cm-operator{color:#a2f;font-weight:bold}.cm-s-ipython span.cm-variable-2{color:#1a1a1a}.cm-s-ipython span.cm-variable-3{color:#333}.cm-s-ipython span.cm-comment{color:#408080;font-style:italic}.cm-s-ipython span.cm-string{color:#ba2121}.cm-s-ipython span.cm-string-2{color:#f50}.cm-s-ipython span.cm-meta{color:#a2f}.cm-s-ipython span.cm-qualifier{color:#555}.cm-s-ipython span.cm-builtin{8000}.cm-s-ipython span.cm-bracket{color:#997}.cm-s-ipython span.cm-tag{color:#170}.cm-s-ipython span.cm-attribute{c}.cm-s-ipython span.cm-header{color:blue}.cm-s-ipython span.cm-quote{90}.cm-s-ipython span.cm-link{c}.cm-s-ipython span.cm-error{color:#f00}.cm-s-ipython span.cm-tab{background:url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADAAAAAMCAYAAAAkuj5RAAAAAXNSR0IArs4c6QAAAGFJREFUSMft1LsRQFAQheHPowAKoACx3IgEKtaEHujDjORSgWTH/ZOdnZOcM/sgk/kFFWY0qV8foQwS4MKBCS3qR6ixBJvElOobYAtivseIE120FaowJPN75GMu8j/LfMwNjh4HUpwg4LUAAAAASUVORK5CYII=);background-position:right;background-repeat:no-repeat}div.output_wrapper{position:relative;display:-webkit-box;-webkit-box-orient:vertical;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:vertical;-moz-box-align:stretch;display:box;box-orient:vertical;box-align:stretch;display:flex;flex-direction:column;align-items:stretch}div.output_scroll{height:24em;width:100%;overflow:auto;border-radius:2px;-webkit-box-shadow:inset 0 2px 8px rgba(0,0,0,0.8);box-shadow:inset 0 2px 8px rgba(0,0,0,0.8);display:block}div.output_collapsed{margin:0;padding:0;display:-webkit-box;-webkit-box-orient:vertical;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:vertical;-moz-box-align:stretch;display:box;box-orient:vertical;box-align:stretch;display:flex;flex-direction:column;align-items:stretch}div.out_prompt_overlay{height:100%;padding:0 .4em;position:absolute;border-radius:2px}div.out_prompt_overlay:hover{-webkit-box-shadow:inset 0 0 1px #000;box-shadow:inset 0 0 1px #000;background:rgba(240,240,240,0.5)}div.output_prompt{color:darkred}div.output_area{padding:0;page-break-inside:avoid;display:-webkit-box;-webkit-box-orient:horizontal;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:horizontal;-moz-box-align:stretch;display:box;box-orient:horizontal;box-align:stretch;display:flex;flex-direction:row;align-items:stretch}div.output_area .MathJax_Display{text-align:left !important}div.output_area div.output_area .output{display:-webkit-box;-webkit-box-orient:vertical;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:vertical;-moz-box-align:stretch;display:box;box-orient:vertical;box-align:stretch;display:flex;flex-direction:column;align-items:stretch}@media (max-width:540px){div.output_area{display:-webkit-box;-webkit-box-orient:vertical;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:vertical;-moz-box-align:stretch;display:box;box-orient:vertical;box-align:stretch;display:flex;flex-direction:column;align-items:stretch}}div.output_area pre{margin:0;padding:0;border:0;vertical-align:baseline;color:black;background-color:transparent;border-radius:0}div.output_subarea{padding:.4em;-webkit-box-flex:1;-moz-box-flex:1;box-flex:1;flex:1}div.output_text{text-align:left;line-height:1.21429em}div.output_stderr{background:#fdd}div.output_latex{text-align:left}div.output_javascript:empty{padding:0}.js-error{color:darkred}div.raw_input_container{font-family:monospace;padding-top:5px}input.raw_input{font-family:inherit;font-size:inherit;color:inherit;width:auto;vertical-align:baseline;padding:0 .25em;margin:0 .25em}input.raw_input:focus{box-shadow:none}p.p-space{margin-bottom:10px}div.output_unrecognized{padding:5px;font-weight:bold;color:red}div.output_unrecognized a{color:inherit;text-decoration:none}div.output_unrecognized a:hover{color:inherit;text-decoration:none}.rendered_html{}.rendered_html :link{text-decoration:underline}.rendered_html :visited{text-decoration:underline}.rendered_html h1:first-child{margin-top:.538em}.rendered_html h2:first-child{margin-top:.636em}.rendered_html h3:first-child{margin-top:.777em}.rendered_html h4:first-child{margin-top:1em}.rendered_html h5:first-child{margin-top:1em}.rendered_html h6:first-child{margin-top:1em}.rendered_html *+ul{margin-top:1em}.rendered_html *+ol{margin-top:1em}.rendered_html *+table{margin-top:1em}.rendered_html *+p{margin-top:1em}.rendered_html *+img{margin-top:1em}div.text_cell{display:-webkit-box;-webkit-box-orient:horizontal;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:horizontal;-moz-box-align:stretch;display:box;box-orient:horizontal;box-align:stretch;display:flex;flex-direction:row;align-items:stretch}@media (max-width:540px){div.text_cell>div.prompt{display:none}}div.text_cell_render{outline:none;resize:none;width:inherit;border-style:none;padding:.5em .5em .5em .4em;box-sizing:border-box;-moz-box-sizing:border-box;-webkit-box-sizing:border-box}a.anchor-link:link{text-decoration:none;padding:0 20px;visibility:hidden}h1:hover .anchor-link,h2:hover .anchor-link,h3:hover .anchor-link,h4:hover .anchor-link,h5:hover .anchor-link,h6:hover .anchor-link{visibility:visible}.text_cell.rendered .input_area{display:none}.text_cell.unrendered .text_cell_render{display:none}.cm-header-1,.cm-header-2,.cm-header-3,.cm-header-4,.cm-header-5,.cm-header-6{font-weight:bold;font-family:\"Helvetica Neue\",Helvetica,Arial,sans-serif}.cm-header-1{font-size:185.7%}.cm-header-2{font-size:157.1%}.cm-header-3{font-size:128.6%}.cm-header-4{font-size:110%}.cm-header-5{font-size:100%;font-style:italic}.cm-header-6{font-size:100%;font-style:italic}.widget-interact>div,.widget-interact>input{padding:2.5px}.widget-area{page-break-inside:avoid;display:-webkit-box;-webkit-box-orient:horizontal;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:horizontal;-moz-box-align:stretch;display:box;box-orient:horizontal;box-align:stretch;display:flex;flex-direction:row;align-items:stretch}.widget-area .widget-subarea{padding:.44em .4em .4em 1px;margin-left:6px;box-sizing:border-box;-moz-box-sizing:border-box;-webkit-box-sizing:border-box;display:-webkit-box;-webkit-box-orient:vertical;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:vertical;-moz-box-align:stretch;display:box;box-orient:vertical;box-align:stretch;display:flex;flex-direction:column;align-items:stretch;-webkit-box-flex:2;-moz-box-flex:2;box-flex:2;flex:2;-webkit-box-align:start;-moz-box-align:start;box-align:start;align-items:flex-start}.widget-area.connection-problems .prompt:after{content:\"\\f127\";font-family:'FontAwesome';color:#d9534f;font-size:14px;top:3px;padding:3px}.slide-track{border:1px solid #ccc;background:#fff;border-radius:2px}.widget-hslider{padding-left:8px;padding-right:2px;overflow:visible;width:350px;height:5px;max-height:5px;margin-top:13px;margin-bottom:10px;border:1px solid #ccc;background:#fff;border-radius:2px;display:-webkit-box;-webkit-box-orient:horizontal;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:horizontal;-moz-box-align:stretch;display:box;box-orient:horizontal;box-align:stretch;display:flex;flex-direction:row;align-items:stretch}.widget-hslider .ui-slider{border:0;background:none;display:-webkit-box;-webkit-box-orient:horizontal;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:horizontal;-moz-box-align:stretch;display:box;box-orient:horizontal;box-align:stretch;display:flex;flex-direction:row;align-items:stretch;-webkit-box-flex:1;-moz-box-flex:1;box-flex:1;flex:1}.widget-hslider .ui-slider .ui-slider-handle{width:12px;height:28px;margin-top:-8px;border-radius:2px}.widget-hslider .ui-slider .ui-slider-range{height:12px;margin-top:-4px;background:#eee}.widget-vslider{padding-bottom:5px;overflow:visible;width:5px;max-width:5px;height:250px;margin-left:12px;border:1px solid #ccc;background:#fff;border-radius:2px;display:-webkit-box;-webkit-box-orient:vertical;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:vertical;-moz-box-align:stretch;display:box;box-orient:vertical;box-align:stretch;display:flex;flex-direction:column;align-items:stretch}.widget-vslider .ui-slider{border:0;background:none;margin-left:-4px;margin-top:5px;display:-webkit-box;-webkit-box-orient:vertical;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:vertical;-moz-box-align:stretch;display:box;box-orient:vertical;box-align:stretch;display:flex;flex-direction:column;align-items:stretch;-webkit-box-flex:1;-moz-box-flex:1;box-flex:1;flex:1}.widget-vslider .ui-slider .ui-slider-handle{width:28px;height:12px;margin-left:-9px;border-radius:2px}.widget-vslider .ui-slider .ui-slider-range{width:12px;margin-left:-1px;background:#eee}.widget-text{width:350px;margin:0}.widget-listbox{width:350px;margin-bottom:0}.widget-numeric-text{width:150px;margin:0}.widget-progress{margin-top:6px;min-width:350px}.widget-progress .progress-bar{-webkit-transition:none;-moz-transition:none;-ms-transition:none;-o-transition:none;transition:none}.widget-combo-btn{min-width:125px}.widget_item .dropdown-menu li a{color:inherit}.widget-hbox{display:-webkit-box;-webkit-box-orient:horizontal;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:horizontal;-moz-box-align:stretch;display:box;box-orient:horizontal;box-align:stretch;display:flex;flex-direction:row;align-items:stretch}.widget-hbox input[type=\"checkbox\"]{margin-top:9px;margin-bottom:10px}.widget-hbox .widget-label{min-width:10ex;padding-right:8px;padding-top:5px;text-align:right;vertical-align:text-top}.widget-hbox .widget-readout{padding-left:8px;padding-top:5px;text-align:left;vertical-align:text-top}.widget-vbox{display:-webkit-box;-webkit-box-orient:vertical;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:vertical;-moz-box-align:stretch;display:box;box-orient:vertical;box-align:stretch;display:flex;flex-direction:column;align-items:stretch}.widget-vbox .widget-label{padding-bottom:5px;text-align:center;vertical-align:text-bottom}.widget-vbox .widget-readout{padding-top:5px;text-align:center;vertical-align:text-top}.widget-box{box-sizing:border-box;-moz-box-sizing:border-box;-webkit-box-sizing:border-box;-webkit-box-align:start;-moz-box-align:start;box-align:start;align-items:flex-start}.widget-radio-box{display:-webkit-box;-webkit-box-orient:vertical;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:vertical;-moz-box-align:stretch;display:box;box-orient:vertical;box-align:stretch;display:flex;flex-direction:column;align-items:stretch;box-sizing:border-box;-moz-box-sizing:border-box;-webkit-box-sizing:border-box;padding-top:4px}.widget-radio-box label{margin-top:0}.widget-radio{margin-left:20px} .highlight .hll { background-color: #ffffcc } .highlight { background: #f8f8f8; } .highlight .c { color: #408080; font-style: italic } /* Comment */ .highlight .err { border: 1px solid #FF0000 } /* Error */ .highlight .k { color: #008000; font-weight: bold } /* Keyword */ .highlight .o { color: #666666 } /* Operator */ .highlight .cm { color: #408080; font-style: italic } /* Comment.Multiline */ .highlight .cp { color: #BC7A00 } /* Comment.Preproc */ .highlight .c1 { color: #408080; font-style: italic } /* Comment.Single */ .highlight .cs { color: #408080; font-style: italic } /* Comment.Special */ .highlight .gd { color: #A00000 } /* Generic.Deleted */ .highlight .ge { font-style: italic } /* Generic.Emph */ .highlight .gr { color: #FF0000 } /* Generic.Error */ .highlight .gh { color: #000080; font-weight: bold } /* Generic.Heading */ .highlight .gi { color: #00A000 } /* Generic.Inserted */ .highlight .go { color: #888888 } /* Generic.Output */ .highlight .gp { color: #000080; font-weight: bold } /* Generic.Prompt */ .highlight .gs { font-weight: bold } /* Generic.Strong */ .highlight .gu { color: #800080; font-weight: bold } /* Generic.Subheading */ .highlight .gt { color: #0044DD } /* Generic.Traceback */ .highlight .kc { color: #008000; font-weight: bold } /* Keyword.Constant */ .highlight .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */ .highlight .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */ .highlight .kp { color: #008000 } /* Keyword.Pseudo */ .highlight .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */ .highlight .kt { color: #B00040 } /* Keyword.Type */ .highlight .m { color: #666666 } /* Literal.Number */ .highlight .s { color: #BA2121 } /* Literal.String */ .highlight .na { color: #7D9029 } /* Name.Attribute */ .highlight .nb { color: #008000 } /* Name.Builtin */ .highlight .nc { color: #0000FF; font-weight: bold } /* Name.Class */ .highlight .no { color: #880000 } /* Name.Constant */ .highlight .nd { color: #AA22FF } /* Name.Decorator */ .highlight .ni { color: #999999; font-weight: bold } /* Name.Entity */ .highlight .ne { color: #D2413A; font-weight: bold } /* Name.Exception */ .highlight .nf { color: #0000FF } /* Name.Function */ .highlight .nl { color: #A0A000 } /* Name.Label */ .highlight .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */ .highlight .nt { color: #008000; font-weight: bold } /* Name.Tag */ .highlight .nv { color: #19177C } /* Name.Variable */ .highlight .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */ .highlight .w { color: #bbbbbb } /* Text.Whitespace */ .highlight .mb { color: #666666 } /* Literal.Number.Bin */ .highlight .mf { color: #666666 } /* Literal.Number.Float */ .highlight .mh { color: #666666 } /* Literal.Number.Hex */ .highlight .mi { color: #666666 } /* Literal.Number.Integer */ .highlight .mo { color: #666666 } /* Literal.Number.Oct */ .highlight .sb { color: #BA2121 } /* Literal.String.Backtick */ .highlight .sc { color: #BA2121 } /* Literal.String.Char */ .highlight .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */ .highlight .s2 { color: #BA2121 } /* Literal.String.Double */ .highlight .se { color: #BB6622; font-weight: bold } /* Literal.String.Escape */ .highlight .sh { color: #BA2121 } /* Literal.String.Heredoc */ .highlight .si { color: #BB6688; font-weight: bold } /* Literal.String.Interpol */ .highlight .sx { color: #008000 } /* Literal.String.Other */ .highlight .sr { color: #BB6688 } /* Literal.String.Regex */ .highlight .s1 { color: #BA2121 } /* Literal.String.Single */ .highlight .ss { color: #19177C } /* Literal.String.Symbol */ .highlight .bp { color: #008000 } /* Name.Builtin.Pseudo */ .highlight .vc { color: #19177C } /* Name.Variable.Class */ .highlight .vg { color: #19177C } /* Name.Variable.Global */ .highlight .vi { color: #19177C } /* Name.Variable.Instance */ .highlight .il { color: #666666 } /* Literal.Number.Integer.Long */ Probabilistic Matrix Factorization for Making Personalized Recommendations Igor Marfin igor.marfin@unister.de Abstract The model discussed in this analysis was presented in the PyMC3 tutorial page . The aim of the work is implementaion and testing the Probabilistic Matrix Factorization model in terms of the Bayesian network with a help of the PyMC2. More details can be found at https://bitbucket.org/iggy_floyd/probabilistic-matrix-factorization . Motivation Say I download a handbook of a hundred jokes, and I'd like to know very quickly which ones will be my favorite. So maybe I read a few, I laugh, I read a few more, I stop laughing, and I indicate on a scale of -10 to 10 how funny I thought each joke was. Maybe I do this for 5 jokes out of the 100. Now I go to the back of the book, and there's a little program included for calculating my preferences for all the other jokes. I enter in my preference numbers and shazam! The program spits out a list of all 100 jokes, sorted in the order I'll like them. Some thoughts I split the making recomendations procedure in four ideas given in the tutorial: preparation for a recommendation: introduction of the ratings for each joke in different categories Normally if we want recommendations for something, we try to find people who are similar to us and ask their opinions. If Bob, Alice, and Monty are all similar to me, and they all like knock-knock jokes, I'll probably like knock-knock jokes. Now this isn't always true. It depends on what we consider to be \"similar\". In order to get the best bang for our buck, we really want to look for people who have the most similar sense of humor. Humor being a complex beast, we'd probably like to break it down into something more understandable. We might try to characterize each joke in terms of various factors. Perhaps jokes can be dry, sarcastic, crude, sexual, political, etc. Now imagine we go through our handbook of jokes and assign each joke a rating in each of the categories. How dry is it? How sarcastic is it? How much does it use sexual innuendos? Perhaps we use numbers between 0 and 1 for each category. Intuitively, we might call this the joke's humor profile. a social recommendation: make a decision using the recommendation from the closest social individuals (based on user-user similarity) Now let's suppose we go back to those 5 jokes we rated. At this point, we can get a richer picture of our own preferences by looking at the humor profiles of each of the jokes we liked and didn't like. Perhaps we take the averages across the 5 humor profiles and call this our ideal type of joke. In other words, we have computed some notion of our inherent preferences for various types of jokes. Suppose Bob, Alice, and Monty all do the same. Now we can compare our preferences and determine how similar each of us really are. I might find that Bob is the most similar and the other two are still more similar than other people, but not as much as Bob. So I want recommendations from all three people, but when I make my final decision, I'm going to put more weight on Bob's recommendation than those I get from Alice and Monty. an individual recommendation: make a decision using ourselves recommendations with the closest humor profiles (based on item-item similarity) While the above procedure sounds fairly effective as is, it also reveals an unexpected additional source of information. If we rated a particular joke highly, and we know its humor profile, we can compare with the profiles of other jokes. If we find one with very close numbers, it is probable we'll also enjoy this joke. Both this approach and the one above are commonly known as neighborhood approaches. Techniques that leverage both of these approaches simultaneously are often called collaborative filtering. combination of the two previous recommendation strategies Ideally, we'd like to use both sources of information. The idea is we have a lot of items available to us, and we'd like to work together with others to filter the list of items down to those we'll each like best. My list should have the items I'll like best at the top and those I'll like least at the bottom. Everyone else wants the same. If I get together with a bunch of other people, we all read 5 jokes, and we have some efficient computational process to determine similarity, we can very quickly order the jokes to our liking. Formalization A little bit of mathematics I begin with introduction of some notations used later in the code. They are $R$,$R&#94;*$, $U$, $V$,$I$. Let's take some time to make the intuitive notions we've been discussing more concrete. We have a set of M jokes, or items (M=100 in our example above). We also have N people, whom we'll call users of our recommender system. For each item, we'd like to find a D dimensional factor composition (humor profile above) to describe the item. Ideally, we'd like to do this without actually going through and manually labeling all of the jokes. Manual labeling would be both slow and error-prone, as different people will likely label jokes differently. So we model each joke as a D dimensional vector, which is its latent factor composition. Furthermore, we expect each user to have some preferences, but without our manual labeling and averaging procedure, we have to rely on the latent factor compositions to learn D dimensional latent preference vectors for each user. The only thing we get to observe is the N×M ratings matrix R provided by the users. Important to remember: Entry $R_{ij}$ is the rating user $i$ gave to item $j$. Many of these entries may be missing, since most users will not have rated all 100 jokes. Our goal is to fill in the missing values with predicted ratings based on the latent variables U and V. We denote the predicted ratings by $R&#94;∗_{ij}$. Important to remember: calculation of $R&#94;∗_{ij}$ is our aim, this what we model and estimate. We also define an indicator matrix I, with entry $I_{ij}=0$ if $R_{ij}$ is missing and $I_{ij}=1$ otherwise. Important: please carefully read the paragraph below So we have an N×D matrix of user preferences which we'll call U and an M×D factor composition matrix we'll call V. We also have a N×M rating matrix we'll call R. We can think of each row $U_i$ as indications of how much each user prefers each of the D latent factors. Each row $V_j$ can be thought of as how much each item can be described by each of the latent factors. In order to make a recommendation, we need a suitable prediction function which maps a user preference vector $U_i$ and an item latent factor vector $V_j$ to a predicted ranking. The choice of this prediction function is an important modeling decision, and a variety of prediction functions have been used. Perhaps the most common is the dot product of the two vectors, $U_i\\cdot V_j$ Important to remember: We calculate $R&#94;∗_{ij}$ as $R&#94;∗_{ij} = U\\cdot V&#94;T$ Also you have to understand that we replace $R_{ij}$ by $R&#94;∗_{ij}$ when we are going to make a decision on the recommendation of the particular joke. Why we do that? Well, because, the matrix $R_{ij}$ usually is the sparse matrix with many NaN values and we use this minimalistic set of the data to get a fully determined new rating matrix $R&#94;*_{ij}$. Initialization In [1]: import sys sys . path = [ '/usr/local/lib/python2.7/dist-packages' ] + sys . path # to fix the problem with numpy: this replaces 1.6 version by 1.9 % matplotlib inline % pylab inline import os import matplotlib import numpy as np import matplotlib.pyplot as pl import matplotlib as mpl import logging import pymc as pm # a plotter and dataframe modules import seaborn as sns # seaborn to make a nice plots of the data import pandas as pd # use a nice style for plots and the notebook import json s = json . load ( open ( \"styles/my_matplotlibrc.json\" ) ) matplotlib . rcParams . update ( s ) from IPython.core.display import HTML from IPython.display import display , Math , Latex import urllib2 def css_styling (): styles = open ( \"styles/custom_v3.css\" , \"r\" ) . read () return HTML ( styles ) css_styling () #HTML( urllib2.urlopen('http://bit.ly/1Bf5Hft').read() ) ion () # Set up logging. logger = logging . getLogger () logger . setLevel ( logging . INFO ) Populating the interactive namespace from numpy and matplotlib /usr/local/lib/python2.7/dist-packages/matplotlib/__init__.py:855: UserWarning: svg.embed_char_paths is deprecated and replaced with svg.fonttype; please use the latter. warnings.warn(self.msg_depr % (key, alt_key)) Data The v1 Jester dataset provides something very much like the handbook of jokes we have been discussing. At this point in time, v1 contains over 4.1 million continuous ratings in the range [-10, 10] of 100 jokes from 73,421 users. These ratings were collected between Apr. 1999 and May 2003. In order to reduce the training time of the model for illustrative purposes, 1,000 users who have rated all 100 jokes will be selected randomly. Let's get our data. In [7]: ! mkdir data ! wget -O data/jester-dataset-v1-dense-first-1000.csv https://raw.githubusercontent.com/pymc-devs/pymc3/master/pymc3/examples/data/jester-dataset-v1-dense-first-1000.csv ! ls data --2015-05-28 11:10:57-- https://raw.githubusercontent.com/pymc-devs/pymc3/master/pymc3/examples/data/jester-dataset-v1-dense-first-1000.csv Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.31.17.133 Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.31.17.133|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 529543 (517K) [text/plain] Saving to: `data/jester-dataset-v1-dense-first-1000.csv' 100%[======================================>] 529,543 352K/s in 1.5s 2015-05-28 11:11:00 (352 KB/s) - `data/jester-dataset-v1-dense-first-1000.csv' saved [529543/529543] jester-dataset-v1-dense-first-1000.csv In [2]: data = pd . read_csv ( 'data/jester-dataset-v1-dense-first-1000.csv' ) data . head ( 10 ) Out[2]: 1 2 3 4 5 6 7 8 9 10 ... 91 92 93 94 95 96 97 98 99 100 0 4.08 -0.29 6.36 4.37 -2.38 -9.66 -0.73 -5.34 8.88 9.22 ... 2.82 -4.95 -0.29 7.86 -0.19 -2.14 3.06 0.34 -4.32 1.07 1 -6.17 -3.54 0.44 -8.50 -7.09 -4.32 -8.69 -0.87 -6.65 -1.80 ... -3.54 -6.89 -0.68 -2.96 -2.18 -3.35 0.05 -9.08 -5.05 -3.45 2 6.84 3.16 9.17 -6.21 -8.16 -1.70 9.27 1.41 -5.19 -4.42 ... 7.23 -1.12 -0.10 -5.68 -3.16 -3.35 2.14 -0.05 1.31 0.00 3 -3.79 -3.54 -9.42 -6.89 -8.74 -0.29 -5.29 -8.93 -7.86 -1.60 ... 4.37 -0.29 4.17 -0.29 -0.29 -0.29 -0.29 -0.29 -3.40 -4.95 4 1.31 1.80 2.57 -2.38 0.73 0.73 -0.97 5.00 -7.23 -1.36 ... 1.46 1.70 0.29 -3.30 3.45 5.44 4.08 2.48 4.51 4.66 5 9.22 9.27 9.22 8.30 7.43 0.44 3.50 8.16 5.97 8.98 ... 8.11 -1.02 5.58 6.84 5.53 -5.92 8.20 8.98 -8.16 6.50 6 8.79 -5.78 6.02 3.69 7.77 -5.83 8.69 8.59 -5.92 7.52 ... 2.72 -5.49 -8.59 8.69 -8.74 -3.01 8.30 -4.81 -2.38 -5.97 7 -3.50 1.55 2.33 -4.13 4.22 -2.28 -2.96 -0.49 2.91 1.99 ... 3.11 1.70 0.24 -5.92 7.28 -1.36 3.74 2.82 -2.86 3.45 8 3.16 7.62 3.79 8.25 4.22 7.62 2.43 0.97 0.53 0.83 ... 0.83 5.68 3.69 0.19 0.29 3.59 0.49 8.06 0.49 7.62 9 2.09 -7.57 4.17 -8.40 -6.31 5.34 -5.19 -5.05 -8.20 2.28 ... 3.06 8.25 8.50 -9.51 8.45 8.16 7.82 8.50 8.35 -8.35 10 rows × 100 columns How many ratings do we have? In [9]: print '# of ratings: %d ' % len (data) # of ratings: 1000 Now we want to get the distribution of ratings: we plot a histogram and get some statistics from it In [13]: # Extract the ratings from the DataFrame all_ratings = np . ndarray . flatten ( data . values ) ratings = pd . Series ( all_ratings ) # Plot a density. fig , ( ax1 , ax2 ) = plt . subplots ( 1 , 2 , figsize = ( 16 , 7 )) ratings . plot ( kind = 'density' , ax = ax1 , grid = False ) ax1 . set_ylim ( 0 , 0.08 ) ax1 . set_xlim ( - 11 , 11 ) # Plot a histogram ratings . plot ( kind = 'hist' , ax = ax2 , bins = 20 , grid = False ) ax2 . set_xlim ( - 11 , 11 ) plt . show () # get some statistics ratings . describe () Out[13]: count 100000.000000 mean 0.996219 std 5.265215 min -9.950000 25% -2.860000 50% 1.650000 75% 5.290000 max 9.420000 dtype: float64 So we can find that the mean of all ratings is close to 1 (almost neutral). The distribution of mean values ( the average on the number of ratings) among jokes is In [16]: # mean ratings for 100 jokes joke_means = data . mean ( axis = 0 ) joke_means . plot ( kind = 'bar' , grid = False , figsize = ( 16 , 6 ), title = \"Mean Ratings for All 100 Jokes\" ) Out[16]: <matplotlib.axes._subplots.AxesSubplot at 0xad40414c> While the majority of the jokes generally get positive feedback from users, there are definitely a few that stand out as poor humor. Let's take a look at the users means. In [17]: # user means user_means = data . mean ( axis = 1 ) fig , ax = plt . subplots ( figsize = ( 16 , 6 )) user_means . plot ( kind = 'bar' , grid = False , ax = ax , title = \"Mean Ratings for All 1000 Users\" ) ax . set_xticklabels ( '' ) # 1000 labels is nonsensical fig . show () /usr/local/lib/python2.7/dist-packages/matplotlib/figure.py:387: UserWarning: matplotlib is currently using a non-GUI backend, so cannot show the figure \"matplotlib is currently using a non-GUI backend, \" We see even more significant trends here. Some users rate nearly everything highly, and some (though not as many) rate nearly everything negatively. These observations will come in handy when considering models to use for predicting user preferences on unseen jokes. Methods We're now ready to dig in and start addressing the problem. We want to predict how much each user is going to like all of the jokes he or she has not yet read, i.e. we want estimate $R&#94;*_{ij}$. Baselines Every good analysis needs some kind of baseline methods to compare against. It's difficult to claim we've produced good results if we have no reference point for what defines \"good\". We'll define three very simple baseline methods and find the RMSE using these methods. Our goal will be to obtain lower RMSE scores with whatever model we produce Important to remember: Baselines are different methods to calculate $R&#94;*_{ij}$ which are compared then. Uniform Random Baseline $R&#94;*_{ij} \\sim Uniform(min\\_rating,max\\_rating)$ Global Mean Baseline $R&#94;*_{ij} \\sim global\\_mean = \\frac{1}{N\\times M} \\sum_{i=1}&#94;N\\sum_{j=1}&#94;M I_{ij}(R_{ij})$, where $I_{ij}$ is the sparse mask of the $R_{ij}$. Mean of the Means Baseline $R&#94;*_{ij} = \\frac{1}{3}(user\\_means_i + joke\\_means_j + global\\_mean)$, $user\\_means_i =\\frac{1}{M} \\sum_{j=1}&#94;M I_{ij}(R_{ij}) $, $joke\\_means_j =\\frac{1}{N} \\sum_{i=1}&#94;N I_{ij}(R_{ij}) $ Let's make a Baseline class which will be the abstact class for all baselines. In [3]: class Baseline ( object ): \"\"\"Calculate baseline predictions.\"\"\" def __init__ ( self , train_data ): \"\"\"Simple heuristic-based transductive learning to fill in missing values in data matrix.\"\"\" self . predict ( train_data . copy ()) def predict ( self , train_data ): raise NotImplementedError ( 'baseline prediction not implemented for base class' ) def rmse ( self , test_data ): \"\"\"Calculate root mean squared error for predictions on test data.\"\"\" # rmse is a global function defined later return rmse ( test_data , self . predicted ) def split_title ( self , title ): \"\"\"Change \"BaselineMethod\" to \"Baseline Method\".\"\"\" words = [] tmp = [ title [ 0 ]] for c in title [ 1 :]: if c . isupper (): words . append ( '' . join ( tmp )) tmp = [ c ] else : tmp . append ( c ) words . append ( '' . join ( tmp )) return ' ' . join ( words ) def __str__ ( self ): return self . split_title ( self . __class__ . __name__ ) Here we implement three basic baselines discussed previuosly. Uniform Random Baseline In [4]: class UniformRandomBaseline ( Baseline ): \"\"\"Fill missing values with uniform random values.\"\"\" def predict ( self , train_data ): # get the mask where 'True' indicates np.nan in the data nan_mask = np . isnan ( train_data ) # get masked_array in order to find min and max values masked_train = np . ma . masked_array ( train_data , nan_mask ) # get min/max values for uniform distribution pmin , pmax = masked_train . min (), masked_train . max () # ge the number of 'True' in the mask N = nan_mask . sum () #replace all np.nan in the pandas dataframe by the uniformly generated value train_data [ nan_mask ] = np . random . uniform ( pmin , pmax , N ) self . predicted = train_data Global Mean Baseline In [5]: class GlobalMeanBaseline ( Baseline ): \"\"\"Fill in missing values using the global mean.\"\"\" def predict ( self , train_data ): nan_mask = np . isnan ( train_data ) train_data [ nan_mask ] = train_data [ ~ nan_mask ] . mean () self . predicted = train_data Mean of the Means Baseline In [6]: class MeanOfMeansBaseline ( Baseline ): \"\"\"Fill in missing values using mean of user/item/global means.\"\"\" def predict ( self , train_data ): # get a mask nan_mask = np . isnan ( train_data ) # get a masked_array to find out min, max, mean values. masked_train = np . ma . masked_array ( train_data , nan_mask ) # the global mean: a mean value of the masked_array global_mean = masked_train . mean () # using different axis, we can get 'user' and 'joke' means user_means = masked_train . mean ( axis = 1 ) # among jokes of one user joke_means = masked_train . mean ( axis = 0 ) # among users self . predicted = train_data . copy () n , m = train_data . shape for i in xrange ( n ): for j in xrange ( m ): if np . ma . isMA ( joke_means [ j ]): # it return true if joke_means[j] doesn't contain np.nan self . predicted [ i , j ] = np . mean ( ( global_mean , user_means [ i ])) else : self . predicted [ i , j ] = np . mean ( ( global_mean , user_means [ i ], joke_means [ j ])) Now, let's put all baselines in one dictionary, in order to test them later In [7]: from collections import OrderedDict baseline_methods = OrderedDict () baseline_methods [ 'ur' ] = UniformRandomBaseline baseline_methods [ 'gm' ] = GlobalMeanBaseline baseline_methods [ 'mom' ] = MeanOfMeansBaseline RMSE We're now ready to dig in and start addressing the problem. We want to predict how much each user is going to like all of the jokes he or she has not yet read, i.e. we want estimate $R&#94;*_{ij}$. The way we are going to use in order to test baselines and PMF bayesian framework is very simple: we will use the RMSE (root mean squared error) for predictions on test data and the method with the smallest RMSE would be defined as the best $RMSE = \\sqrt{\\frac{ \\sum_{i=1}&#94;N\\sum_{j=1}&#94;M I_{ij}(R_{ij}-R&#94;*_{ij})&#94;2}{\\sum_{i=1}&#94;N\\sum_{j=1}&#94;M I_{ij}}}$ Important to understand: the we use the test data to calculate $RMSE$. In [8]: # Define our evaluation function. def rmse ( test_data , predicted ): \"\"\"Calculate root mean squared error. Ignoring missing values in the test data. \"\"\" I = ~ np . isnan ( test_data ) # indicator for missing values N = I . sum () # number of non-missing values sqerror = abs ( test_data - predicted ) ** 2 # squared error array mse = sqerror [ I ] . sum () / N # mean squared error return np . sqrt ( mse ) Train/Test data Originally, authors of the tutorial say: So we produce a test set by taking a random sample of the cells in the full N×M data matrix $R_{ij}$. The values selected as test samples are replaced with nan values in a copy of the original data matrix to produce the training set. Since we'll be producing random splits, let's also write out the train/test sets generated. This will allow us to replicate our results. We'd like to be able to idenfity which split is which, so we'll take a hash of the indices selected for testing and use that to save the data. But do you understand this statement properly? I will try to summarize all points here: Use the origianl $R_{ij}$ uploaded at the beginning. Then randomly choose N cells in $R_{ij}$ and replace them by np.nan. This is a train dataset. The origianl $R_{ij}$ is a test dataset. Then create a hash code for chosen indexes and store it to the file. We can later to replicate the train/test splitting using this hash code. In [9]: import hashlib try : import ujson as json except ImportError : import json # First, define functions to save our parameters to the file. def save_np_vars ( vars , savedir ): \"\"\"Save a dictionary of numpy variables to `savedir`. We assume the directory does not exist; an OSError will be raised if it does. \"\"\" logging . info ( 'writing numpy vars to directory: %s ' % savedir ) os . mkdir ( savedir ) shapes = {} for varname in vars : data = vars [ varname ] var_file = os . path . join ( savedir , varname + '.txt' ) np . savetxt ( var_file , data . reshape ( - 1 , data . size )) shapes [ varname ] = data . shape ## Store shape information for reloading. shape_file = os . path . join ( savedir , 'shapes.json' ) with open ( shape_file , 'w' ) as sfh : json . dump ( shapes , sfh ) # Define the function to upload our parameters from the file def load_np_vars ( savedir ): \"\"\"Load numpy variables saved with `save_np_vars`.\"\"\" shape_file = os . path . join ( savedir , 'shapes.json' ) with open ( shape_file , 'r' ) as sfh : shapes = json . load ( sfh ) vars = {} for varname , shape in shapes . items (): var_file = os . path . join ( savedir , varname + '.txt' ) vars [ varname ] = np . loadtxt ( var_file ) . reshape ( shape ) return vars # Define a function for splitting train/test data. def split_train_test ( data , percent_test = 0.1 ): \"\"\"Split the data into train/test sets. :param int percent_test: Percentage of data to use for testing. Default 10%. \"\"\" n , m = data . shape # num. of users, num. of jokes N = n * m # num. of cells in matrix test_size = N * percent_test # use 10% of data as test set train_size = N - test_size # and remainder for training # Prepare train/test ndarrays. train = data . copy () . values # original Rij's values -> this is our 'train' dataset test = np . ones ( train . shape ) * np . nan # our 'test' dataset is the Rij matrix with np.nan everywhere # Draw random sample of training data to use for testing. #tosample = np.where(~np.isnan(train)) # ignore u nan values in data tosample = ( ~ np . isnan ( train )) . nonzero () # nonzero() return a pair of indexes for non np.nan values idx_pairs = zip ( tosample [ 0 ], tosample [ 1 ]) # tuples of row/col index pairs indices = np . arange ( len ( idx_pairs )) # indices of index pairs sample = np . random . choice ( indices , replace = False , size = test_size ) # here we use the sampling scheme without replacement # Transfer random sample from train set to test set. for idx in sample : idx_pair = idx_pairs [ idx ] test [ idx_pair ] = train [ idx_pair ] # transfer to test set train [ idx_pair ] = np . nan # remove from train set # Verify everything worked properly # what we have now: the train dataset contains np.nan at positions defined by the randomly selected indexes # the test dataset has all defined values at these positions but others are np.nan assert ( np . isnan ( train ) . sum () == test_size ) assert ( np . isnan ( test ) . sum () == train_size ) # Finally, hash the indices and save the train/test sets. index_string = '' . join ( map ( str , np . sort ( sample ))) name = hashlib . sha1 ( index_string ) . hexdigest () # saving train/test datasets to the folder 'name' savedir = os . path . join ( 'data' , name ) save_np_vars ({ 'train' : train , 'test' : test }, savedir ) print 'train/test datsets were stored in %s folder' % savedir # Return train set, test set, and unique hash of indices. return train , test , name # Finally, define the function to upload def load_train_test ( name ): \"\"\"Load the train/test sets.\"\"\" savedir = os . path . join ( 'data' , name ) vars = load_np_vars ( savedir ) return vars [ 'train' ], vars [ 'test' ] Later, we are going to use Bayesian approach to calculate Probability Matrix $R_{ij}$. Becasuse, we intend to use D=5, the dimension for hidden variables, it will bring D(N + M) = 5500 latent variables. It will take a huge memory to process the model. So, we restrict ourselves to 50 users, and 20 jokes: In [10]: data = pd . read_csv ( 'data/jester-dataset-v1-dense-first-1000.csv' ) data = data . loc [: 50 , data . columns . values [: 20 ]] In [11]: # test of splitting train , test , split_hash = split_train_test ( data ) train_reload , test_reload = load_train_test ( split_hash ) assert ( np . isnan ( train ) . sum () == np . isnan ( train_reload ) . sum () ) assert ( np . isnan ( test ) . sum () == np . isnan ( test_reload ) . sum () ) INFO:root:writing numpy vars to directory: data/9707154a0142b6aa18153ba96ee20c36f647bbf8 train/test datsets were stored in data/9707154a0142b6aa18153ba96ee20c36f647bbf8 folder Performance of the Baselines Now we can train our baselines, calculate their RMSE and compare them In [12]: # Let's see the results: train , test = load_train_test ( split_hash ) baselines = {} for name in baseline_methods : Method = baseline_methods [ name ] method = Method ( train ) baselines [ name ] = method . rmse ( test ) print ' %s RMSE: \\t %.5f ' % ( method , baselines [ name ]) # let's plot results size = 100 results = pd . DataFrame ({ 'uniform random' : np . repeat ( baselines [ 'ur' ], size ), 'global means' : np . repeat ( baselines [ 'gm' ], size ), 'mean of means' : np . repeat ( baselines [ 'mom' ], size ), }) fig , ax = plt . subplots ( figsize = ( 10 , 5 )) results . plot ( kind = 'line' , grid = False , ax = ax , title = 'RMSE for all methods' ) ax . set_xlabel ( \"Number of Samples\" ) ax . set_ylabel ( \"RMSE\" ) Uniform Random Baseline RMSE: 8.21210 Global Mean Baseline RMSE: 4.99941 Mean Of Means Baseline RMSE: 4.76052 Out[12]: <matplotlib.text.Text at 0xb6c32ec> Probabilistic Matrix Factorization Probabilistic Matrix Factorization (PMF) [3] is a probabilistic approach to the collaborative filtering problem that takes a Bayesian perspective. The ratings R are modeled as draws from a Gaussian distribution. The mean for $R_{ij}$ is $U_i\\cdot V&#94;T_j$. The precision α is a fixed parameter that reflects the uncertainty of the estimations; the normal distribution is commonly reparameterized in terms of precision, which is the inverse of the variance. Complexity is controlled by placing zero-mean spherical Gaussian priors on U and V. In other words, each $i&#94;{th}$ row of $U$ is drawn from a multivariate Gaussian with mean $\\mu_U=0$ and precision $\\alpha_U$ which is some multiple of the identity matrix I. Each $j&#94;{th}$ row of $V$ is drawn from a multivariate Gaussian with mean $\\mu_V=0$ and precision $\\alpha_V$ which is some multiple of the identity matrix I. Let's try to define the class realizing the model of PMF In [13]: class PMF ( object ): \"\"\"Probabilistic Matrix Factorization model using pymc3.\"\"\" def __init__ ( self , train , dim , alpha = 2 , std = 0.01 , bounds = ( - 10 , 10 )): \"\"\"Build the Probabilistic Matrix Factorization model using pymc2. :param np.ndarray train: The training data to use for learning the model. :param int dim: Dimensionality of the model; number of latent factors. :param int alpha: Fixed precision for the likelihood function. :param float std: Amount of noise to use for model initialization. :param (tuple of int) bounds: (lower, upper) bound of ratings. These bounds will simply be used to cap the estimates produced for R. \"\"\" self . dim = dim self . alpha = alpha self . std = np . sqrt ( 1.0 / alpha ) self . bounds = bounds # occupy a lot of memory #self.data = train.copy() self . data = train n , m = self . data . shape # Perform mean value imputation nan_mask = np . isnan ( self . data ) self . data [ nan_mask ] = self . data [ ~ nan_mask ] . mean () # Low precision reflects uncertainty; prevents overfitting. # Set to the mean variance across users and items. # var() gives std. variation self . alpha_u = 1 / self . data . var ( axis = 1 ) . mean () self . alpha_v = 1 / self . data . var ( axis = 0 ) . mean () # Specify the model. logging . info ( 'building the PMF model' ) # define our bayesian model here # define the U_i vector self . Ui = [ pm . MvNormal ( 'U %i ' % i ,np.zeros(dim),self.alpha_u * np.eye(dim)) for i in range(n) ] # define the V_j vector self . Vj = [ pm . MvNormal ( 'V %i ' % j ,np.zeros(dim),self.alpha_v * np.eye(dim)) for j in range(m) ] # define the mean value Rij = [] for i in range ( n ): _tmp = [] for j in range ( m ): _tmp += [ pm . Lambda ( '_R %i%i ' % ( i , j ), lambda U = self . Ui [ i ], V = self . Vj [ j ]: np . dot ( U , V . T ))] Rij += [ _tmp ] # define the observed self . Rij = [] for i in range ( n ): _tmp = [] for j in range ( m ): #_tmp +=[pm.Normal('R%i%i'%(i,j),np.dot(self.Ui[i].value,self.Vj[j].value.T),self.alpha,value=self.data[i,j],observed=True)] _tmp += [ pm . Normal ( 'R %i%i ' % ( i , j ), Rij [ i ][ j ], self . alpha , value = self . data [ i , j ], observed = True )] self . Rij += [ _tmp ] self . Rij = np . array ( self . Rij ) . flatten () . tolist () # define our model self . model = pm . Model ( self . Ui + self . Vj + self . Rij ) logging . info ( 'done building the PMF model' ) def __str__ ( self ): return self . name In [14]: # Test of Bayesian PMF # We use a fixed precision for the likelihood. # This reflects uncertainty in the dot product. # We choose 2 in the footsteps Salakhutdinov # Mnihof. ALPHA = 2 # The dimensionality D; the number of latent factors. # We can adjust this higher to try to capture more subtle # characteristics of each joke. However, the higher it is, # the more expensive our inference procedures will be. # Specifically, we have D(N + M) latent variables. For our # Jester dataset, this means we have D(1100), so for 5 # dimensions, we are sampling 5500 latent variables. DIM = 5 pmf = PMF ( train , DIM , ALPHA , std = 0.05 ) ? pmf INFO:root:building the PMF model INFO:root:done building the PMF model Ok. Let's make some tunning of our PMF class. First, we add 'MAP' (maximum a posteriori) property and functionality to read it and save it to the file In [15]: import scipy as sp import time # Now define the MAP estimation infrastructure. # property 'map_dir' def _map_dir ( self ): ''' return the dir name where we store the MAP''' basename = 'pmf-map-d %d ' % self . dim return os . path . join ( 'data' , basename ) # support for the property 'map' def _find_map ( self ): \"\"\"Find mode of posterior using Powell optimization.\"\"\" tstart = time . time () logging . info ( 'finding PMF MAP using Powell optimization...' ) self . _map = pm . MAP ( self . model ) self . _map . fit () elapsed = int ( time . time () - tstart ) logging . info ( 'found PMF MAP in %d seconds' % elapsed ) #pymc2 # This is going to take a good deal of time to find, so let's save it. savedir = os . path . join ( 'data' , name ) map_to_save = {} for var in list ( self . _map . variables ): map_to_save . update ({ str ( var ): var . value }) self . _map = map_to_save save_np_vars ( self . _map , self . map_dir ) print 'MAP was stored in %s folder' % savedir # support for the property 'map' def _load_map ( self ): self . _map = load_np_vars ( self . map_dir ) def _map ( self ): try : return self . _map except : if os . path . isdir ( self . map_dir ): self . load_map () else : self . find_map () return self . _map # Update our class with the new MAP infrastructure. PMF . find_map = _find_map PMF . load_map = _load_map PMF . map_dir = property ( _map_dir ) PMF . map = property ( _map ) Let's try to find MAP In [ ]: pmf . load_map () In [130]: # Find MAP for PMF. #pmf.find_map() INFO:root:finding PMF MAP using Powell optimization... INFO:root:found PMF MAP in 435 seconds INFO:root:writing numpy vars to directory: data/pmf-map-d5 MAP was stored in data/mom folder In [132]: print pmf . map [ 'U0' ] [ 2.61198177 0.03008766 -0.46349007 1.78042112 -2.81458536] Now we would like to introduce the MCMC sampling In [41]: # Draw MCMC samples. def _trace_dir ( self ): basename = 'pmf-mcmc-d %d ' % self . dim return os . path . join ( 'data' , basename ) # Sampler def _draw_samples ( self , nsamples = 1000 , njobs = 2 ): # First make sure the trace_dir does not already exist. if os . path . isdir ( self . trace_dir ): raise OSError ( 'trace directory %s already exists. Please move or delete.' % self . trace_dir ) # pymc2 logging . info ( 'drawing %d samples using %d jobs' % ( nsamples , njobs )) self . mcmc = pm . MCMC ( self . model , db = 'txt' , dbname = self . trace_dir ) logging . info ( 'backing up trace to directory: %s ' % self . trace_dir ) self . mcmc . sample ( nsamples ) self . traces = {} for var in list ( self . mcmc . stochastics ): self . traces . update ( { str ( var ): self . mcmc . trace ( str ( var ))[:] } ) self . mcmc . db . close () def _load_trace ( self ): self . mcmc = pm . MCMC ( self . model , db = pm . database . txt . load ( self . trace_dir )) self . traces = {} for var in list ( self . mcmc . stochastics ): self . traces . update ( { str ( var ): self . mcmc . trace ( str ( var ))[:] } ) In [42]: # Update our class with the sampling infrastructure. PMF . trace_dir = property ( _trace_dir ) PMF . draw_samples = _draw_samples PMF . load_trace = _load_trace Let's try to MCMC In [21]: pmf . draw_samples ( 5000 , njobs = 3 ) INFO:root:drawing 5000 samples using 3 jobs INFO:root:backing up trace to directory: data/pmf-mcmc-d5 [-----------------100%-----------------] 5000 of 5000 complete in 152.4 sec In [46]: pmf . load_trace () Finally, we need to define the 'predict' function. For user $i$ and joke $j$, a prediction is generated by drawing from $N(U_i\\cdot V&#94;T_j,\\alpha)$. To generate predictions from the sampler, we generate an $R_{ij}$ matrix for each $U$ and $V$ sampled, then we combine these by averaging over the $K$ samples. We introduce two versions of the 'predict': One version with averaging: $P(R&#94;*_{ij}|R,\\alpha,\\alpha_U,\\alpha_V) = \\frac{1}{K}\\sum_{k=1}&#94;K N(U_i\\cdot V&#94;T_j,\\alpha)$ Another version w/o averaging: $P(R&#94;*_{ij}|R,\\alpha,\\alpha_U,\\alpha_V) = N(U_i\\cdot V&#94;T_j,\\alpha)$ The version of $P(R&#94;*_{ij}|R,\\alpha,\\alpha_U,\\alpha_V)$ w/o averaging will be used in the diagnostics where diagnostic code will do averaging piece during evaluation. In [118]: # version w/o averaging def _predict ( self , U , V ): \"\"\"Estimate R from the given values of U and V.\"\"\" R = np . dot ( U , V . T ) n , m = R . shape sample_R = np . array ([ [ np . random . normal ( R [ i , j ], self . std ) for j in xrange ( m )] for i in xrange ( n ) ]) # bound ratings low , high = self . bounds sample_R [ sample_R < low ] = low sample_R [ sample_R > high ] = high return sample_R # version w/ averaging def _predict_trace ( self , trace_U , trace_V ): \"\"\"Estimate R from the given traces of U and V.\"\"\" R = np . zeros ( self . data . shape ) for sample in range ( len ( trace_U )): sample_R = self . predict ( trace_U [ sample ], trace_V [ sample ]) R += sample_R running_R = R / ( sample + 1 ) return running_R # here we define a function which construct a matrix 'U' from the list of 'U%i' def _getTraceMatrices ( self ): ''' construct the numpy.ndarray from the list of stochatisc variables for all traces''' Ui_names = map ( lambda x : str ( x ), self . Ui ) Vj_names = map ( lambda x : str ( x ), self . Vj ) _tmp_Ui = {} _tmp_Vj = {} nsample = None for i in Ui_names : _tmp_Ui . update ( { i :[ self . traces [ i ][ j ] for j in range ( len ( self . traces [ i ])) ] } ) if nsample is None : nsample = len ( self . traces [ i ]) for j in Vj_names : _tmp_Vj . update ( { j :[ self . traces [ j ][ i ] for i in range ( len ( self . traces [ j ])) ] } ) Ui = [] Vj = [] for i in range ( nsample ): _tmp_Ui_list = [] for row in Ui_names : _tmp_Ui_list += [ _tmp_Ui [ row ][ i ]] Ui += [ np . array ( _tmp_Ui_list )] _tmp_Vj_list = [] for row in Vj_names : _tmp_Vj_list += [ _tmp_Vj [ row ][ i ]] Vj += [ np . array ( _tmp_Vj_list )] return Ui , Vj # Update our class with the new Predict infrastructure PMF . getTraceMatrices = _getTraceMatrices PMF . predict = _predict PMF . predict_trace = _predict_trace Test of the predict function In [120]: traces_U , traces_V = pmf . getTraceMatrices () print '10 th sample R : ' , pmf . predict ( traces_U [ 9 ], traces_V [ 9 ]) print ' \\n\\n ' print 'averaging sampled R:' , pmf . predict_trace ( traces_U , traces_V ) 10 th sample R : [[ 0.73738038 2.8829047 4.15213768 ..., 0.90930744 -0.54034526 -1.37353771] [-4.66542936 -4.84431165 -4.0234287 ..., -3.47845611 -1.78232155 -3.3289911 ] [ 0.08347473 -0.16927429 -0.73111556 ..., -5.42224673 -4.75400413 -0.97107831] ..., [ 5.9403842 1.0674877 8.3158795 ..., -3.92076744 2.43819077 -4.45348299] [-3.40283744 -5.17037176 -3.23143203 ..., 0.55740903 -0.50587615 1.70313551] [ 1.8666256 2.66123111 3.16467822 ..., 0.16980542 1.15414995 -1.67072825]] averaging sampled R: [[ 1.02171491 2.42804436 2.90836343 ..., 1.94257543 1.57047715 -1.95718612] [-5.9003429 -5.91843978 -3.78319629 ..., -3.51430279 -2.18267165 -2.83427926] [ 1.18093384 0.24779066 -0.47771771 ..., -6.11247345 -5.17088762 -0.2462697 ] ..., [ 6.47288567 1.26964384 9.11230926 ..., -4.53319801 1.32721136 -4.26946043] [-2.95453856 -5.68951228 -2.75327014 ..., 1.02271064 -0.76349737 0.14336738] [ 2.44892463 1.68377936 3.6400361 ..., -0.31709323 1.84852407 -1.33852907]] Diagnostics and Posterior Predictive Check The next step is to check how many samples we should discard as burn-in. Normally, we'd do this using a traceplot to get some idea of where the sampled variables start to converge. In this case, we have high-dimensional samples, so we need to find a way to approximate them. One way was proposed by Salakhutdinov and Mnih, p.886. We can calculate the Frobenius norms of U and V at each step and monitor those for convergence. This essentially gives us some idea when the average magnitude of the latent variables is stabilizing. The equations for the Frobenius norms of U and V are shown below. We will use numpy's linalg package to calculate these. In [125]: def _norms ( pmf_model , ord = 'fro' ): \"\"\"Return norms of latent variables at each step in the sample trace. These can be used to monitor convergence of the sampler. \"\"\" traces_U , traces_V = pmf_model . getTraceMatrices () norms = { 'U' : traces_U , 'V' : traces_V } for var in norms : norms [ var ] = map ( lambda x : np . linalg . norm ( x , ord ), norms [ var ]) return norms def _traceplot ( pmf_model ): \"\"\"Plot Frobenius norms of U and V as a function of sample #.\"\"\" trace_norms = pmf_model . norms () u_series = pd . Series ( trace_norms [ 'U' ]) v_series = pd . Series ( trace_norms [ 'V' ]) fig , ( ax1 , ax2 ) = plt . subplots ( 1 , 2 , figsize = ( 15 , 7 )) u_series . plot ( kind = 'line' , ax = ax1 , grid = False , title = \"$\\|U\\|_{Fro}&#94;2$ at Each Sample\" ) v_series . plot ( kind = 'line' , ax = ax2 , grid = False , title = \"$\\|V\\|_{Fro}&#94;2$ at Each Sample\" ) ax1 . set_xlabel ( \"Sample Number\" ) ax2 . set_xlabel ( \"Sample Number\" ) PMF . norms = _norms PMF . traceplot = _traceplot pmf . traceplot () It appears we get convergence of U and V after about 4000 samples. Let's also do a traceplot of the RSME. We'll compute RMSE for both the train and the test set, even though the convergence is indicated by RMSE on the training set alone. In addition, let's compute a running RMSE on the train/test sets to see how aggregate performance improves or decreases as we continue to sample. In [138]: def _running_rmse ( pmf_model , test_data , train_data , burn_in = 0 , plot = True ): \"\"\"Calculate RMSE for each step of the trace to monitor convergence. \"\"\" traces_U , traces_V = pmf_model . getTraceMatrices () burn_in = burn_in if len ( traces_U ) >= burn_in else 0 results = { 'per-step-train' : [], 'running-train' : [], 'per-step-test' : [], 'running-test' : []} R = np . zeros ( test_data . shape ) for cnt in range ( burn_in , len ( traces_U )): sample_R = pmf_model . predict ( traces_U [ cnt ], traces_V [ cnt ]) R += sample_R running_R = R / ( cnt + 1 ) results [ 'per-step-train' ] . append ( rmse ( train_data , sample_R )) results [ 'running-train' ] . append ( rmse ( train_data , running_R )) results [ 'per-step-test' ] . append ( rmse ( test_data , sample_R )) results [ 'running-test' ] . append ( rmse ( test_data , running_R )) results = pd . DataFrame ( results ) if plot : results . plot ( kind = 'line' , grid = False , figsize = ( 15 , 7 ), title = 'Per-step and Running RMSE From Posterior Predictive' ) # Return the final predictions, and the RMSE calculations return running_R , results PMF . running_rmse = _running_rmse predicted , results = pmf . running_rmse ( test , train , burn_in = 200 ) Excellent. Another thing we want to do is make sure the MAP estimate we obtained is reasonable. We can do this by computing RMSE on the predicted ratings obtained from the MAP values of U and V. In [133]: # here we define a function which construct a matrix 'U' from the list of 'U%i' def _getMAPMatrices ( self ): ''' construct the numpy.ndarray from the list of stochatisc variables for all traces''' Ui_names = map ( lambda x : str ( x ), self . Ui ) Vj_names = map ( lambda x : str ( x ), self . Vj ) _tmp_Ui = {} _tmp_Vj = {} for i in Ui_names : _tmp_Ui . update ( { i : self . map [ i ] } ) for j in Vj_names : _tmp_Vj . update ( { j : self . map [ j ] } ) _tmp_Ui_list = [] for row in Ui_names : _tmp_Ui_list += [ _tmp_Ui [ row ]] Ui = np . array ( _tmp_Ui_list ) _tmp_Vj_list = [] for row in Vj_names : _tmp_Vj_list += [ _tmp_Vj [ row ]] Vj = np . array ( _tmp_Vj_list ) return Ui , Vj # estimation of the MAP performance def _eval_map ( pmf_model , train , test ): U , V = pmf_model . getMAPMatrices () # Make predictions and calculate RMSE on train & test sets. predictions = pmf_model . predict ( U , V ) train_rmse = rmse ( train , predictions ) test_rmse = rmse ( test , predictions ) overfit = test_rmse - train_rmse # Print report. print 'PMF MAP training RMSE: %.5f ' % train_rmse print 'PMF MAP testing RMSE: %.5f ' % test_rmse print 'Train/test difference: %.5f ' % overfit return test_rmse # Add eval function to PMF class. PMF . getMAPMatrices = _getMAPMatrices PMF . eval_map = _eval_map In [134]: # Evaluate PMF MAP estimates. pmf_map_rmse = pmf . eval_map ( train , test ) pmf_improvement = baselines [ 'mom' ] - pmf_map_rmse print 'PMF MAP Improvement: %.5f ' % pmf_improvement PMF MAP training RMSE: 3.31098 PMF MAP testing RMSE: 4.92116 Train/test difference: 1.61018 PMF MAP Improvement: -0.16064 Let's compare the MAP results with MCMC ones: In [135]: # And our final RMSE? final_test_rmse = results [ 'running-test' ] . values [ - 1 ] final_train_rmse = results [ 'running-train' ] . values [ - 1 ] print 'Posterior predictive train RMSE: %.5f ' % final_train_rmse print 'Posterior predictive test RMSE: %.5f ' % final_test_rmse print 'Train/test difference: %.5f ' % ( final_test_rmse - final_train_rmse ) print 'Improvement from MAP: %.5f ' % ( pmf_map_rmse - final_test_rmse ) print 'Improvement from Mean of Means: %.5f ' % ( baselines [ 'mom' ] - final_test_rmse ) Posterior predictive train RMSE: 3.26451 Posterior predictive test RMSE: 4.76826 Train/test difference: 1.50375 Improvement from MAP: 0.15290 Improvement from Mean of Means: -0.00774 Summary of Results Let's summarize our results. In [141]: # let's plot results size = 4800 final_results = pd . DataFrame ({ 'uniform random' : np . repeat ( baselines [ 'ur' ], size ), 'global means' : np . repeat ( baselines [ 'gm' ], size ), 'mean of means' : np . repeat ( baselines [ 'mom' ], size ), 'PMF MAP' : np . repeat ( pmf_map_rmse , size ), 'PMF MCMC' : results [ 'running-test' ][: size ] }) fig , ax = plt . subplots ( figsize = ( 10 , 5 )) final_results . plot ( kind = 'line' , grid = False , ax = ax , title = 'RMSE for all methods' ) ax . set_xlabel ( \"Number of Samples\" ) ax . set_ylabel ( \"RMSE\" ) Out[141]: <matplotlib.text.Text at 0xa081f0cc> Our results demonstrate that the mean of means method is our best baseline on our prediction task. We illustrated one way to monitor convergence of an MCMC sampler with a high-dimensionality sampling space using the Frobenius norms of the sampled variables. The traceplots using this method seem to indicate that our sampler converged to the posterior. Results using this posterior showed that attempting to improve the estimation using MCMC sampling actually overfit the training data and increased test RMSE. This was likely caused by the constraining of the posterior via fixed precision parameters $\\alpha$, $\\alpha_U$, and $\\alpha_V$. To solve the problem of the overfitting, perhaps, we could implement the fully Bayesian version of PMF (BPMF). which places hyperpriors on the model parameters to automatically learn ideal mean and precision parameters for U and V. This would likely resolve the issue we faced in this analysis. We would expect BPMF to improve upon the MAP estimation produced here by learning more suitable hyperparameters and parameters. Fully Bayesian Probabilistic Matrix Factorization(FBPMF) (optional) It was inspired by the gist . In [158]: class FBPMF ( object ): \"\"\"Fully Bayesian Probabilistic Matrix Factorization model using pymc2.\"\"\" def __init__ ( self , train , dim , alpha = 2 , std = 0.01 , bounds = ( - 10 , 10 )): \"\"\"Build the Probabilistic Matrix Factorization model using pymc2. :param np.ndarray train: The training data to use for learning the model. :param int dim: Dimensionality of the model; number of latent factors. :param int alpha: Fixed precision for the likelihood function. :param float std: Amount of noise to use for model initialization. :param (tuple of int) bounds: (lower, upper) bound of ratings. These bounds will simply be used to cap the estimates produced for R. \"\"\" self . dim = dim self . alpha = alpha self . std = np . sqrt ( 1.0 / alpha ) self . bounds = bounds # occupy a lot of memory #self.data = train.copy() self . data = train n , m = self . data . shape beta_0 = 1 # scaling factor for lambdas; unclear on its use # Perform mean value imputation nan_mask = np . isnan ( self . data ) self . data [ nan_mask ] = self . data [ ~ nan_mask ] . mean () # DELETED in FBMF # Low precision reflects uncertainty; prevents overfitting. # Set to the mean variance across users and jokes. # var() gives std. variation #self.alpha_u = 1 / self.data.var(axis=1).mean() #self.alpha_v = 1 / self.data.var(axis=0).mean() # Specify the model. logging . info ( 'building the FBPMF model' ) # define our bayesian model here # NEW in FBMF: self . lambda_u = pm . Wishart ( 'lambda_u' , dim , np . eye ( dim )) self . alpha_u = pm . Lambda ( 'alpha_u' , lambda inv_cov_matr = self . lambda_u : np . sqrt ( np . diag ( inv_cov_matr )) . mean ()) self . mu_u = pm . Normal ( 'mu_u' , np . zeros ( dim ), self . alpha_u , size = dim ) #self.mu_u = pm.Normal('mu_u', np.zeros(dim), 1, size=dim) # define the U_i vector: CHANGED in FBMF self . Ui = [ pm . MvNormal ( 'U %i ' % i ,self.mu_u,self.lambda_u) for i in range(n) ] # NEW in FBMF: self . lambda_v = pm . Wishart ( 'lambda_v' , dim , np . eye ( dim )) self . alpha_v = pm . Lambda ( 'alpha_v' , lambda inv_cov_matr = self . lambda_v : np . sqrt ( np . diag ( inv_cov_matr )) . mean ()) self . mu_v = pm . Normal ( 'mu_v' , np . zeros ( dim ), self . alpha_v , size = dim ) #self.mu_v = pm.Normal('mu_v', np.zeros(dim), self.lambda_v, size=dim) #self.mu_v = pm.Normal('mu_v', np.zeros(dim), 1, size=dim) # define the V_j vector: CHANGED in FBMF self . Vj = [ pm . MvNormal ( 'V %i ' % j ,self.mu_v,self.lambda_v) for j in range(m) ] # define the mean value Rij = [] for i in range ( n ): _tmp = [] for j in range ( m ): _tmp += [ pm . Lambda ( '_R %i%i ' % ( i , j ), lambda U = self . Ui [ i ], V = self . Vj [ j ]: np . dot ( U , V . T ))] Rij += [ _tmp ] # define the observed self . Rij = [] for i in range ( n ): _tmp = [] for j in range ( m ): #_tmp +=[pm.Normal('R%i%i'%(i,j),np.dot(self.Ui[i].value,self.Vj[j].value.T),self.alpha,value=self.data[i,j],observed=True)] _tmp += [ pm . Normal ( 'R %i%i ' % ( i , j ), Rij [ i ][ j ], self . alpha , value = self . data [ i , j ], observed = True )] self . Rij += [ _tmp ] self . Rij = np . array ( self . Rij ) . flatten () . tolist () # define our model self . model = pm . Model ( self . Ui + self . Vj + self . Rij ) logging . info ( 'done building the FBPMF model' ) def __str__ ( self ): return self . name In [159]: # Test of Fully Bayesian PMF # We use a fixed precision for the likelihood. # This reflects uncertainty in the dot product. # We choose 2 in the footsteps Salakhutdinov # Mnihof. ALPHA = 2 # The dimensionality D; the number of latent factors. # We can adjust this higher to try to capture more subtle # characteristics of each joke. However, the higher it is, # the more expensive our inference procedures will be. # Specifically, we have D(N + M) latent variables. For our # Jester dataset, this means we have D(1100), so for 5 # dimensions, we are sampling 5500 latent variables. DIM = 5 fbpmf = FBPMF ( train , DIM , ALPHA , std = 0.05 ) ? fbpmf INFO:root:building the FBPMF model INFO:root:done building the FBPMF model Here we add all functionality of the basic PMF: In [168]: # property 'map_dir' def _map_dir_bfpmf ( self ): ''' return the dir name where we store the MAP''' basename = 'bfpmf-map-d %d ' % self . dim return os . path . join ( 'data' , basename ) # Update our class with the new MAP infrastructure. FBPMF . find_map = _find_map FBPMF . load_map = _load_map FBPMF . map_dir = property ( _map_dir_bfpmf ) FBPMF . map = property ( _map ) # Add eval function to PMF class. FBPMF . getMAPMatrices = _getMAPMatrices FBPMF . eval_map = _eval_map # Draw MCMC samples. def _trace_dir_bfpmf ( self ): basename = 'bfpmf-mcmc-d %d ' % self . dim return os . path . join ( 'data' , basename ) # Update our class with the sampling infrastructure. FBPMF . trace_dir = property ( _trace_dir_bfpmf ) FBPMF . draw_samples = _draw_samples FBPMF . load_trace = _load_trace # Update our class with the new Predict infrastructure FBPMF . getTraceMatrices = _getTraceMatrices FBPMF . predict = _predict FBPMF . predict_trace = _predict_trace # plotting stuff FBPMF . norms = _norms FBPMF . traceplot = _traceplot FBPMF . running_rmse = _running_rmse In [166]: fbpmf . draw_samples ( 5000 , njobs = 3 ) INFO:root:drawing 5000 samples using 3 jobs INFO:root:backing up trace to directory: data/bfpmf-mcmc-d5 [-----------------100%-----------------] 5000 of 5000 complete in 168.1 sec Diagnostics In [169]: fbpmf . traceplot () In [170]: _ , results_fbpmf = fbpmf . running_rmse ( test , train , burn_in = 200 ) In [171]: # Find MAP for FBPMF. fbpmf . find_map () INFO:root:finding PMF MAP using Powell optimization... INFO:root:found PMF MAP in 1522 seconds INFO:root:writing numpy vars to directory: data/bfpmf-map-d5 MAP was stored in data/mom folder In [174]: # Evaluate PMF MAP estimates. fbpmf_map_rmse = fbpmf . eval_map ( train , test ) fbpmf_improvement = baselines [ 'mom' ] - fbpmf_map_rmse print 'PMF MAP Improvement: %.5f ' % fbpmf_improvement PMF MAP training RMSE: 3.30869 PMF MAP testing RMSE: 4.77023 Train/test difference: 1.46154 PMF MAP Improvement: -0.00970 In [175]: # And our final RMSE? fbpmb_final_test_rmse = results_fbpmf [ 'running-test' ] . values [ - 1 ] fbpmb_final_train_rmse = results_fbpmf [ 'running-train' ] . values [ - 1 ] print 'Posterior predictive train RMSE: %.5f ' % fbpmb_final_train_rmse print 'Posterior predictive test RMSE: %.5f ' % fbpmb_final_test_rmse print 'Train/test difference: %.5f ' % ( fbpmb_final_test_rmse - fbpmb_final_train_rmse ) print 'Improvement from MAP: %.5f ' % ( fbpmf_map_rmse - fbpmb_final_test_rmse ) print 'Improvement from Mean of Means: %.5f ' % ( baselines [ 'mom' ] - fbpmb_final_test_rmse ) Posterior predictive train RMSE: 3.46717 Posterior predictive test RMSE: 4.79397 Train/test difference: 1.32679 Improvement from MAP: -0.02374 Improvement from Mean of Means: -0.03345 In [176]: # let's plot results size = 4800 final_results = pd . DataFrame ({ 'uniform random' : np . repeat ( baselines [ 'ur' ], size ), 'global means' : np . repeat ( baselines [ 'gm' ], size ), 'mean of means' : np . repeat ( baselines [ 'mom' ], size ), 'FBPMF MAP' : np . repeat ( fbpmf_map_rmse , size ), 'FBPMF MCMC' : results_fbpmf [ 'running-test' ][: size ] }) fig , ax = plt . subplots ( figsize = ( 10 , 5 )) final_results . plot ( kind = 'line' , grid = False , ax = ax , title = 'RMSE for all methods' ) ax . set_xlabel ( \"Number of Samples\" ) ax . set_ylabel ( \"RMSE\" ) Out[176]: <matplotlib.text.Text at 0x1076beac> In [ ]: if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML'; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: 'center',\" + \" displayIndent: '0em',\" + \" showMathMenu: true,\" + \" tex2jax: { \" + \" inlineMath: [ ['$','$'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }\" + \" } \" + \"}); \"; (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Statistics","title":"Probabilistic Matrix Factorization for Making Personalized Recommendations"},{"url":"http://igormarfin.github.io/blog/2016/02/07/about-meI'm a data scientist and programmer currently based in Chemnitz, Germany./","text":"Previuosly, I was engaged with researches in particle and theoretical physics at CERN, Switzerland. My work was related not only to physics but also to statistical analyisis and software development for experiments at the Compact Muon Solenoid (CMS) detector at Large Hadron Collider. I was involved with my group in the search for the Higgs particle. I think we have all done a great job to discover the Higgs boson in 2012. As a final point of my \"physics\" story was the PhD thesis, which I defended with summa cum laude. Now I am investigating and developing the techniques of the machine learning. Also I am a big fan of the Bayesian Statistics. My current projects are related to the Device Fingerprinting, Fraud Prevention System for online transactions and Predictive Customer Behavior Modeling. You can visit my repository to look at my projects. I'm interested in doing distance work in the field of the data analysis. You could contact me at iggy.floyd.de at gmail.com","tags":"About me","title":"About Me"}]}