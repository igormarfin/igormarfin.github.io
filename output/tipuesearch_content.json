{"pages":[{"url":"http://igormarfin.github.io/blog/2016/04/15/application-of-the-monte-carlo-integration-in-options-trading-(part-i)/","text":"/*! * * IPython notebook * */.ansibold{font-weight:bold}.ansiblack{color:black}.ansired{color:darkred}.ansigreen{color:darkgreen}.ansiyellow{color:#c4a000}.ansiblue{color:darkblue}.ansipurple{color:darkviolet}.ansicyan{color:steelblue}.ansigray{color:gray}.ansibgblack{background-color:black}.ansibgred{background-color:red}.ansibggreen{background-color:green}.ansibgyellow{background-color:yellow}.ansibgblue{background-color:blue}.ansibgpurple{background-color:magenta}.ansibgcyan{background-color:cyan}.ansibggray{background-color:gray}div.cell{border:1px solid transparent;display:-webkit-box;-webkit-box-orient:vertical;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:vertical;-moz-box-align:stretch;display:box;box-orient:vertical;box-align:stretch;display:flex;flex-direction:column;align-items:stretch;border-radius:2px;box-sizing:border-box;-moz-box-sizing:border-box;-webkit-box-sizing:border-box;border-width:thin;border-style:solid;width:100%;padding:5px;margin:0;outline:none}div.cell.selected{border-color:#ababab}@media print{div.cell.selected{border-color:transparent}}.edit_mode div.cell.selected{border-color:green}@media print{.edit_mode div.cell.selected{border-color:transparent}}.prompt{min-width:14ex;padding:.4em;margin:0;font-family:monospace;text-align:right;line-height:1.21429em}@media (max-width:540px){.prompt{text-align:left}}div.inner_cell{display:-webkit-box;-webkit-box-orient:vertical;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:vertical;-moz-box-align:stretch;display:box;box-orient:vertical;box-align:stretch;display:flex;flex-direction:column;align-items:stretch;-webkit-box-flex:1;-moz-box-flex:1;box-flex:1;flex:1}@-moz-document url-prefix(){div.inner_cell{overflow-x:hidden}}div.input_area{border:1px solid #cfcfcf;border-radius:2px;background:#f7f7f7;line-height:1.21429em}div.prompt:empty{padding-top:0;padding-bottom:0}div.unrecognized_cell{padding:5px 5px 5px 0;display:-webkit-box;-webkit-box-orient:horizontal;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:horizontal;-moz-box-align:stretch;display:box;box-orient:horizontal;box-align:stretch;display:flex;flex-direction:row;align-items:stretch}div.unrecognized_cell .inner_cell{border-radius:2px;padding:5px;font-weight:bold;color:red;border:1px solid #cfcfcf;background:#eaeaea}div.unrecognized_cell .inner_cell a{color:inherit;text-decoration:none}div.unrecognized_cell .inner_cell a:hover{color:inherit;text-decoration:none}@media (max-width:540px){div.unrecognized_cell>div.prompt{display:none}}@media print{div.code_cell{page-break-inside:avoid}}div.input{page-break-inside:avoid;display:-webkit-box;-webkit-box-orient:horizontal;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:horizontal;-moz-box-align:stretch;display:box;box-orient:horizontal;box-align:stretch;display:flex;flex-direction:row;align-items:stretch}@media (max-width:540px){div.input{display:-webkit-box;-webkit-box-orient:vertical;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:vertical;-moz-box-align:stretch;display:box;box-orient:vertical;box-align:stretch;display:flex;flex-direction:column;align-items:stretch}}div.input_prompt{color:navy;border-top:1px solid transparent}div.input_area>div.highlight{margin:.4em;border:none;padding:0;background-color:transparent}div.input_area>div.highlight>pre{margin:0;border:none;padding:0;background-color:transparent}.CodeMirror{line-height:1.21429em;font-size:14px;height:auto;background:none}.CodeMirror-scroll{overflow-y:hidden;overflow-x:auto}.CodeMirror-lines{padding:.4em}.CodeMirror-linenumber{padding:0 8px 0 4px}.CodeMirror-gutters{border-bottom-left-radius:2px;border-top-left-radius:2px}.CodeMirror pre{padding:0;border:0;border-radius:0}.highlight-base{}.highlight-variable{}.highlight-variable-2{color:#1a1a1a}.highlight-variable-3{color:#333}.highlight-string{color:#ba2121}.highlight-comment{color:#408080;font-style:italic}.highlight-number{80}.highlight-atom{color:#88f}.highlight-keyword{8000;font-weight:bold}.highlight-builtin{8000}.highlight-error{color:#f00}.highlight-operator{color:#a2f;font-weight:bold}.highlight-meta{color:#a2f}.highlight-def{f}.highlight-string-2{color:#f50}.highlight-qualifier{color:#555}.highlight-bracket{color:#997}.highlight-tag{color:#170}.highlight-attribute{c}.highlight-header{color:blue}.highlight-quote{90}.highlight-link{c}.cm-s-ipython span.cm-keyword{8000;font-weight:bold}.cm-s-ipython span.cm-atom{color:#88f}.cm-s-ipython span.cm-number{80}.cm-s-ipython span.cm-def{f}.cm-s-ipython span.cm-variable{}.cm-s-ipython span.cm-operator{color:#a2f;font-weight:bold}.cm-s-ipython span.cm-variable-2{color:#1a1a1a}.cm-s-ipython span.cm-variable-3{color:#333}.cm-s-ipython span.cm-comment{color:#408080;font-style:italic}.cm-s-ipython span.cm-string{color:#ba2121}.cm-s-ipython span.cm-string-2{color:#f50}.cm-s-ipython span.cm-meta{color:#a2f}.cm-s-ipython span.cm-qualifier{color:#555}.cm-s-ipython span.cm-builtin{8000}.cm-s-ipython span.cm-bracket{color:#997}.cm-s-ipython span.cm-tag{color:#170}.cm-s-ipython span.cm-attribute{c}.cm-s-ipython span.cm-header{color:blue}.cm-s-ipython span.cm-quote{90}.cm-s-ipython span.cm-link{c}.cm-s-ipython span.cm-error{color:#f00}.cm-s-ipython span.cm-tab{background:url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADAAAAAMCAYAAAAkuj5RAAAAAXNSR0IArs4c6QAAAGFJREFUSMft1LsRQFAQheHPowAKoACx3IgEKtaEHujDjORSgWTH/ZOdnZOcM/sgk/kFFWY0qV8foQwS4MKBCS3qR6ixBJvElOobYAtivseIE120FaowJPN75GMu8j/LfMwNjh4HUpwg4LUAAAAASUVORK5CYII=);background-position:right;background-repeat:no-repeat}div.output_wrapper{position:relative;display:-webkit-box;-webkit-box-orient:vertical;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:vertical;-moz-box-align:stretch;display:box;box-orient:vertical;box-align:stretch;display:flex;flex-direction:column;align-items:stretch}div.output_scroll{height:24em;width:100%;overflow:auto;border-radius:2px;-webkit-box-shadow:inset 0 2px 8px rgba(0,0,0,0.8);box-shadow:inset 0 2px 8px rgba(0,0,0,0.8);display:block}div.output_collapsed{margin:0;padding:0;display:-webkit-box;-webkit-box-orient:vertical;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:vertical;-moz-box-align:stretch;display:box;box-orient:vertical;box-align:stretch;display:flex;flex-direction:column;align-items:stretch}div.out_prompt_overlay{height:100%;padding:0 .4em;position:absolute;border-radius:2px}div.out_prompt_overlay:hover{-webkit-box-shadow:inset 0 0 1px #000;box-shadow:inset 0 0 1px #000;background:rgba(240,240,240,0.5)}div.output_prompt{color:darkred}div.output_area{padding:0;page-break-inside:avoid;display:-webkit-box;-webkit-box-orient:horizontal;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:horizontal;-moz-box-align:stretch;display:box;box-orient:horizontal;box-align:stretch;display:flex;flex-direction:row;align-items:stretch}div.output_area .MathJax_Display{text-align:left !important}div.output_area div.output_area .output{display:-webkit-box;-webkit-box-orient:vertical;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:vertical;-moz-box-align:stretch;display:box;box-orient:vertical;box-align:stretch;display:flex;flex-direction:column;align-items:stretch}@media (max-width:540px){div.output_area{display:-webkit-box;-webkit-box-orient:vertical;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:vertical;-moz-box-align:stretch;display:box;box-orient:vertical;box-align:stretch;display:flex;flex-direction:column;align-items:stretch}}div.output_area pre{margin:0;padding:0;border:0;vertical-align:baseline;color:black;background-color:transparent;border-radius:0}div.output_subarea{padding:.4em;-webkit-box-flex:1;-moz-box-flex:1;box-flex:1;flex:1}div.output_text{text-align:left;line-height:1.21429em}div.output_stderr{background:#fdd}div.output_latex{text-align:left}div.output_javascript:empty{padding:0}.js-error{color:darkred}div.raw_input_container{font-family:monospace;padding-top:5px}input.raw_input{font-family:inherit;font-size:inherit;color:inherit;width:auto;vertical-align:baseline;padding:0 .25em;margin:0 .25em}input.raw_input:focus{box-shadow:none}p.p-space{margin-bottom:10px}div.output_unrecognized{padding:5px;font-weight:bold;color:red}div.output_unrecognized a{color:inherit;text-decoration:none}div.output_unrecognized a:hover{color:inherit;text-decoration:none}.rendered_html{}.rendered_html :link{text-decoration:underline}.rendered_html :visited{text-decoration:underline}.rendered_html h1:first-child{margin-top:.538em}.rendered_html h2:first-child{margin-top:.636em}.rendered_html h3:first-child{margin-top:.777em}.rendered_html h4:first-child{margin-top:1em}.rendered_html h5:first-child{margin-top:1em}.rendered_html h6:first-child{margin-top:1em}.rendered_html *+ul{margin-top:1em}.rendered_html *+ol{margin-top:1em}.rendered_html *+table{margin-top:1em}.rendered_html *+p{margin-top:1em}.rendered_html *+img{margin-top:1em}div.text_cell{display:-webkit-box;-webkit-box-orient:horizontal;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:horizontal;-moz-box-align:stretch;display:box;box-orient:horizontal;box-align:stretch;display:flex;flex-direction:row;align-items:stretch}@media (max-width:540px){div.text_cell>div.prompt{display:none}}div.text_cell_render{outline:none;resize:none;width:inherit;border-style:none;padding:.5em .5em .5em .4em;box-sizing:border-box;-moz-box-sizing:border-box;-webkit-box-sizing:border-box}a.anchor-link:link{text-decoration:none;padding:0 20px;visibility:hidden}h1:hover .anchor-link,h2:hover .anchor-link,h3:hover .anchor-link,h4:hover .anchor-link,h5:hover .anchor-link,h6:hover .anchor-link{visibility:visible}.text_cell.rendered .input_area{display:none}.text_cell.unrendered .text_cell_render{display:none}.cm-header-1,.cm-header-2,.cm-header-3,.cm-header-4,.cm-header-5,.cm-header-6{font-weight:bold;font-family:\"Helvetica Neue\",Helvetica,Arial,sans-serif}.cm-header-1{font-size:185.7%}.cm-header-2{font-size:157.1%}.cm-header-3{font-size:128.6%}.cm-header-4{font-size:110%}.cm-header-5{font-size:100%;font-style:italic}.cm-header-6{font-size:100%;font-style:italic}.widget-interact>div,.widget-interact>input{padding:2.5px}.widget-area{page-break-inside:avoid;display:-webkit-box;-webkit-box-orient:horizontal;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:horizontal;-moz-box-align:stretch;display:box;box-orient:horizontal;box-align:stretch;display:flex;flex-direction:row;align-items:stretch}.widget-area .widget-subarea{padding:.44em .4em .4em 1px;margin-left:6px;box-sizing:border-box;-moz-box-sizing:border-box;-webkit-box-sizing:border-box;display:-webkit-box;-webkit-box-orient:vertical;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:vertical;-moz-box-align:stretch;display:box;box-orient:vertical;box-align:stretch;display:flex;flex-direction:column;align-items:stretch;-webkit-box-flex:2;-moz-box-flex:2;box-flex:2;flex:2;-webkit-box-align:start;-moz-box-align:start;box-align:start;align-items:flex-start}.widget-area.connection-problems .prompt:after{content:\"\\f127\";font-family:'FontAwesome';color:#d9534f;font-size:14px;top:3px;padding:3px}.slide-track{border:1px solid #ccc;background:#fff;border-radius:2px}.widget-hslider{padding-left:8px;padding-right:2px;overflow:visible;width:350px;height:5px;max-height:5px;margin-top:13px;margin-bottom:10px;border:1px solid #ccc;background:#fff;border-radius:2px;display:-webkit-box;-webkit-box-orient:horizontal;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:horizontal;-moz-box-align:stretch;display:box;box-orient:horizontal;box-align:stretch;display:flex;flex-direction:row;align-items:stretch}.widget-hslider .ui-slider{border:0;background:none;display:-webkit-box;-webkit-box-orient:horizontal;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:horizontal;-moz-box-align:stretch;display:box;box-orient:horizontal;box-align:stretch;display:flex;flex-direction:row;align-items:stretch;-webkit-box-flex:1;-moz-box-flex:1;box-flex:1;flex:1}.widget-hslider .ui-slider .ui-slider-handle{width:12px;height:28px;margin-top:-8px;border-radius:2px}.widget-hslider .ui-slider .ui-slider-range{height:12px;margin-top:-4px;background:#eee}.widget-vslider{padding-bottom:5px;overflow:visible;width:5px;max-width:5px;height:250px;margin-left:12px;border:1px solid #ccc;background:#fff;border-radius:2px;display:-webkit-box;-webkit-box-orient:vertical;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:vertical;-moz-box-align:stretch;display:box;box-orient:vertical;box-align:stretch;display:flex;flex-direction:column;align-items:stretch}.widget-vslider .ui-slider{border:0;background:none;margin-left:-4px;margin-top:5px;display:-webkit-box;-webkit-box-orient:vertical;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:vertical;-moz-box-align:stretch;display:box;box-orient:vertical;box-align:stretch;display:flex;flex-direction:column;align-items:stretch;-webkit-box-flex:1;-moz-box-flex:1;box-flex:1;flex:1}.widget-vslider .ui-slider .ui-slider-handle{width:28px;height:12px;margin-left:-9px;border-radius:2px}.widget-vslider .ui-slider .ui-slider-range{width:12px;margin-left:-1px;background:#eee}.widget-text{width:350px;margin:0}.widget-listbox{width:350px;margin-bottom:0}.widget-numeric-text{width:150px;margin:0}.widget-progress{margin-top:6px;min-width:350px}.widget-progress .progress-bar{-webkit-transition:none;-moz-transition:none;-ms-transition:none;-o-transition:none;transition:none}.widget-combo-btn{min-width:125px}.widget_item .dropdown-menu li a{color:inherit}.widget-hbox{display:-webkit-box;-webkit-box-orient:horizontal;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:horizontal;-moz-box-align:stretch;display:box;box-orient:horizontal;box-align:stretch;display:flex;flex-direction:row;align-items:stretch}.widget-hbox input[type=\"checkbox\"]{margin-top:9px;margin-bottom:10px}.widget-hbox .widget-label{min-width:10ex;padding-right:8px;padding-top:5px;text-align:right;vertical-align:text-top}.widget-hbox .widget-readout{padding-left:8px;padding-top:5px;text-align:left;vertical-align:text-top}.widget-vbox{display:-webkit-box;-webkit-box-orient:vertical;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:vertical;-moz-box-align:stretch;display:box;box-orient:vertical;box-align:stretch;display:flex;flex-direction:column;align-items:stretch}.widget-vbox .widget-label{padding-bottom:5px;text-align:center;vertical-align:text-bottom}.widget-vbox .widget-readout{padding-top:5px;text-align:center;vertical-align:text-top}.widget-box{box-sizing:border-box;-moz-box-sizing:border-box;-webkit-box-sizing:border-box;-webkit-box-align:start;-moz-box-align:start;box-align:start;align-items:flex-start}.widget-radio-box{display:-webkit-box;-webkit-box-orient:vertical;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:vertical;-moz-box-align:stretch;display:box;box-orient:vertical;box-align:stretch;display:flex;flex-direction:column;align-items:stretch;box-sizing:border-box;-moz-box-sizing:border-box;-webkit-box-sizing:border-box;padding-top:4px}.widget-radio-box label{margin-top:0}.widget-radio{margin-left:20px} .highlight .hll { background-color: #ffffcc } .highlight { background: #f8f8f8; } .highlight .c { color: #408080; font-style: italic } /* Comment */ .highlight .err { border: 1px solid #FF0000 } /* Error */ .highlight .k { color: #008000; font-weight: bold } /* Keyword */ .highlight .o { color: #666666 } /* Operator */ .highlight .cm { color: #408080; font-style: italic } /* Comment.Multiline */ .highlight .cp { color: #BC7A00 } /* Comment.Preproc */ .highlight .c1 { color: #408080; font-style: italic } /* Comment.Single */ .highlight .cs { color: #408080; font-style: italic } /* Comment.Special */ .highlight .gd { color: #A00000 } /* Generic.Deleted */ .highlight .ge { font-style: italic } /* Generic.Emph */ .highlight .gr { color: #FF0000 } /* Generic.Error */ .highlight .gh { color: #000080; font-weight: bold } /* Generic.Heading */ .highlight .gi { color: #00A000 } /* Generic.Inserted */ .highlight .go { color: #888888 } /* Generic.Output */ .highlight .gp { color: #000080; font-weight: bold } /* Generic.Prompt */ .highlight .gs { font-weight: bold } /* Generic.Strong */ .highlight .gu { color: #800080; font-weight: bold } /* Generic.Subheading */ .highlight .gt { color: #0044DD } /* Generic.Traceback */ .highlight .kc { color: #008000; font-weight: bold } /* Keyword.Constant */ .highlight .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */ .highlight .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */ .highlight .kp { color: #008000 } /* Keyword.Pseudo */ .highlight .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */ .highlight .kt { color: #B00040 } /* Keyword.Type */ .highlight .m { color: #666666 } /* Literal.Number */ .highlight .s { color: #BA2121 } /* Literal.String */ .highlight .na { color: #7D9029 } /* Name.Attribute */ .highlight .nb { color: #008000 } /* Name.Builtin */ .highlight .nc { color: #0000FF; font-weight: bold } /* Name.Class */ .highlight .no { color: #880000 } /* Name.Constant */ .highlight .nd { color: #AA22FF } /* Name.Decorator */ .highlight .ni { color: #999999; font-weight: bold } /* Name.Entity */ .highlight .ne { color: #D2413A; font-weight: bold } /* Name.Exception */ .highlight .nf { color: #0000FF } /* Name.Function */ .highlight .nl { color: #A0A000 } /* Name.Label */ .highlight .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */ .highlight .nt { color: #008000; font-weight: bold } /* Name.Tag */ .highlight .nv { color: #19177C } /* Name.Variable */ .highlight .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */ .highlight .w { color: #bbbbbb } /* Text.Whitespace */ .highlight .mb { color: #666666 } /* Literal.Number.Bin */ .highlight .mf { color: #666666 } /* Literal.Number.Float */ .highlight .mh { color: #666666 } /* Literal.Number.Hex */ .highlight .mi { color: #666666 } /* Literal.Number.Integer */ .highlight .mo { color: #666666 } /* Literal.Number.Oct */ .highlight .sb { color: #BA2121 } /* Literal.String.Backtick */ .highlight .sc { color: #BA2121 } /* Literal.String.Char */ .highlight .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */ .highlight .s2 { color: #BA2121 } /* Literal.String.Double */ .highlight .se { color: #BB6622; font-weight: bold } /* Literal.String.Escape */ .highlight .sh { color: #BA2121 } /* Literal.String.Heredoc */ .highlight .si { color: #BB6688; font-weight: bold } /* Literal.String.Interpol */ .highlight .sx { color: #008000 } /* Literal.String.Other */ .highlight .sr { color: #BB6688 } /* Literal.String.Regex */ .highlight .s1 { color: #BA2121 } /* Literal.String.Single */ .highlight .ss { color: #19177C } /* Literal.String.Symbol */ .highlight .bp { color: #008000 } /* Name.Builtin.Pseudo */ .highlight .vc { color: #19177C } /* Name.Variable.Class */ .highlight .vg { color: #19177C } /* Name.Variable.Global */ .highlight .vi { color: #19177C } /* Name.Variable.Instance */ .highlight .il { color: #666666 } /* Literal.Number.Integer.Long */ Application of the Monte Carlo integration in Options trading (part I) $$\\\\[2pt]$$ Igor Marfin [Unister Gmb@2014] < igor.marfin@unister.de > $$\\\\[40pt]$$ Table of Contents 0.1 Abstract 0.2 Initialization 0.3 Introduction 0.3.1 The Black–Scholes model of the option value 0.4 Integration methods (schemes) for the stock prices 0.4.1 Convergence of the Monte Carlo schemes 0.4.2 Quantile of the distribution of stochastic variables 0.5 Machinery to operate the Option trading strategies 0.5.1 Stock volatility 0.5.1.1 Historical volatility calculator 0.5.1.2 Instantaneous volatility calculator 0.5.1.3 Implied ( the future realized) volatility 0.5.2 Monte Carlo calculation of the Option's Payoff 0.6 Monte-Carlo Estimation of integrals 0.6.1 Monte-Carlo Importance Sampling 0.6.1.1 Reject/Accept method for the Monte-Carlo Importance Sampling 0.6.2 Monte-Carlo with Stratified Sampling 1.1 Abstract Today I will start a series of tutorials on numerical methods useful in solving Stochastic Differential Equations (SDE). The stochastic differential equation is usually governing the price evolution of a Option call or Option put. I am going to introduce you into the modeling the option tradings using python and Monte Carlo techniques. Honestly saying, I got an idea to investigate the topic of the modeling stochastic processes in the Finance a long time ago. Recently, I have bumped into a small pdf file (CamDavidson-Pilon, 2012) at the Cam Davidson-Pilon's github page with an Assignment on the stochastic processes in Economics. And I have decided to understand and learn basic points given in the tasks. Therefore, I target two aims, first one is a step-by-step academical understanding of methods to work with stochastic processes in the Finance and second one is development of numerical methods to model stochastic processes in the stock/option tradings. I have prepared the bitbucket repository https://bitbucket.org/iggy_floyd/monte_carlo_stochastic_process , where I have placed these tutorials. $$\\\\[5pt]$$ 1.2 Initialization To set up the python environment for the data analysis and make a nicer style of the notebook, one can run the following commands in the beginning of our modeling: In [2]: import sys sys . path = [ '/usr/local/lib/python2.7/dist-packages' ] + sys . path # to fix the problem with numpy: this replaces 1.6 version by 1.9 % matplotlib inline % pylab inline ion () import os import matplotlib import numpy as np import matplotlib.pyplot as pl import matplotlib as mpl import logging import pymc as pm # a plotter and dataframe modules import seaborn as sns # seaborn to make a nice plots of the data import pandas as pd import scipy.stats as stats # Set up logging. logger = logging . getLogger () logger . setLevel ( logging . INFO ) from book_format import load_style , figsize , set_figsize load_style () Populating the interactive namespace from numpy and matplotlib WARNING: pylab import has clobbered these variables: ['figsize'] `%matplotlib` prevents importing * from pylab and numpy Out[2]: @import url('http://fonts.googleapis.com/css?family=Source+Code+Pro'); @import url('http://fonts.googleapis.com/css?family=Vollkorn'); @import url('http://fonts.googleapis.com/css?family=Arimo'); div.cell{ width: 1200px; margin-left: 0% !important; margin-right: auto; } div.text_cell code { background: transparent; color: #000000; font-weight: 600; font-size: 11pt; font-style: bold; font-family: 'Source Code Pro', Consolas, monocco, monospace; } h1 { font-family: 'Open sans',verdana,arial,sans-serif; } div.input_area { background: #F6F6F9; border: 1px solid #586e75; } .text_cell_render h1 { font-weight: 200; font-size: 30pt; line-height: 100%; color:#c76c0c; margin-bottom: 0.5em; margin-top: 1em; display: block; white-space: wrap; } h2 { font-family: 'Open sans',verdana,arial,sans-serif; } .text_cell_render h2 { font-weight: 200; font-size: 16pt; font-style: italic; line-height: 100%; color:#c76c0c; margin-bottom: 0.5em; margin-top: 1.5em; display: inline; white-space: wrap; } h3 { font-family: 'Open sans',verdana,arial,sans-serif; } .text_cell_render h3 { font-weight: 200; font-size: 14pt; line-height: 100%; color:#d77c0c; margin-bottom: 0.5em; margin-top: 2em; display: block; white-space: nowrap; } h4 { font-family: 'Open sans',verdana,arial,sans-serif; } .text_cell_render h4 { font-weight: 100; font-size: 14pt; color:#d77c0c; margin-bottom: 0.5em; margin-top: 0.5em; display: block; white-space: nowrap; } h5 { font-family: 'Open sans',verdana,arial,sans-serif; } .text_cell_render h5 { font-weight: 200; font-style: normal; color: #1d3b84; font-size: 16pt; margin-bottom: 0em; margin-top: 0.5em; display: block; white-space: nowrap; } div.text_cell_render{ font-family: 'Arimo',verdana,arial,sans-serif; line-height: 125%; font-size: 120%; text-align:justify; text-justify:inter-word; } div.output_subarea.output_text.output_pyout { overflow-x: auto; overflow-y: scroll; max-height: 50000px; } div.output_subarea.output_stream.output_stdout.output_text { overflow-x: auto; overflow-y: scroll; max-height: 50000px; } div.output_wrapper{ margin-top:0.2em; margin-bottom:0.2em; } code{ font-size: 70%; } .rendered_html code{ background-color: transparent; } ul{ margin: 2em; } ul li{ padding-left: 0.5em; margin-bottom: 0.5em; margin-top: 0.5em; } ul li li{ padding-left: 0.2em; margin-bottom: 0.2em; margin-top: 0.2em; } ol{ margin: 2em; } ol li{ padding-left: 0.5em; margin-bottom: 0.5em; margin-top: 0.5em; } ul li{ padding-left: 0.5em; margin-bottom: 0.5em; margin-top: 0.2em; } a:link{ font-weight: bold; color:#447adb; } a:visited{ font-weight: bold; color: #1d3b84; } a:hover{ font-weight: bold; color: #1d3b84; } a:focus{ font-weight: bold; color:#447adb; } a:active{ font-weight: bold; color:#447adb; } .rendered_html :link { text-decoration: underline; } .rendered_html :hover { text-decoration: none; } .rendered_html :visited { text-decoration: none; } .rendered_html :focus { text-decoration: none; } .rendered_html :active { text-decoration: none; } .warning{ color: rgb( 240, 20, 20 ) } hr { color: #f3f3f3; background-color: #f3f3f3; height: 1px; } blockquote{ display:block; background: #fcfcfc; border-left: 5px solid #c76c0c; font-family: 'Open sans',verdana,arial,sans-serif; width:1000px; padding: 10px 10px 10px 10px; text-align:justify; text-justify:inter-word; } blockquote p { margin-bottom: 0; line-height: 125%; font-size: 100%; } MathJax.Hub.Config({ TeX: { extensions: [\"AMSmath.js\"] }, tex2jax: { inlineMath: [ ['$','$'], [\"\\\\(\",\"\\\\)\"] ], displayMath: [ ['$$','$$'], [\"\\\\[\",\"\\\\]\"] ] }, displayAlign: 'center', // Change this to 'center' to center equations. \"HTML-CSS\": { scale:100, availableFonts: [\"Neo-Euler\"], preferredFont: \"Neo-Euler\", webFont: \"Neo-Euler\", styles: {'.MathJax_Display': {\"margin\": 4}} } }); In [3]: %% javascript IPython . load_extensions ( \"calico-spell-check\" , \"calico-document-tools\" , \"calico-cell-tools\" ); <IPython.core.display.Javascript object> In [3]: %% javascript // to use latex environment, please install the plugin first: // sudo ipython install-nbextension https://rawgit.com/jfbercher/latex_envs/master/latex_envs.zip require ( \"base/js/utils\" ). load_extensions ( \"latex_envs/latex_envs\" ) <IPython.core.display.Javascript object> $$\\\\[5pt]$$ 1.3 Introduction A typical stochastic differential equation is of the form (Wiki:SDE, 2016) \\begin{eqnarray}\\label{def:SDE} \\mathrm{d} X_t = \\mu(X_t,t)\\, \\mathrm{d} t + \\sigma(X_t,t)\\, \\mathrm{d} B_t , \\end{eqnarray}$$\\\\[1pt]$$ where $X_t$ is a price of the asset (option),which follows to the stochastic process, $\\mu(X_t,t)$ is a so-called trend function and $\\sigma(X_t,t) \\mathrm{d} B_t$ is a variation defined by the Wiener process $B_t$. Before we go further and start a simple analysis, we want to understand the basics of the stochastic calculations. There is an important Itô's lemma (Wiki:Ito_Lemmma, 2016) , which is used in to find the differential of a time-dependent function of a stochastic process. This is an informal derivation of the lemma. Assume $X_t$ is a Ito drift-diffusion process that satisfies the stochastic differential equation \\ref{def:SDE}, where $\\mu(X_t,t)=\\mu_t$ and $\\sigma(X_t,t) = \\sigma_t$ functions are constans. If f(t,x) is a twice-differentiable scalar function, its expansion in a Taylor series is \\begin{eqnarray} df = \\frac{\\partial f}{\\partial t}\\,dt + \\frac{\\partial f}{\\partial x}\\,dx + \\frac{1}{2}\\frac{\\partial&#94;2 f}{\\partial x&#94;2}\\,dx&#94;2 + \\cdots , \\end{eqnarray}\\begin{eqnarray} df = \\frac{\\partial f}{\\partial t}\\,dt + \\frac{\\partial f}{\\partial x}(\\mu_t\\,dt + \\sigma_t\\,dB_t) + \\frac{1}{2}\\frac{\\partial&#94;2 f}{\\partial x&#94;2} \\left (\\mu_t&#94;2\\,dt&#94;2 + 2\\mu_t\\sigma_t\\,dt\\,dB_t + \\sigma_t&#94;2\\,dB_t&#94;2 \\right ) + \\cdots. \\end{eqnarray} In the limit as $dt \\rightarrow 0$, the terms $dt&#94;2$ and $dtdBt$ tend to zero faster than $dB&#94;2$, which is $O(dt)$. Setting them to zero, substituting dt for $dB&#94;2$, and collecting the $dt$ and $dB$ terms, we obtain \\begin{eqnarray}\\label{def:ito} df = \\left(\\frac{\\partial f}{\\partial t} + \\mu_t\\frac{\\partial f}{\\partial x} + \\frac{\\sigma_t&#94;2}{2}\\frac{\\partial&#94;2 f}{\\partial x&#94;2}\\right)dt + \\sigma_t\\frac{\\partial f}{\\partial x}\\,dB_t. \\end{eqnarray} The equation \\ref{def:ito} has an important meaning: the two-differential function of the stochastic process is also a stochastic process. $$\\\\[5pt]$$ 1.3.1 The Black–Scholes model of the option value Itô's lemma is used to model the behavior of the option values. This model is called the the Black–Scholes model for options. What is an option and what does the value of the option mean? To answer the questions, I address you to the wiki page (Wiki:option, 2016) on the option trading. The important term of the option trading is the value $V_t$ of the option. Basically, the value of the option is just the profit which would be obtained from selling the option at the moment of the expiry date, i.e. \\begin{eqnarray} V_t = (S_t - C_t)\\cdot N_s, \\end{eqnarray} where $S_t$, $C_t$ and $N_s$ are the stock price at the moment $t$ (before or at day of the expirity), the strike price and the number of the stock shares in the option. Suppose a stock price follows a geometric Brownian motion given by the stochastic differential equation \\cite{def:SDE}. Then, for the value of an option at time $t$ $V(t, S_t)$, Itô's lemma gives \\begin{eqnarray} dV(t,S_t) = \\left(\\frac{\\partial V}{\\partial t} + \\frac{1}{2}\\left(S_t\\sigma\\right)&#94;2\\frac{\\partial&#94;2 V}{\\partial S&#94;2}\\right)\\,dt +\\frac{\\partial V}{\\partial S}\\,dS_t. \\end{eqnarray} This formula has a simple explanation: Increase $dV(t,S_t)$ of the option value is proportional to the profit of having an amount $\\frac{\\partial V}{\\partial S}$ of the stock with rising prise $S_t$ and increasing process behavior because of its stochastic nature. The last point is proportional to the magnitude of the volatility of the process. The larger variation the stock price shows, the more profit can be achieved by buying the option in the case when the tendency $$ \\frac{\\partial&#94;2 V}{\\partial S&#94;2} >0 $$ is fulfilled. If we buy the option, and our left cash held is assumed to grow at the risk free rate $r$, then the increase of the value $V_t$ at the time $t$ of our portfolio satisfies the following equation \\begin{eqnarray} dV_t = r\\left(V_t-\\frac{\\partial V}{\\partial S}S_t\\right)\\,dt + \\frac{\\partial V}{\\partial S}\\,dS_t, \\end{eqnarray} where the first term describes the profit we can obtain from putting our money into the risk-free investment fonds. Here I use the the notation of the option value $V_t$ for total portfolio value. The above formula of the total portfolio increase only makes a sense in the case when you have already bought the option. Combining the two equations gives the celebrated Black–Scholes equation \\begin{eqnarray}\\label{def:bs} \\frac{\\partial V}{\\partial t} + \\frac{\\sigma&#94;2S&#94;2}{2}\\frac{\\partial&#94;2 V}{\\partial S&#94;2} + rS\\frac{\\partial V}{\\partial S}-rV = 0. \\end{eqnarray} If we could integrate \\cite{def:bs} to predict the value of the option in the future time $t$, we would be able to minimize the risk by skipping from consideration such options which values will be predicted to be lowering. $$\\\\[5pt]$$ 1.4 Integration methods (schemes) for the stock prices To introduce integration techniques for solving different SDE like the Black–Scholes model \\cite{def:bs}, we start our study from solving the first tasks of the Cam Davidson-Pilson assignment (CamDavidson-Pilon, 2012) . Consider the following class of interest rate models dr_t = k(b − r_t )dt + sigma r_t&#94;y dWt , t ≥ 0. For your selection of the parameters r0 , k, b, sigma, and y (but y should be different > from 1), use the Euler scheme, the Milstein scheme, and the second order approximation scheme to generate paths of this process. (a) Using the three schemes to simulate paths, propose a Monte Carlo simulation method > to find q that satisfies P{r3 > q} ≤ 0.1. By considering the time step ∆ equal either to 0.004 or to 0.1, discuss accuracy of each method. First we write a simple simulator of the stochastic process using Euler, Milstein and Runge-Kutta schemes. In [5]: ''' An example of the stochastic simulation ''' from numpy.random import standard_normal from numpy import array , zeros , sqrt , shape from pylab import * import numpy as np # Let's consider this function # dr_t = k(b − r_t )dt + sigma r_t&#94;y dWt , t ≥ 0. # It exactly matches to the Euler recurrent scheme # our parameters k = 2e-1 b = 7e0 sigma = 5e-1 y = 1.1 # an Euler Scheme def rt_Euler ( rt_prev , k , b , dt , sigma , y , dWt ): ''' rate simulator: Euler scheme ''' return rt_prev + k * ( b - rt_prev ) * dt + sigma * pow (( rt_prev ), y ) * dWt # a scheme functor depends on r, delta_t, delta_W stochastic_Euler = lambda r , dt , dWt : rt_Euler ( r , k , b , dt , sigma , y , dWt ) def Path ( func , x_0 = 1e-9 , T = 100 , dt = 1e0 , N_Sim = 3 ): ''' generates the path of the stochastic process using schema ''' Steps = round ( T / dt ); #Steps in years S = zeros ([ N_Sim , Steps ], dtype = float ) x = range ( 0 , int ( Steps ), 1 ) for j in range ( 0 , N_Sim , 1 ): S [ j , 0 ] = x_0 for i in x [: - 1 ]: dWt = sqrt ( dt ) * standard_normal () val = func ( S [ j , i ], dt , dWt ) S [ j , i + 1 ] = x_0 if np . isnan ( val ) else val plot ( x , S [ j ], label = 'asset %i ' % j ) title ( ' %d Simulations of %d Days' % ( int ( N_Sim ), int ( Steps ))) xlabel ( 'time (days)' ) ylabel ( 'stock price' ) legend ( loc = \"upper left\" ) show () Path ( stochastic_Euler ) # a Milstein Scheme # Y_{n + 1} = Y_n + a(Y_n) \\Delta t + b(Y_n) \\Delta W_n + # \\frac{1}{2} b(Y_n) b'(Y_n) \\left( (\\Delta W_n)&#94;2 - \\Delta t \\right), def afunc ( rt_prev , k , b ): return k * ( b - rt_prev ) def bfunc ( rt_prev , sigma , y ): return sigma * pow (( rt_prev ), y ) def bfuncprime ( rt_prev , sigma , y ): return sigma * y * pow (( rt_prev ), y - 1 ) def rt_Milstein ( rt_prev , k , b , dt , sigma , y , dWt ): ''' rate simulator: Milstein scheme ''' return rt_prev + afunc ( rt_prev , k , b ) * dt + bfunc ( rt_prev , sigma , y ) * dWt + 0.5 * bfunc ( rt_prev , sigma , y ) * bfuncprime ( rt_prev , sigma , y ) * ( dWt * dWt - dt ) stochastic_Milstein = lambda r , dt , dWt : rt_Milstein ( r , k , b , dt , sigma , y , dWt ) Path ( stochastic_Milstein ) # Runge–Kutta method or the second order approximation scheme def rt_Middle ( rt_prev , k , b , dt , sigma , y ): return rt_prev + afunc ( rt_prev , k , b ) * dt + bfunc ( rt_prev , sigma , y ) * pow ( dt , 0.5 ) def rt_RG ( rt_prev , k , b , dt , sigma , y , dWt ): _rt = rt_Middle ( rt_prev , k , b , dt , sigma , y ) return rt_prev + afunc ( rt_prev , k , b ) * dt + bfunc ( rt_prev , sigma , y ) * dWt + 0.5 * ( bfunc ( _rt , sigma , y ) - bfunc ( rt_prev , sigma , y )) * ( dWt * dWt - dt ) * pow ( dt , - 0.5 ) stochastic_RG = lambda r , dt , dWt : rt_RG ( r , k , b , dt , sigma , y , dWt ) Path ( stochastic_RG ) def PathComparison ( funcs , x_0 = 1e-9 , T = 100 , dt = 1e0 ): ''' generates the path of the stochastic process using schema ''' Steps = round ( T / dt ); #Steps in years N_Sim = len ( funcs ) S = zeros ([ N_Sim , Steps ], dtype = float ) x = range ( 0 , int ( Steps ), 1 ) for j in range ( 0 , N_Sim , 1 ): S [ j , 0 ] = x_0 for i in x [: - 1 ]: dWt = sqrt ( dt ) * standard_normal () for j in range ( 0 , N_Sim , 1 ): val = funcs [ j ]( S [ j , i ], dt , dWt ) S [ j , i + 1 ] = x_0 if np . isnan ( val ) else val for j in range ( 0 , N_Sim , 1 ): plot ( x , S [ j ], label = 'scheme %s ' % funcs [j].func_code.co_names[0]) title ( ' %d Simulations of %d Days' % ( int ( N_Sim ), int ( Steps ))) xlabel ( 'time (days)' ) ylabel ( 'stock price' ) legend ( loc = \"upper left\" ) show () PathComparison ([ stochastic_Euler , stochastic_Milstein , stochastic_RG ]) /usr/local/lib/python2.7/dist-packages/IPython/kernel/__main__.py:35: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future /usr/local/lib/python2.7/dist-packages/IPython/kernel/__main__.py:93: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future Our generators look good. 1.4.1 Convergence of the Monte Carlo schemes How good is the convergence of each scheme? To answer this question, I will use a method proposed in this lecture (JanPalcyewski, 2016) . In [6]: ''' Estimation of the Convergence for different schemes of SDE simulations ''' def X_t_dt ( t_n , dt , X_n , X_n_1 ): t = np . random . uniform ( t_n , dt ) return X_n + ( t - t_n ) * ( X_n_1 - X_n ) / dt def PathForConvergenceHighPrec ( func , x_0 = 1e-9 , T = 100 , dt = 1e0 , N_Sim = 300 ): ''' generates the path of the stochastic process using some schema with High Precision ''' Steps = round ( T / dt ); #Steps in years S = zeros ([ N_Sim , Steps ], dtype = float ) x = range ( 0 , int ( Steps ), 1 ) for j in range ( 0 , N_Sim , 1 ): S [ j , 0 ] = x_0 for i in x [: - 1 ]: dWt = sqrt ( dt ) * standard_normal () val = func ( S [ j , i ], dt , dWt ) S [ j , i + 1 ] = x_0 if np . isnan ( val ) else val mean = np . mean ( S , axis = 0 ) ts = np . empty ( Steps ) ts . fill ( dt ) return mean , np . cumsum ( ts ) path_high_prec , x = PathForConvergenceHighPrec ( stochastic_Euler , dt = 1e-3 ) def ReplicatePathShape ( path1 , path2 ): return np . repeat ( path1 , path2 . shape [ 0 ] / path1 . shape [ 0 ]) def PathToTestConvergence ( func , x_0 = 1e-9 , T = 100 , dt = 1e0 , N_Sim = 300 ): ''' generates the path of the stochastic process using some schema ''' Steps = round ( T / dt ); #Steps in years S = zeros ([ N_Sim , Steps ], dtype = float ) x = range ( 0 , int ( Steps ), 1 ) for j in range ( 0 , N_Sim , 1 ): S [ j , 0 ] = x_0 for i in x [: - 1 ]: dWt = sqrt ( dt ) * standard_normal () val = func ( S [ j , i ], dt , dWt ) S [ j , i + 1 ] = x_0 if np . isnan ( val ) else val mean = np . mean ( S , axis = 0 ) return mean /usr/local/lib/python2.7/dist-packages/IPython/kernel/__main__.py:14: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future /usr/local/lib/python2.7/dist-packages/IPython/kernel/__main__.py:24: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future Then I define the estimator of the strong convergence as it was supposed in the lecture. Also I illustrate the convergence of the Euler scheme and estimate the convergence limit parameters. In [12]: def ConvergenceStrong ( func , dt , path_ref , N_Sim = 100 ): path = PathToTestConvergence ( func , dt = dt , N_Sim = N_Sim ) path = ReplicatePathShape ( path , path_ref ) return np . mean ( np . absolute ( path - path_ref )) deltaT = [ 5e0 , 1e0 , 5e-1 , 4e-1 , 2e-1 , 1e-1 , 5e-2 , 2e-2 ] def ConvergenceStrongSeries ( func , path_ref , deltaT ): return [ ConvergenceStrong ( func , dt , path_ref ) for dt in deltaT ] # for dt in deltaT: # yield ConvergenceStrong(func,dt,path_ref) # to produce some ensemble of the different convergences how_many = 10 ConvergenceStrongSeriesMany = [ ConvergenceStrongSeries ( stochastic_Euler , path_high_prec , deltaT ) for i in range ( how_many )] ConvergenceSeries = ConvergenceStrongSeries ( stochastic_Euler , path_high_prec , deltaT ) scheme = stochastic_Euler . func_code . co_names [ 0 ] def plotConvergengeSingle ( deltaT , ConvergenceSeries , scheme ): fig , ( ax1 , ax2 ) = plt . subplots ( 2 , 1 , figsize = ( 12 , 12 )) # a first plot: improving convergence with descreasing deltaT ax1 . plot ( range ( len ( deltaT )), ConvergenceSeries ) ax1 . set_title ( \"Convergence of the %s \" % scheme ) # a second plot: convergence as a function of the deltaT ax2 . plot ( deltaT , ConvergenceSeries , label = 'emperical' ) ax2 . plot ( deltaT , deltaT , label = 'theoretical' ) ax2 . set_title ( \"Convergence of the %s \" % sc heme ) ax2 . legend ( loc = \"upper left\" ) show () def plotConvergengeMultiple ( deltaT , ConvergenceStrongSeriesMany , scheme ): ''' it is as plotConvergengeSingle but it plots multiple stochastic series of the convergencies ''' fig , ( ax1 , ax2 ) = plt . subplots ( 2 , 1 , figsize = ( 12 , 12 )) for ConvergenceSeries in ConvergenceStrongSeriesMany : ax1 . plot ( range ( len ( deltaT )), ConvergenceSeries ) ax1 . set_title ( \"Convergence of the %s \" % scheme ) # for i , ConvergenceSeries in enumerate ( ConvergenceStrongSeriesMany ): ax2 . plot ( deltaT , ConvergenceSeries , label = 'emperical %d ' % i ) ax2 . plot ( deltaT , deltaT , label = 'theoretical' ) ax2 . set_title ( \"Convergence of the %s \" % sc heme ) ax2 . legend ( loc = \"upper left\" ) show () from scipy import optimize def findConvergenceParams ( deltaT , ConvergenceSeries ): # fit our result to find convergence parameters # method 1 #fitfunc = lambda p, x: p[0]*pow(x,p[1])# Target function #errfunc = lambda p, x, y: fitfunc(p, x) - y # Distance to the target function #p0 = [1., 1.] # Initial guess for the parameters #p1, success = optimize.leastsq(errfunc, p0[:], args=(np.array(deltaT), np.array(ConvergenceSeries))) # method 2 def f ( x , a , b ): return a * pow ( x , b ) p1 = optimize . curve_fit ( f , np . array ( deltaT ), np . array ( ConvergenceSeries ))[ 0 ] return p1 def findConvergenceParamsFromMany ( deltaT , ConvergenceStrongSeriesMany ): ''' fit parameters of the convergence limits from several stochastic series and return the average values of the parameters ''' res = [] for ConvergenceSeries in ConvergenceStrongSeriesMany : res += [ findConvergenceParams ( deltaT , ConvergenceSeries )] #print res return np . mean ( np . array ( res ), axis = 0 ) . tolist () p1 = findConvergenceParams ( deltaT , ConvergenceSeries ) p2 = findConvergenceParamsFromMany ( deltaT , ConvergenceStrongSeriesMany ) #print p2 print \"A Convergence limit obtained from a random path: {0:.2f}*deltaT&#94;{1:.2f}\" . format ( * p1 ) print \"A Convergence limit obtained from an ensemble of paths: {0:.2f}*deltaT&#94;{1:.2f}\" . format ( * p2 ) plotConvergengeMultiple ( deltaT , ConvergenceStrongSeriesMany , scheme ) A Convergence limit obtained from a random path: 1.57*deltaT&#94;0.21 A Convergence limit obtained from an ensemble of paths: 2.42*deltaT&#94;0.78 /usr/local/lib/python2.7/dist-packages/IPython/kernel/__main__.py:10: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future From previous estimations, we get that the strong convergence for the Euler scheme is about $$\\lim_{\\delta \\to 0}E(|S_t&#94;T - S_t&#94;{Euler}|)\\sim 1.79*{\\delta t}&#94;{0.45},$$ where $S_t&#94;T$ is an exact solution of the SDE. 1.4.2 Quantile of the distribution of stochastic variables Let's try to answer the question (a): (a) Using the three schemes to simulate paths, propose a Monte Carlo simulation method > to find q that satisfies P{r3 > q} ≤ 0.1. In [13]: def PathGenerator ( func , x_0 = 1e-9 , T = 100 , dt = 1e0 , N_Sim = 1000 ): ''' generates the path of the stochastic process using some schema ''' Steps = round ( T / dt ); #Steps in years S = zeros ([ N_Sim , Steps ], dtype = float ) x = range ( 0 , int ( Steps ), 1 ) for j in range ( 0 , N_Sim , 1 ): S [ j , 0 ] = x_0 for i in x [: - 1 ]: dWt = sqrt ( dt ) * standard_normal () val = func ( S [ j , i ], dt , dWt ) S [ j , i + 1 ] = x_0 if np . isnan ( val ) else val return S distr3elem = PathGenerator ( stochastic_Euler )[:, 2 ] hist ( distr3elem ) # get a r3 distributions (3rd step of the monte carlo simulation) title ( \"A distribution of the 3rd element from Euler scheme\" ) # calculate the quantile of 0.1 quantile = 0.1 # method 1 from scipy.stats.mstats import mquantiles print \"Method #1: Pr(r3>{0:.2f})<=0.1\" . format ( * mquantiles ( distr3elem , prob = [ 1. - quantile ])) # method 2 using the normal approximation import scipy mean = np . mean ( distr3elem ) var = np . std ( distr3elem ) print \"Method #2: Pr(r3>{0:.2f})<=0.1\" . format ( * scipy . stats . norm . ppf ( [ 1. - quantile ], mean , var )) Method #1: Pr(r3>3.47)<=0.1 Method #2: Pr(r3>3.46)<=0.1 /usr/local/lib/python2.7/dist-packages/IPython/kernel/__main__.py:6: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future Here I repeat all previous exercises for the Milstein scheme. In [14]: # let's get our theoretical solution of the stochastic differential equation :-) path_high_prec , x = PathForConvergenceHighPrec ( stochastic_Milstein , dt = 1e-3 ) /usr/local/lib/python2.7/dist-packages/IPython/kernel/__main__.py:14: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future /usr/local/lib/python2.7/dist-packages/IPython/kernel/__main__.py:24: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future In [15]: # now produce a set of the convergence series ConvergenceStrongSeriesMany = [ ConvergenceStrongSeries ( stochastic_Milstein , path_high_prec , deltaT ) for i in range ( how_many )] ConvergenceSeries = ConvergenceStrongSeries ( stochastic_Milstein , path_high_prec , deltaT ) scheme = stochastic_Milstein . func_code . co_names [ 0 ] /usr/local/lib/python2.7/dist-packages/IPython/kernel/__main__.py:10: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future In [16]: # here we find parameters of the convergence limit p1 = findConvergenceParams ( deltaT , ConvergenceSeries ) p2 = findConvergenceParamsFromMany ( deltaT , ConvergenceStrongSeriesMany ) In [17]: # plot obtained results print \"A Convergence limit obtained from a random path: {0:.2f}*deltaT&#94;{1:.2f}\" . format ( * p1 ) print \"A Convergence limit obtained from an ensemble of paths: {0:.2f}*deltaT&#94;{1:.2f}\" . format ( * p2 ) plotConvergengeMultiple ( deltaT , ConvergenceStrongSeriesMany , scheme ) A Convergence limit obtained from a random path: 1.48*deltaT&#94;2.30 A Convergence limit obtained from an ensemble of paths: 1.74*deltaT&#94;1.06 In [18]: # a quantile study distr3elem = PathGenerator ( stochastic_Milstein )[:, 2 ] hist ( distr3elem ) # get a r3 distributions (3rd step of the monte carlo simulation) title ( \"A distribution of the 3rd element from Milstein scheme\" ) print \"Method #1: Pr(r3>{0:.2f})<=0.1\" . format ( * mquantiles ( distr3elem , prob = [ 1. - quantile ])) Method #1: Pr(r3>3.54)<=0.1 /usr/local/lib/python2.7/dist-packages/IPython/kernel/__main__.py:6: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future Here is a summary on the obtained results for the Milstein scheme the strong convergence is about $$\\lim_{\\delta \\to 0}E(|S_t&#94;T - S_t&#94;{Milstein}|)\\sim 1.74*{\\delta t}&#94;{1.06} $$ Pr(r3>3.60)<=0.1 $$\\\\[5pt]$$ 1.5 Machinery to operate the Option trading strategies Now it is a time when we can introduce a few useful terms needed to understand what the European vanilla option is. From the wiki about Options (Wiki:option, 2016) , we can conclude that an option is a contract which gives the buyer (the owner or holder of the option) the right, but not the obligation, to buy or sell an underlying asset or instrument at a specified strike price on or before a specified date; there are the Call Option, which is a contract to buy stock shares and the Put Option, which guarantees you to sell stocks at the strike price. We restrict ourselves to consideration of the Call Options . Call Options have the following properties (I have just copied-pasted the wiki text here): Strike price : this is the price at which you can buy the stock (if you have bought a call option) or the price at which you must sell your stock (if you have sold a call option). Expiry date : this is the date on which the option expires, or becomes worthless, if the buyer doesn't exercise it. Premium : this is the price you pay when you buy an option and the price you receive when you sell an option. The current price of the Call Option on the market $S_t&#94;{Option}$ is defined by the formula \\begin{eqnarray}\\label{def:priceOptionCall} S_t&#94;{Option} = S_t&#94;{stock} + Premium - S&#94;{Strike}, \\end{eqnarray}$$\\\\[1pt]$$ where $S_t&#94;{stock}$ is the current stock price, $Premium$ is the price you have to pay to the option holder because of the volatility of the stock prices. The call premium tends to go down as the option gets closer to the call date. And it goes down as the option price rises relative to the stock price. Also it is worth to mention that there are Bid/Ask prices of the option which are is more relevant in ascertaining the value of the option than the last price since options are not frequently traded. Meaning the value is usually the Ask/Bid Price . I want to clarify the Bid/Ask price definitions a little bit. As Investopedia (Investopedia, 2016) states, the Bid/Ask/Spread terms on the market are fully determined by the concept of the Supply and Demand . Supply refers to the volume or abundance of a particular item in the marketplace, such as the supply of stock for sale. Demand refers to an individual's willingness to pay a particular price for an item or stock. Suppose that a one-of-a-kind diamond is found in the remote countryside of Africa by a miner. Two potential buyers make themselves known about the diamond and submit bids for \\$1.2 million and \\$1.3 million dollars ( the Bid prices ), respectively. The asking ( Ask ) price of that diamond will be \\$1.3 million dollars more likely. The spread is the difference between the bid and asking prices for a particular security. An option usually covers 100 shares. So the bid/ask price should be multiplied by 100 to get the total cost . Let`s consider a realistic example for trading option strategy from the wiki: Let's say we bought 3 PNC strike \\$45, January 2012 call options in August for \\$11.75. That means we paid \\$3,525 (11.75 \\* 3 options for 100 shares each) for the right to buy 300 (3\\*100) PNC shares for \\$45 per share between now and January 2012. The stock at that time traded at \\$50.65 meaning the theoretical call premium was \\$6.1 as shown by our formula: (current price + theoretical time/volatility premium) – strike price, (50.65 + 6.1 – 45 = 11.75). Today the stock is trading at \\$64 making the call option worth \\$19.45 with a theoretical call premium now of 45 cents above its in-the-money intrinsic value \\$19 (the \\$64 market price minus the call option \\$45 strike price). First, we want to develop a simple scrambler to get data from Yahoo Finance. The pandas framework has an experimental tool pandas.io.data.Options which directly operates with Yahoo Finance!. This tool requires lxml library to be installed in the system. To install lxml support for the python in the Debian , we can apt-get like it is shown below: sudo apt-get install python-lxml In [19]: \"\"\" A simple Scrapper of the Option data \"\"\" from pandas.io.data import Options import datetime def get_option_data ( ticker , exp_date , strike_price_slice , type = \"call\" ): \"\"\" return call and put options \"\"\" x = Options ( ticker , 'yahoo' ) index = ( strike_price_slice , slice ( None ), type ) return x . get_options_data ( expiry = exp_date ) . loc [ index ,:] # our test parameters of the option market # I want to get information on options with # 1) expiry--> 1st October of 2016 expiry = datetime . date ( 2016 , 10 , 1 ) # 2) stock name (underlying) ticker = 'aapl' # 3) slice on the strike price strike_price_slice = slice ( 100. , 110. , 1 ) # 4) type of the option: 'call' option_type = \"call\" options_data = get_option_data ( ticker , expiry , strike_price_slice , option_type ) options_data Out[19]: Last Bid Ask Chg PctChg Vol Open_Int IV Root IsNonstandard Underlying Underlying_Price Quote_Time Strike Expiry Type Symbol 100 2016-10-21 call AAPL161021C00100000 14.95 14.85 15.00 -0.31 -2.03% 65 11208 25.91% AAPL False AAPL 111.58 2016-04-14 10:25:00 105 2016-10-21 call AAPL161021C00105000 11.55 11.45 11.55 -0.25 -2.12% 18 7428 25.07% AAPL False AAPL 111.58 2016-04-14 10:25:00 110 2016-10-21 call AAPL161021C00110000 8.60 8.55 8.65 -0.28 -3.15% 183 13655 24.52% AAPL False AAPL 111.58 2016-04-14 10:25:00 OK. What can be done else? We will need to have a calculator of the stock volatility. $$\\\\[5pt]$$ 1.5.1 Stock volatility There are many definitions of the Stock Volatility. I will try to undercover all of the them. First, let's recover the difinion of the stock return. Suppose $S_t$ is the price of the stock on day $t$, then the daily return on day $t$ is: \\begin{eqnarray}\\label{def:return} r_t = \\frac{ S_t - S_{t-1} }{ S_{t-1} }. \\end{eqnarray}$$\\\\[1pt]$$ Also logarithmic returns are defined according to the Financial Mathematics (WikiVolatility, 2016) . The logarithmic returns are determined as follows \\begin{eqnarray}\\label{def:logreturn} logr_t = log(r_t+1). \\end{eqnarray}$$\\\\[1pt]$$ Let's implement the calculation of returns for stocks. I will copy/paste a few code from my previous study \"Money Management System: Portfolio Optimization Approaches or How to Become a Millionaire from stock tradings\" . In [20]: ''' Igor Marfin <Unister Gmb, 2014> igor.marfin@unister.de Extraction the financial data from Yahoo! Finance Returns calculation ''' from pandas.io.data import DataReader from datetime import datetime instruments = [ 'AAPL' ] def getStockPrices ( startdate = datetime ( 2016 , 3 , 1 ), enddate = datetime . now (), stocks = instruments ): '''returns the stock prices''' data = map ( lambda x : DataReader ( x , \"yahoo\" , startdate , enddate ), stocks ) for i , symbol in enumerate ( stocks ): data [ i ][ 'Symbol' ] = stocks [ i ] return data def addReturns ( dataframe , price = 'Close' ): '''adds the returns on the price''' # get Series on the interested column _series = dataframe [ price ] _series = _series . pct_change () dataframe [ price + '_return' ] = _series return dataframe def addLogReturns ( dataframe , price = 'Close' ): '''adds the logarithmic returns on the price''' # get Series on the interested column _series = dataframe [ price ] _series = _series . pct_change () _series = _series . apply ( lambda x : np . log ( x + 1 )) dataframe [ price + '_logreturn' ] = _series return dataframe # test of the reader and 'PctChange' adder data = getStockPrices () index = 0 # get data of the 1st instrument # check the difference between two definitions pd . Series ( addLogReturns ( addReturns ( data [ index ])) . dropna ()[ \"Close_return\" ] - addLogReturns ( addReturns ( data [ index ])) . dropna ()[ \"Close_logreturn\" ]) . plot () Out[20]: <matplotlib.axes._subplots.AxesSubplot at 0xd23b5ac> In [21]: # a plot of the logarithmic returns addLogReturns ( addReturns ( data [ index ])) . dropna ()[ \"Close_logreturn\" ] . plot () Out[21]: <matplotlib.axes._subplots.AxesSubplot at 0xd22f76c> So, the difference in results is marginal. $$\\\\[5pt]$$ 1.5.1.1 Historical volatility calculator $$\\\\[5pt]$$ Let's add several calculators of the stock volatility. In [22]: ''' Igor Marfin <Unister Gmb, 2014> igor.marfin@unister.de Volatility calculators ''' def VolatilityPriceStdDev ( dataframe , price = 'Close' ): ''' It returns Std Deviation of the price as its Volatility. More details can be found here http://stockcharts.com/school/doku.php?id=chart_school:technical_indicators:standard_deviation_volatility ''' #print dataframe[price].describe() return dataframe [ price ] . std () # test of the Std Deviation for price print 'Std Deviation for the Close price ' , VolatilityPriceStdDev ( data [ index ]) def VolatilityReturnsStdDev ( dataframe , price = 'Close' , type = \"return\" ): ''' calculate returns of the prices. More details https://gist.github.com/johntyree/4587049 http://www.arngarden.com/2013/06/02/calculating-volatility-of-multi-asset-portfolio-example-using-python/ ''' return dataframe [ price + '_' + type ] . std () # test of the Std Deviation for returns print 'Std Deviation for the return ' , VolatilityReturnsStdDev ( data [ index ]) print 'Std Deviation for the logarithmic return ' , VolatilityReturnsStdDev ( data [ index ], type = \"logreturn\" ) ''' Realizations of the Volatility given at https://en.wikipedia.org/wiki/Volatility_%28finance%29 They relates to so-called \"actual historical volatility\" ''' import math import datetime def annualized_historical_volatility ( past_rollback_in_days = 30 , stocks = instruments ): ''' returns the annualized volatility https://gist.github.com/johntyree/4587049 ''' enddate = datetime . datetime . now () startdate = enddate - datetime . timedelta ( days = past_rollback_in_days ) data = getStockPrices ( startdate = startdate , enddate = enddate , stocks = stocks ) data = [ addLogReturns ( addReturns ( data [ i ])) for i , elem in enumerate ( data )] return [ math . sqrt ( 252. ) * VolatilityReturnsStdDev ( elem , price = 'Close' , type = \"logreturn\" ) for _ , elem in enumerate ( data )] print 'Historical annualized volatility for last 30 days ' , annualized_historical_volatility () print 'Historical annualized volatility for last 60 days ' , annualized_historical_volatility ( past_rollback_in_days = 60 ) def historical_volatility ( period_fraction , past_rollback_in_days = 30 , stocks = instruments ): ''' return a generalized volatility for some period: day,monthly ''' vol = annualized_historical_volatility ( past_rollback_in_days = past_rollback_in_days , stocks = stocks ) return map ( lambda x : math . sqrt ( period_fraction ) * x , vol ) period_frac = 1. / 12. # to calculate monthly volatility: 12 months in 1 year print 'Historical generalized volatility for the period %.3f (in the fraction of year) %r ' % ( period_frac , historical_volatility ( period_frac )) period_frac = 1. / 252. # to calculate daily volatility: 252 working days in 1 year print 'Historical generalized volatility for the period %.3f (in the fraction of year) %r ' % ( period_frac , historical_volatility ( period_frac )) Std Deviation for the Close price 3.60323590933 Std Deviation for the return 0.0103423551266 Std Deviation for the logarithmic return 0.0103178135132 Historical annualized volatility for last 30 days [0.17287079679166056] Historical annualized volatility for last 60 days [0.19563449702221694] Historical generalized volatility for the period 0.083 (in the fraction of year) [0.049903500531345159] Historical generalized volatility for the period 0.004 (in the fraction of year) [0.010889836601340866] $$\\\\[5pt]$$ 1.5.1.2 Instantaneous volatility calculator $$\\\\[2pt]$$ Also sometimes it is useful to calculate so-called instantaneous volatility. I address you to these slides https://www.google.de/url?sa=t&rct=j&q=&esrc=s&source=web&cd=6&cad=rja&uact=8&ved=0ahUKEwiUoKCKtvTLAhVElw8KHT4uBlUQFgg2MAU&url=http%3A%2F%2Farchive.euroscipy.org%2Ffile%2F6368%2Fraw%2FESP11-Fast_MonteCarlo_Slides.pdf&usg=AFQjCNEZ8ZuJvqfRdgU_Buxm_Aji1d3zHA&sig2=eVWUafGBNSSS4f9Ymk0z9Q&bvm=bv.118443451,d.bGg about Monte Carlo simulations of the stock volatility where you can find the definition of the instantaneous volatility. Because the instantaneous volatility has a theoretical meaning (it is a parameter of the particular SDE), I can only approximate its value using intra-day data of the stock prices. All steps below will show how I calculate the instantaneous volatility. First, download the api to work out intra-day data from GOOGLE. In [58]: ! git clone https://github.com/maxvitek/intradata Cloning into 'intradata'... remote: Counting objects: 54, done.[K remote: Compressing objects: 100% (31/31), done.[K remote: Total 54 (delta 20), reused 54 (delta 20), pack-reused 0[K Unpacking objects: 100% (54/54), done. In [23]: # %load intradata/intradata.py import time import datetime import pandas import requests import csv import io import pytz PROTOCOL = 'http://' BASE_URL = 'www.google.com/finance/getprices' def get_google_data ( symbol , interval = 60 , lookback = 1 , end_time = time . time ()): \"\"\" Get intraday data for the symbol from google finance and return a pandas DataFrame :param symbol (str) :param interval (int) :param lookback (int) :param end_time (unix timestamp) :returns pandas.DataFrame \"\"\" resource_url = PROTOCOL + BASE_URL payload = { 'q' : symbol , 'i' : str ( interval ), 'p' : str ( lookback ) + 'd' , 'ts' : str ( int ( end_time * 1000 )), 'f' : 'd,o,h,l,c,v' } r = requests . get ( resource_url , params = payload ) quotes = [] with io . BytesIO ( r . content ) as csvfile : quote_reader = csv . reader ( csvfile ) timestamp_start = None timestamp_offset = None timezone_offset = 0 for row in quote_reader : if row [ 0 ][: 16 ] == 'TIMEZONE_OFFSET=' : timezone_offset = - 1 * int ( row [ 0 ][ 16 :]) elif row [ 0 ][ 0 ] not in 'a1234567890' : # discard headers continue elif row [ 0 ][ 0 ] == 'a' : # 'a' prepended to the timestamp that starts each day timestamp_start = pytz . utc . localize ( datetime . datetime . fromtimestamp ( float ( row [ 0 ][ 1 :])) + datetime . timedelta ( minutes = timezone_offset )) timestamp_offset = 0 elif timestamp_start : timestamp_offset = int ( row [ 0 ]) if not timestamp_start and not timestamp_offset : continue timestamp = timestamp_start + datetime . timedelta ( seconds = timestamp_offset * interval ) closing_price = float ( row [ 1 ]) high_price = float ( row [ 2 ]) low_price = float ( row [ 3 ]) open_price = float ( row [ 4 ]) volume = float ( row [ 5 ]) quotes . append (( timestamp , closing_price , high_price , low_price , open_price , volume )) df = pandas . DataFrame ( quotes , columns = [ 'datetime' , 'close' , 'high' , 'low' , 'open' , 'volume' ]) df = df . set_index ( 'datetime' ) return df Afterwards I implement the calculation of the instantaneous volatility using the following function. In [24]: ''' Igor Marfin <Unister Gmb, 2014> igor.marfin@unister.de Instantaneous Volatility Calculator ''' import time def instantaneous_volatility ( hours_ago = 20 , stocks = instruments ): ''' returns instantaneous volatility for one day ''' end_time = datetime . datetime . now () - datetime . timedelta ( hours = hours_ago ) data = map ( lambda x : get_google_data ( x , interval = 60 , lookback = 30 )[ end_time : datetime . datetime . now ()] , instruments ) data = [ addLogReturns ( addReturns ( data [ i ], price = 'close' ), price = 'close' ) for i , elem in enumerate ( data )] # math.sqrt(24.*60) is because we want to calculate the # instantaneous_volatility for one day using intra data with 1 minute resolution! #vol = lambda x: math.sqrt(24.*60.)*VolatilityReturnsStdDev(x,price='close',type=\"logreturn\") vol = lambda x : VolatilityReturnsStdDev ( x , price = 'close' , type = \"logreturn\" ) return map ( lambda x : math . sqrt ( float ( len ( x ))) * vol ( x ), data ) instantaneous_volatility ( hours_ago = 20 ) INFO:requests.packages.urllib3.connectionpool:Starting new HTTP connection (1): www.google.com Out[24]: [0.0070833300621983919] $$\\\\[5pt]$$ 1.5.1.3 Implied ( the future realized) volatility $$\\\\[2pt]$$ Black-Scholes model describes the option prices as a function of the underlying price, strike, risk-free interest rate, time to expiry and volatility: $$ V=BS(S,K,r,T, \\sigma), $$ where $V$ is a solution of the \\cite{def:bs}. The volatility value $\\sigma$ used here is an estimate of the future realized price volatility or so-called implied volatility. Given that the stock price $S$, the strike $K$, risk-free interest rate $r$, and time to expiry $T$ are all known and easily found, we can actually think of a price for an option in the market as a function of $\\sigma$ only. What we want to find is a root of the following equation, where $V$ is known $$V=BS(\\sigma).$$ Where can we get all this information on $S$, $K$, $r$, $T$ and $V$? All data can be found at https://www.google.com/finance/ . For example for 'AAPL', check this link https://www.google.com/finance/option_chain?q=NASDAQ:AAPL or http://www.google.com/finance/option_chain?q=AAPL&output=json . Also you can use the Yahoo Finance, i.e. https://finance.yahoo.com/q/op?s=AAPL . We have already built a scrapper to get these data from Yahoo Finance get_option_data(ticker,expiry,strike_price_slice,option_type) Where someone can get $r$,risk-free interest rate? I recommend to visit this page https://www.treasury.gov/resource-center/data-chart-center/interest-rates/Pages/TextView.aspx?data=longtermrate where I have found for \"ten year US Treasury rate value $r$ as 2.41%. Ok. Let's extract needed data. Some parameters should be provided by us: Strike price (K) Time to maturity (T) Ticker (Underlying) We want to get $V$ (the call option prise) when K=100 T=30 days Ticker = 'AAPL' In [25]: until_days = 30. expiry = datetime . datetime . now () + datetime . timedelta ( days = until_days ) T = until_days / 252. K = 110. ticker = 'aapl' r = 0.0241 S = getStockPrices ( datetime . datetime . now () - datetime . timedelta ( days = 1 ), enddate = datetime . datetime . now (), stocks = [ ticker ])[ 0 ][ 'Close' ] option_type = \"call\" options_data = get_option_data ( ticker , expiry , K , option_type ) V = options_data [ 'Last' ] . values [ 0 ] option_data = pd . DataFrame ({ 'T' :[ T ], 'expiry' :[ expiry ], 'K' :[ K ], 'ticker' :[ ticker ], 'V' :[ V ], 'r' :[ r ], 'S' : S }) option_data Out[25]: K S T V expiry r ticker Date 2016-04-13 110 112.040001 0.119048 4.6 2016-05-14 16:26:42.737908 0.0241 aapl We need numerically to solve this equation $$ 3.22\\$ = BS(109.01\\$,110\\$,0.024,0.12,\\sigma) ,$$ where $BS$ is the Black–Scholes formula (WikiBlack–Scholesformula, 2016) . In [26]: ''' Igor Marfin <Unister Gmb, 2014> igor.marfin@unister.de Implied Volatility Calculator ''' import scipy N = scipy . stats . norm . cdf def bs_price ( sigma , cp_flag , S , K , T , r ): ''' returns Black–Scholes formula \\begin{align} C(S, t) &= N(d_1)S - N(d_2) Ke&#94;{-r(T - t)} \\\\ d_1 &= \\frac{1}{\\sigma\\sqrt{T - t}}\\left[\\ln\\left(\\frac{S}{K}\\right) + \\left(r + \\frac{\\sigma&#94;2}{2}\\right)(T - t)\\right] \\\\ d_2 &= d_1 - \\sigma\\sqrt{T - t} \\\\ \\end{align} ''' d1 = ( np . log ( S / K ) + ( r + sigma * sigma / 2. ) * T ) / ( sigma * sqrt ( T )) d2 = d1 - sigma * np . sqrt ( T ) if cp_flag == 'call' : price = S * N ( d1 , 0.0 , 1.0 ) - K * exp ( - r * T ) * N ( d2 , 0.0 , 1.0 ) else : price = K * exp ( - r * T ) * N ( - d2 ) - S * N ( - d1 ) return price args = [ 0.01 , # fake sigma 'call' , option_data [ 'S' ] . values [ 0 ], option_data [ 'K' ] . values [ 0 ], option_data [ 'T' ] . values [ 0 ], option_data [ 'r' ] . values [ 0 ], ] print \"Option Price at the fake sigma %.2f is %.2f \" % ( 0.01 , bs_price ( * args )) args = [ 'call' , option_data [ 'S' ] . values [ 0 ], option_data [ 'K' ] . values [ 0 ], option_data [ 'T' ] . values [ 0 ], option_data [ 'r' ] . values [ 0 ], ] print args , option_data [ 'V' ] . values [ 0 ] def implied_volatility ( option_params , V ): ''' numerically solves the BS equation ''' # the secant method solver = scipy . optimize . newton _target_fun = lambda x : bs_price ( x , * option_params ) - V #print scipy.optimize.brentq(_target_fun, 0., 1.) return solver ( _target_fun , 1. ) impliedSigma = implied_volatility ( args , option_data [ 'V' ] . values [ 0 ]) args = [ impliedSigma ] + args [ 0 :] print \"Calculated %.2f and Real %.2f Option Prices at sigma %.2f \" % ( bs_price ( * args ), V , impliedSigma ) Option Price at the fake sigma 0.01 is 2.36 ['call', 112.040001, 110.0, 0.11904761904761904, 0.0241] 4.6 Calculated 4.60 and Real 4.60 Option Prices at sigma 0.22 In [27]: \"\"\" Newton's method of the solving BS formula. To validate the previous result \"\"\" def find_vol ( target_value , call_put , S , K , T , r ): MAX_ITERATIONS = 100 PRECISION = 1.0e-5 sigma = 0.5 for i in xrange ( 0 , MAX_ITERATIONS ): price = bs_price ( call_put , S , K , T , r , sigma ) vega = bs_vega ( call_put , S , K , T , r , sigma ) price = price diff = target_value - price # our root print i , sigma , diff if ( abs ( diff ) < PRECISION ): return sigma sigma = sigma + diff / vega # f(x) / f'(x) # value wasn't found, return best guess so far return sigma n = scipy . stats . norm . pdf N = scipy . stats . norm . cdf def bs_price ( cp_flag , S , K , T , r , v , q = 0.0 ): d1 = ( log ( S / K ) + ( r + v * v / 2. ) * T ) / ( v * sqrt ( T )) d2 = d1 - v * sqrt ( T ) if cp_flag == 'c' : price = S * exp ( - q * T ) * N ( d1 ) - K * exp ( - r * T ) * N ( d2 ) else : price = K * exp ( - r * T ) * N ( - d2 ) - S * exp ( - q * T ) * N ( - d1 ) return price def bs_vega ( cp_flag , S , K , T , r , v , q = 0.0 ): d1 = ( log ( S / K ) + ( r + v * v / 2. ) * T ) / ( v * sqrt ( T )) return S * sqrt ( T ) * n ( d1 ) V_market = option_data [ 'V' ] . values [ 0 ] K = option_data [ 'K' ] . values [ 0 ] T = option_data [ 'T' ] . values [ 0 ] S = option_data [ 'S' ] . values [ 0 ] r = option_data [ 'r' ] . values [ 0 ] cp = 'c' # call option implied_vol = find_vol ( V_market , cp , S , K , T , r ) print 'Implied vol: %.2f%% ' % ( implied_vol * 100 ) 0 0.5 -4.25568928009 1 0.217935595223 -0.0342444025965 2 0.215598376654 -1.45533695619e-05 3 0.215597382521 -2.69473332537e-12 Implied vol: 21.56% Finally, to close the topic on the implied volatility, we can compare obtained our last result with one obtained from Yahoo Finance!: In [28]: until_days = 30. expiry = datetime . datetime . now () + datetime . timedelta ( days = until_days ) T = until_days / 252. K = 110. ticker = 'aapl' r = 0.0241 option_type = \"call\" options_data = get_option_data ( ticker , expiry , K , option_type ) print \"Calculated implied volatility %.2f and Implied Volatility %.6f from Yahoo Finance!\" % ( impliedSigma , float ( options_data [ \"IV\" ] . values [ 0 ] . replace ( \"%\" , \"\" )) * 1e-2 ) Calculated implied volatility 0.22 and Implied Volatility 0.265400 from Yahoo Finance! Unfortunately, I can't explain the difference in results, I address you to this discussion where this question was arisen http://quant.stackexchange.com/questions/21744/formula-behind-pandas-options-implied-volatility . $$\\\\[5pt]$$ 1.5.2 Monte Carlo calculation of the Option's Payoff $$\\\\[5pt]$$ We have reached the point when we would like to estimate the profit from buying the call options. This option's profit is called the option's payoff. The formula of the payoff is quite simple \\begin{eqnarray}\\label{def:payoff} P(S(t)) = Max (S(t)-K,0.), \\end{eqnarray}$$\\\\[1pt]$$ where $P(t)$, $S(t)$ and $K$ are the payoff, stock and strike prices of the option by the time $t$. To estimate the $P(t)$ \\ref{def:payoff}, we assume that the stock prices follows the Ito drift-diffusion process, namely the Geometric Brownian Motion, which is defined as \\begin{eqnarray}\\label{def:GBM} \\mathrm{d}S(t)=rS(t)\\mathrm{d}t+\\sigma S(t)\\mathrm{d}B(t). \\end{eqnarray}$$\\\\[1pt]$$ Recalling the Ito lemma \\ref{def:ito}, we can rewrite the equation \\ref{def:GBM} in the form \\begin{eqnarray}\\label{def:GBM2} \\mathrm{d}log(S(t)) = (r−\\frac{1}{2}\\sigma&#94;2)\\mathrm{d}t+\\sigma \\mathrm{d}B(t). \\end{eqnarray}$$\\\\[1pt]$$ The SDE \\ref{def:GMB2} has an analytical solution, \\begin{eqnarray}\\label{def:GBM3} S(t)=S(0)e&#94;{(r−\\frac{1}{2}\\sigma&#94;2)T+\\sigma\\sqrt{T}N(0,1)}, \\end{eqnarray}$$\\\\[1pt]$$ where we've used the fact that since $B(t)$ is a Brownian motion, it has the distribution as a normal distribution with variance $T$. Our aim is the developing the Monte Carlo estimator of the $P(S(t))$, i.e. $\\hat{E}(P(S(0)e&#94;{(r−\\frac{1}{2}\\sigma&#94;2)T+\\sigma\\sqrt{T}N(0,1)}))$ \\begin{eqnarray} \\hat{E}(P(S(t)) = \\frac{1}{I}\\sum_i&#94;I P(S_i(t)). \\end{eqnarray}$$\\\\[1pt]$$ Usually, the risk-neutral pay-off for a call option, $e&#94;{-rT}\\cdot E(P(S(t)$, is considered. In [29]: ''' Igor Marfin <Unister Gmb, 2014> igor.marfin@unister.de The Option Payoff Estimator ''' import math def StockPriceGBM ( r , sigma , dt , z ): ''' return the exp part the GBM analytical solution ''' #print \"1)\", sigma * math.sqrt(dt)*z #print \"2)\",(r-0.5*sigma**2)*dt #print \"3)\",np.exp((r-0.5*sigma**2)*dt + sigma * math.sqrt(dt)*z) return np . exp (( r - 0.5 * sigma ** 2 ) * dt + sigma * math . sqrt ( dt ) * z ) def MonteCarloPriceGenerator ( func , S_0 = 100. , T = 100 , dt = 1e0 , N_Sim = 300 ): ''' generates the MC paths of the func (either S(t) or P(t)) N_sim times ''' Steps = round ( T / dt ); #Steps in one MC chain (path) S = zeros ([ Steps + 1 , N_Sim ], dtype = float ) S [ 0 ] = S_0 # first element is S(0) for t in range ( 1 , int ( Steps ) + 1 ): z = standard_normal ( N_Sim ) S [ t ] = S [ t - 1 ] * func ( dt , z ) return S # parameters of the MC simulations until_days = 90. # 3 month forecast expiry = datetime . datetime . now () + datetime . timedelta ( days = until_days ) T = until_days / 252. dt = 1. / 252. K = 110. ticker = 'aapl' r = 0.0241 # a free-risk rate #r=0.0541 # a free-risk rate S_0 = getStockPrices ( datetime . datetime . now () - datetime . timedelta ( days = 1 ), enddate = datetime . datetime . now (), stocks = [ ticker ])[ 0 ][ 'Close' ] N_Sim = 10000 option_type = \"call\" options_data = get_option_data ( ticker , expiry , K , option_type ) V = options_data [ 'Last' ] . values [ 0 ] option_data = pd . DataFrame ({ 'T' :[ T ], 'expiry' :[ expiry ], 'K' :[ K ], 'ticker' :[ ticker ], 'V' :[ V ], 'r' :[ r ], 'S' : S }) # a volatility for dt period (daily) args = [ 'call' , option_data [ 'S' ] . values [ 0 ], option_data [ 'K' ] . values [ 0 ], option_data [ 'T' ] . values [ 0 ], option_data [ 'r' ] . values [ 0 ], ] #It can be either implied volatility or historical volatility #sigma = implied_volatility(args, option_data['V'].values[0]) sigma = historical_volatility ( dt )[ 0 ] print \"Sigma is \" , sigma stockprice_func = lambda dt , z : StockPriceGBM ( r , sigma , dt , z ) paths = MonteCarloPriceGenerator ( stockprice_func , S_0 = S_0 , T = T , dt = dt , N_Sim = N_Sim ) Sigma is 0.0108898366013 /usr/local/lib/python2.7/dist-packages/IPython/kernel/__main__.py:20: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future In [30]: # Our MC estimator def PayOffEstimator ( paths , r , T , K ): ''' returns Payoff estimation''' #N_Sim = paths.shape[1] #profit_trials=np.where(paths[-1,:]>=K) #zero_trials=np.where(paths[-1,:]<K) payoffs = np . maximum ( paths [ - 1 ,:] - K , 0. ) return payoffs * np . exp ( - r * T ), np . average ( payoffs ) * np . exp ( - r * T ) payoffs , avg_payoff = PayOffEstimator ( paths , r , T , K ) print \"On average, we earn about %.4f $ if we are buying %d -day-option (100 shares) of the ' %s ' now\" % ( avg_payoff * 100 , until_days , ticker ) zero_trials = np . where ( payoffs == 0 )[ 0 ] zero_trials_pct = float ( len ( zero_trials )) / len ( payoffs ) * 100. print \"Percentage of the zero-trials: %.2f%% \" % zero_trials_pct import seaborn as sns #hist(payoffs) # get a r3 distributions (3rd step of the monte carlo simulation) selection = np . where ( payoffs < 1.0 )[ 0 ] sns . distplot ( payoffs [ selection ]) #sns.distplot(payoffs) title ( \"A distribution of the payoff of %d -day-option of the ' %s '\" % ( until_days , ticker )) #pd.Series(payoffs).plot() On average, we earn about 298.0684$ if we are buying 90-day-option (100 shares) of the 'aapl' now Percentage of the zero-trials: 0.02% /usr/local/lib/python2.7/dist-packages/numpy/lib/function_base.py:564: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future n = np.zeros(bins, ntype) /usr/local/lib/python2.7/dist-packages/numpy/lib/function_base.py:600: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future n += np.bincount(indices, weights=tmp_w, minlength=bins).astype(ntype) Out[30]: <matplotlib.text.Text at 0xca4670c> It is important that your GBM model of the $S(t)$ has a right estimation of the daily drift term $\\mu$, which is determined by $r$ and $\\sigma$, \\begin{eqnarray}\\label{def:drift_term} \\mu =r−\\frac{1}{2}\\sigma&#94;2. \\end{eqnarray}$$\\\\[1pt]$$ What happens when we increase the free-risk rate $r$ in two times, and hence the daily trend term $\\mu$ takes a larger value? The answer is given below. In [31]: #we have artificially increased the drift term r = 0.0241 # a free-risk rate r = 2.0 * r # let's double itb stockprice_func = lambda dt , z : StockPriceGBM ( r , sigma , dt , z ) paths = MonteCarloPriceGenerator ( stockprice_func , S_0 = S_0 , T = T , dt = dt , N_Sim = N_Sim ) payoffs , avg_payoff = PayOffEstimator ( paths , r , T , K ) print \"On average, we earn about %.4f $ if we are buying %d -day-option (100 shares) of the ' %s ' now\" % ( avg_payoff * 100 , until_days , ticker ) zero_trials = np . where ( payoffs == 0 )[ 0 ] zero_trials_pct = float ( len ( zero_trials )) / len ( payoffs ) * 100. print \"Percentage of the zero-trials: %.2f%% \" % zero_trials_pct import seaborn as sns #hist(payoffs) # get a r3 distributions (3rd step of the monte carlo simulation) selection = np . where ( payoffs < 4.0 )[ 0 ] sns . distplot ( payoffs [ selection ]) #sns.distplot(payoffs) title ( \"A distribution of the payoff of %d -day-option of the ' %s when the r is doubled'\" % ( until_days , ticker )) On average, we earn about 391.4910$ if we are buying 90-day-option (100 shares) of the 'aapl' now Percentage of the zero-trials: 0.00% /usr/local/lib/python2.7/dist-packages/IPython/kernel/__main__.py:20: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future Out[31]: <matplotlib.text.Text at 0xca3724c> $$\\\\[5pt]$$ 1.6 Monte-Carlo Estimation of integrals $$\\\\[5pt]$$ In the end of my tutorial, I want to solve numerically a few integrals presented in the assignment (CamDavidson-Pilon, 2012) . Let's consider the exercise Nr. 2 The next four questions deal with the problem of finding the integral $$I=\\int_0&#94;2e&#94;{-x&#94;2}dx$$ Use both crude and antithetic random numbers to estimate the value of the integral. How large a sample size should be, for antithetic and random numbers, in order to estimate correctly I up to three decimal places with probability at least 95%? $$\\\\[5pt]$$ 1.6.1 Monte-Carlo Importance Sampling $$\\\\[5pt]$$ I will use a MC integrator proposed by Cam Davidson-Pilon, and slightly modified by me. The technology of the importance sampling is very important in the world of the Monte Carlo integration. I'd like to refer a nice lecture (JessiCisewski, 2014) from Jessie Cisewski where you can find simple explanation of the technology. In [32]: # %load MCIntegrator.py ''' Taken from https://github.com/CamDavidsonPilon/Python-Numerics/blob/master/MonteCarlo/Integration/MonteCarloIntegrator.py Author CamDavidsonPilon Modified by Igor Marfin ''' import numpy as np import scipy.stats as stats import time class MCIntegrator ( object ): \"\"\" target_function: a function that accepts a n-D array, and returns an n-D array. interval: the interval of the integration b_antithetic: whether to use antithesis variables. Much quicker, but only useful on monotonic target_functions sampling_dist: a scipy frozen distribution with support equal to the interval N: number of variables to use in the initial estimate. control_variates = a list of function that accepts a nD array, and return an nD array \"\"\" def __init__ ( self , target_function , interval = ( 0 , 1 ), N = 10000 , b_antithetic = False , sampling_dist = stats . uniform (), verbose = False , control_variates = []): self . target_function = target_function self . min_interval , self . max_interval = interval self . N_ = N self . N = 0 self . sampling_dist = sampling_dist self . value = 0 self . var = np . nan self . b_antithetic = b_antithetic self . verbose = verbose self . control_variates = control_variates self . sample = np . array ([]) def estimate_N ( self , N ): self . N += N return self . _estimate ( N ) def _estimate ( self , N ): #generate N values from sampling_dist if not self . b_antithetic : U = self . sampling_dist . rvs ( size = N ) Y = self . target_function ( U ) for func in self . control_variates : X = func ( U ) Y += X if self . verbose : print Y . var () self . value += Y . sum () else : U_ = self . sampling_dist . rvs ( size = N / 2 ) antiU_ = self . min_interval + ( self . max_interval - U_ ) Y = ( self . target_function ( U_ ) + self . target_function ( antiU_ ) ) if self . verbose : print Y . var () self . value += Y . sum () self . sample = np . concatenate (( self . sample , Y )) self . var = self . sample . var ( ddof = 1 ) return self . value / self . N def estimate ( self ): self . N += self . N_ return self . _estimate ( self . N_ ) def SE ( self ): return np . sqrt ( self . var / self . N ) if self . var is not np . nan else np . nan If we use uniform random numbers, we utilize a basic Monte-Carlo estimator to evaluate the value of the integral. In [33]: ''' Monte Carlo estimation of the integral from the assignment ''' import time import scipy.stats as stats def target ( u ): return np . exp ( - u ** 2 ) * 2 mci = MCIntegrator ( target , interval = ( 0 , 2 ), b_antithetic = False , sampling_dist = stats . uniform ( 0 , 2 ), verbose = True ) N = 1e6 start = time . clock () print \"Using %d samples,\" % N print \"Non-antithetic: %.5f . ( %.5f .)\" % ( mci . estimate_N ( N ), mci . SE ()) print \"Duration: %.3f s.\" % ( time . clock () - start ) print /usr/local/lib/python2.7/dist-packages/scipy/stats/_continuous_distns.py:3909: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future return mtrand.uniform(0.0, 1.0, self._size) /usr/local/lib/python2.7/dist-packages/numpy/core/fromnumeric.py:225: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future return reshape(newshape, order=order) Using 1000000 samples, 0.475396645407 Non-antithetic: 0.88140. (0.00069.) Duration: 0.140 s. If we apply the sampling different from the uniform distribution in MC, i.e. we replace uniform distribution by some distribution close to the target function, then we can expect to increase the performance of the Monte Carlo. In [34]: ''' Importance Sampling Monte Carlo estimation of the integral from the assignment ''' import math def target ( u , interval ): return (( u >= interval [ 0 ]) & ( u <= interval [ 1 ])) * np . sqrt ( math . pi ) targetPrime = lambda u : target ( u ,( 0 , 2 )) mci = MCIntegrator ( targetPrime , interval = ( 0 , 2 ), b_antithetic = False , sampling_dist = stats . norm ( loc = 0. , scale = 1. ), verbose = True ) N = 1e6 start = time . clock () print \"Using %d samples,\" % N print \"Non-antithetic: %.5f . ( %.5f .)\" % ( mci . estimate_N ( N ), mci . SE ()) print \"Duration: %.3f s.\" % ( time . clock () - start ) print Using 1000000 samples, 0.783959574083 Non-antithetic: 0.84830. (0.00089.) Duration: 0.130 s. /usr/local/lib/python2.7/dist-packages/scipy/stats/_continuous_distns.py:127: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future return mtrand.standard_normal(self._size) The importance sampling improves the MC efficiency, i.e. achieving high accuracy of the integration requires less computing time. Let's check another importance sampling functions. In [35]: x = np . linspace ( 0 , 2 , 100 ) def target ( u ): return np . exp ( - u ** 2 ) * 2 def sampler1 ( u ): ''' a linear approximation of the exponent tail ''' return ( -. 5 * u + 1 ) * 2 def sampler2 ( u ): ''' Taylor Series Expansions of Exponential Functions http://www.efunda.com/math/taylor_series/exponential.cfm ''' #return (1-u**2+u**4/2.-u**6/2./3.+u**8/2./3./4.)*2 a = np . array ([ 0. if b < 1. else round ( b ) - 1 for b in u ]) # a regularization term return np . abs ( 1. - ( u - a ) ** 2 ) * 2 plot ( x , target ( x ), label = \"original\" ) plot ( x , sampler1 ( x ), label = \"sampler1\" ) plot ( x , sampler2 ( x ), label = \"sampler2\" ) legend ( loc = \"upper right\" ) Out[35]: <matplotlib.legend.Legend at 0xca5420c> $$\\\\[5pt]$$ 1.6.1.1 Reject/Accept method for the Monte-Carlo Importance Sampling $$\\\\[5pt]$$ The code, shown below, is a simple Reject/Accept method to generate samples according some analytical functions. In [37]: ''' A realization of the importance sampler for any sampling distribution See http://people.duke.edu/~ccc14/sta-663/MonteCarlo.html ''' import scipy.stats as st # Reject/Accept method to generate samples according the target function class Importance ( object ): def __init__ ( self , target , interval ): self . target = target self . interval = interval def rvs ( self , size ): u = np . random . uniform ( * self . interval , size = size ) # accept-reject criterion for each point in sampling distribution # what is the best interval of the randomly generated numbers? # this one ? r = np . random . uniform ( np . min ( self . target ( u )), np . max ( self . target ( u )), size ) # or this one? #r = np.random.uniform(0, np.max(self.target(u)), size) # or this one? #r = np.random.uniform(*self.interval, size=size) # accepted points will come from target distribution v = u [ r <= self . target ( u )] return v # An alternative approach. # This calculator is technically hard to control: rv_continuous can produce # negative values of the pdf. #class Importance(st.rv_continuous): # def _pdf(self,x): # pass #Importance._pdf = lambda self,x: sampler1(x) #mysampler = Importance(name='mysampler') interval = ( 0 , 2 ) x = np . linspace ( * interval , num = 100 ) def target ( u ): ''' a function to be integrated''' return np . exp ( - u ** 2 ) * 2 # An Example of the importance sampling with the Sampler1 mysampler1 = Importance ( sampler1 , interval ) hist ( mysampler1 . rvs ( 10000 ), label = \"sampler1\" ) # to plot distribution # normalization of the sampler normalization1 = MCIntegrator ( sampler1 , interval = interval , b_antithetic = False , sampling_dist = stats . uniform ( * interval )) . estimate_N ( 1e6 ) # target function should be modified accordingly! def targetPrime ( x , target , sampler , interval , normalization ): return target ( x ) / ( sampler ( x ) / ( interval [ 1 ] - interval [ 0 ])) * normalization _targetPrime = lambda x : targetPrime ( x , target , sampler1 , interval , normalization1 ) mci = MCIntegrator ( _targetPrime , interval = interval , b_antithetic = False , sampling_dist = mysampler1 , verbose = True ) N = 1e6 start = time . clock () print \"Sampler1\" print \"Using %d samples,\" % N print \"Non-antithetic: %.5f . ( %.5f .)\" % ( mci . estimate_N ( N ), mci . SE ()) print \"Duration: %.3f s.\" % ( time . clock () - start ) print # An Example of the importance sampling with the Sampler2 mysampler2 = Importance ( sampler2 , interval ) hist ( mysampler2 . rvs ( 10000 ), label = \"sampler2\" ) # to plot distribution # normalization of the sampler normalization2 = MCIntegrator ( sampler2 , interval = interval , b_antithetic = False , sampling_dist = stats . uniform ( * interval )) . estimate_N ( 1e6 ) _targetPrime2 = lambda x : targetPrime ( x , target , sampler2 , interval , normalization2 ) mci = MCIntegrator ( _targetPrime2 , interval = interval , b_antithetic = False , sampling_dist = mysampler2 , verbose = True ) N = 1e6 start = time . clock () print \"Sampler2\" print \"Using %d samples,\" % N print \"Non-antithetic: %.5f . ( %.5f .)\" % ( mci . estimate_N ( N ), mci . SE ()) print \"Duration: %.3f s.\" % ( time . clock () - start ) print legend ( loc = \"upper right\" ) /usr/local/lib/python2.7/dist-packages/IPython/kernel/__main__.py:15: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future /usr/local/lib/python2.7/dist-packages/IPython/kernel/__main__.py:19: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future Sampler1 Using 1000000 samples, 0.21254036049 Non-antithetic: 0.88171. (0.00046.) Duration: 0.170 s. Sampler2 Using 1000000 samples, 5.44010673961 Non-antithetic: 0.82234. (0.00233.) Duration: 2.520 s. Out[37]: <matplotlib.legend.Legend at 0xd1b97ac> In [38]: # An Example of the importance sampling with the original target mysampler3 = Importance ( target , interval ) hist ( mysampler3 . rvs ( 10000 ), label = \"original\" ) # to plot distribution # normalization of the sampler normalization3 = MCIntegrator ( target , interval = interval , b_antithetic = False , sampling_dist = stats . uniform ( * interval )) . estimate_N ( 1e6 ) _targetPrime3 = lambda x : targetPrime ( x , target , target , interval , normalization3 ) print normalization3 mci = MCIntegrator ( _targetPrime3 , interval = interval , b_antithetic = False , sampling_dist = mysampler3 , verbose = True ) N = 1e7 start = time . clock () print \"Original Target\" print \"Using %d samples,\" % N print \"Non-antithetic: %.5f . ( %.5f .)\" % ( mci . estimate_N ( N ), mci . SE ()) print \"Duration: %.3f s.\" % ( time . clock () - start ) print legend ( loc = \"upper right\" ) /usr/local/lib/python2.7/dist-packages/IPython/kernel/__main__.py:15: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future /usr/local/lib/python2.7/dist-packages/IPython/kernel/__main__.py:19: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future 0.881972798098 Original Target Using 10000000 samples, 3.86541843558e-29 Non-antithetic: 0.75968. (0.00000.) Duration: 3.480 s. Out[38]: <matplotlib.legend.Legend at 0xd1b9b4c> As it was expected, different samplers give different results. The obtained results show that the simpler sampler is, the better performance can be achieved. $$\\\\[5pt]$$ 1.6.2 Monte-Carlo with Stratified Sampling $$\\\\[5pt]$$ Next our task is to implement the stratified sampling (WikiStratifiedSampling, 2016) in the Monte Carlo estimator. Why do we need to stratify sample at all? This can improve the accuracy of the Monte Carlo estimation. Let's integrate our target function in the 5 regions as $0\\leq x\\leq 0.5$, $0.5\\leq x\\leq 1.5$,$1.5\\leq x\\leq 2$, $0\\leq x\\leq 1$ and $1\\leq x\\leq 2$ using a basic MC. In [39]: from operator import isub import scipy.integrate as integrate def target ( u ): return np . exp ( - u ** 2 ) * 2 def validation_of_integration ( target , interval , val ): ''' compare results obtained by MC and using a technique from the Fortran library QUADPACK. ''' result_quad = integrate . quad ( target , * interval )[ 0 ] print \"QUADPACK: %.5f and MC: %r \" % ( result_quad , val [ 0 ]) print \"MC Error: %r \" % ( val [ 1 ]) return val [ 0 ], result_quad def MCEstimation ( target , interval , N = 1e6 ): mci = MCIntegrator ( target , interval = interval , b_antithetic = False , sampling_dist = stats . uniform ( * interval ), verbose = True ) return - mci . estimate_N ( N ) * isub ( * interval ), mci . SE () # Integrate the target function in the 3 sub-regions print \"3 sub-regions\" intervals = [( 0 , 0.5 ),( 0.5 , 1.5 ),( 1.5 , 2 )] integralMC = 0 integralQUAD = 0 for interval in intervals : print _tmp1 , _tmp2 = validation_of_integration ( target , interval , MCEstimation ( target , interval )) integralMC += _tmp1 integralQUAD += _tmp2 print print \"Result MC is \" , integralMC print \"Result QUAD is \" , integralQUAD print print \"2 sub-regions\" intervals = [( 0 , 1 ),( 1 , 2 )] integralMC = 0 integralQUAD = 0 for interval in intervals : print _tmp1 , _tmp2 = validation_of_integration ( target , interval , MCEstimation ( target , interval )) integralMC += _tmp1 integralQUAD += _tmp2 print print \"Result MC is \" , integralMC print \"Result QUAD is \" , integralQUAD print print \"1 region\" intervals = [( 0 , 2 )] integralMC = 0 integralQUAD = 0 for interval in intervals : print _tmp1 , _tmp2 = validation_of_integration ( target , interval , MCEstimation ( target , interval )) integralMC += _tmp1 integralQUAD += _tmp2 print print \"Result MC is \" , integralMC print \"Result QUAD is \" , integralQUAD 3 sub-regions 0.018022824605 QUADPACK: 0.92256 and MC: 0.92266051398506843 MC Error: 0.00013424918110660584 0.215369406245 QUADPACK: 0.78981 and MC: 0.56114633640158273 MC Error: 0.00046407932685548262 0.00248147688842 QUADPACK: 0.05179 and MC: 0.015017947367964572 MC Error: 4.9814449408738103e-05 Result MC is 1.49882479775 Result QUAD is 1.76416278152 2 sub-regions 0.161585774809 QUADPACK: 1.49365 and MC: 1.4939922544849218 MC Error: 0.00040197753220198699 0.0377347962115 QUADPACK: 0.27051 and MC: 0.1397071083441512 MC Error: 0.00019425455965400439 Result MC is 1.63369936283 Result QUAD is 1.76416278152 1 region 0.474715539751 QUADPACK: 1.76416 and MC: 1.7633504652270953 MC Error: 0.00068899638204205304 Result MC is 1.76335046523 Result QUAD is 1.76416278152 It is clearly visible that Monte Carlo is inaccurate in the region of $0.5\\leq x\\leq 1.5$. We could perhaps increase the MC accuracy if more points were generated in this region. Let's try to stratify the sample. I have found a simple code, stratified.py , for this purpose. In [169]: ! wget https://gist.githubusercontent.com/spacelis/6088623/raw/1c71eeb3ab0548e33bae99afacaacd9963438664/stratified.py --2016-04-13 16:54:41-- https://gist.githubusercontent.com/spacelis/6088623/raw/1c71eeb3ab0548e33bae99afacaacd9963438664/stratified.py Resolving gist.githubusercontent.com (gist.githubusercontent.com)... 23.235.43.133 Connecting to gist.githubusercontent.com (gist.githubusercontent.com)|23.235.43.133|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 2186 (2.1K) [text/plain] Saving to: `stratified.py' 100%[======================================>] 2,186 --.-K/s in 0s 2016-04-13 16:54:43 (31.3 MB/s) - `stratified.py' saved [2186/2186] In [51]: # %load stratified.py #!/usr/bin/env python \"\"\" File: stratified.py Author: SpaceLis Email: Wen.Li@tudelft.nl Github: http://github.com/spacelis Description: Sampling in a stratified way. That is sampling from each subpopulation to make the sample set more representative than simple random sampling. For example, a population of places from each category is not uniform, it is needed to insure each category has a place sampled and the number of the samples from each category should be propotional to its size. \"\"\" import numpy as np from itertools import tee , izip , chain from collections import Counter def pairwise ( iterable ): \"s -> (s0,s1), (s1,s2), (s2, s3), ...\" a , b = tee ( iterable ) next ( b , None ) return izip ( a , b ) def int_partition ( K , percentages , minimum = 1 ): \"\"\" Scale the percentages up K times \"\"\" assert K > minimum * len ( percentages ), 'K is too small for partitioning' p = np . array ( percentages , dtype = np . float ) dist = np . ones ( len ( p ), dtype = np . float ) * minimum dist += ( K - len ( p )) * p left = K - dist . sum () if left > 0 : while left > 0 : diff = p * K - dist dist [ np . argmax ( diff )] += 1 left -= 1 elif left < 0 : # FIXME seems never having chance to run actually while left < 0 : diff = p * K - dist dist [ np . argmin ( diff )] -= 1 left += 1 return dist def stratified_samples ( iterable , percentages , size , prop_sizes = True , replace = False ): \"\"\" Sampling the data with stratified sampling \"\"\" partitions = len ( percentages ) dist = sorted ( Counter ( iterable ) . iteritems (), key = lambda x : x [ 1 ], reverse = True ) if prop_sizes : samplesize = int_partition ( float ( size ), percentages ) else : samplesize = int_partition ( size , [ 1. / partitions ] * partitions ) pivots = list ( np . cumsum ( int_partition ( len ( dist ), percentages ))) chozen_idx = [ np . random . choice ( range ( int ( s ), int ( t )), n , replace = replace ) for n , ( s , t ) in zip ( samplesize , pairwise ([ 0 ] + pivots ))] chozen = [[ dist [ i ][ 0 ] for i in ig ] for ig in chozen_idx ] return chozen In [54]: ''' Stratified Sampling Igor Marfin @2014 <Unister Gmbh> ''' interval = ( 0 , 2 ) intervals = [( 0 , 0.5 ),( 0.5 , 1.5 ),( 1.5 , 2 )] x = stats . uniform ( * interval ) . rvs ( 10000 ) subsamples = [] func_vals = [] for interval in intervals : condition = ( x >= interval [ 0 ]) & ( x <= interval [ 1 ]) subsamples += [ x [ condition ] . tolist ()] func_vals += [ target ( x [ condition ]) . tolist ()] #func_vals+=target(x[condition]).tolist() import itertools import operator subsamplesmerged = list ( itertools . chain . from_iterable ([ zip ( b , a ) for a , b in zip ( subsamples , func_vals )])) a = stratified_samples ( subsamplesmerged ,[ 0.8 , 0.1 , 0.1 ], 1000 ) for i , item in enumerate ( a ): print \"Average in %d region is %f \" % ( i , np . average ( np . array ( map ( operator . itemgetter ( 0 ), item )))) Average in 0 region is 0.863028 Average in 1 region is 0.874898 Average in 2 region is 0.813675 /usr/local/lib/python2.7/dist-packages/IPython/kernel/__main__.py:64: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future and it is obvious that it only works for categorical values and not numbers. $$\\\\[5pt]$$ 1.6.2.1 The adaptive Monte Carlo VEGAS algorithm $$\\\\[5pt]$$ There is a method implementing the adaptive stratified sampling for Monte Carlo simulations. This method is called VEGAS algorithm. You can install python version of the VEGAS algorithm (G.P.Lepage, 2015) . In [221]: ! sudo pip install vegas [sudo] password for debian: If we repeat the MC integration of the target function with VEGAS, we can build the adaptive grid of the stratified sampling used in this MC. The example below shows that adaptive stratified sampling improves Monte-Carlo estimation dramatically. In [62]: ''' An example of the application of the VEGAS algorithm ''' import vegas # a problematic interval interval = ( 0.5 , 1.5 ) def target ( u ): return np . exp ( - u ** 2 ) * 2 integ = vegas . Integrator ([ interval ]) result = integ ( target , nitn = 5 , neval = 1000 ) print ( result . summary ()) #print('result = %s Q = %.2f' % (result, result.Q)) validation_of_integration ( target , interval ,( result , result . Q )) # show the adaptive grid integ . map . show_grid ( 40 ) itn integral wgt average chi2/dof Q ------------------------------------------------------- 1 0.789791(70) 0.789791(70) 0.00 1.00 2 0.789910(65) 0.789855(48) 1.55 0.21 3 0.789973(66) 0.789895(39) 1.82 0.16 4 0.789858(68) 0.789886(34) 1.29 0.28 5 0.789829(72) 0.789876(30) 1.10 0.35 QUADPACK: 0.78981 and MC: RAvgArray([0.789876(30)], dtype=object) MC Error: 0.35462157595551524 That's it for this lecture. $$\\\\[5pt]$$ 2. Plans for next tutorials $$\\\\[5pt]$$ A short `to-do list for my future lectures is the following: Introduction to Strategies of the option trading like Covered Call Protective Put Option Strangle Butterfly spread etc Developing the self-learned machinery to make decisions following some option strategy Miscellaneous In [198]: ### Do not delete the following Markdown section! ### This is the BibTeX references! References &#94; &#94; &#94; Cam Davidson-Pilon,. 2012. STAT906 - Assignemtn Nr.3 . URL &#94; Wiki:SDE,. 2016. Stochastic differential equation . URL &#94; Wiki:Ito Lemmma,. 2016. _Itô's lemma . URL &#94; &#94; Wiki:option,. 2016. Call Option . URL &#94; Jan Palcyewski,. 2016. Lecture5: Numerical schemes for SDEs . URL &#94; Investopedia,. 2016. The Basics Of The Bid-Ask Spread . URL &#94; WikiVolatility,. 2016. Volatility in finance . URL &#94; Wiki Black–Scholes formula,. 2016. Black–Scholes formula . URL &#94; Jessi Cisewski,. 2014. Importance Sampling . URL &#94; Wiki Stratified Sampling,. 2016. Stratified Sampling . URL &#94; G.P.Lepage,. 2015. Adaptive Monte Carlo vegas algorithm . URL if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML'; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: 'center',\" + \" displayIndent: '0em',\" + \" showMathMenu: true,\" + \" tex2jax: { \" + \" inlineMath: [ ['$','$'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }\" + \" } \" + \"}); \"; (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Statistics","title":"Application of the Monte Carlo integration in Options trading (part I)"},{"url":"http://igormarfin.github.io/blog/2016/02/08/brownian-motion-and-the-wiener-process/","text":"/*! * * IPython notebook * */.ansibold{font-weight:bold}.ansiblack{color:black}.ansired{color:darkred}.ansigreen{color:darkgreen}.ansiyellow{color:#c4a000}.ansiblue{color:darkblue}.ansipurple{color:darkviolet}.ansicyan{color:steelblue}.ansigray{color:gray}.ansibgblack{background-color:black}.ansibgred{background-color:red}.ansibggreen{background-color:green}.ansibgyellow{background-color:yellow}.ansibgblue{background-color:blue}.ansibgpurple{background-color:magenta}.ansibgcyan{background-color:cyan}.ansibggray{background-color:gray}div.cell{border:1px solid transparent;display:-webkit-box;-webkit-box-orient:vertical;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:vertical;-moz-box-align:stretch;display:box;box-orient:vertical;box-align:stretch;display:flex;flex-direction:column;align-items:stretch;border-radius:2px;box-sizing:border-box;-moz-box-sizing:border-box;-webkit-box-sizing:border-box;border-width:thin;border-style:solid;width:100%;padding:5px;margin:0;outline:none}div.cell.selected{border-color:#ababab}@media print{div.cell.selected{border-color:transparent}}.edit_mode div.cell.selected{border-color:green}@media print{.edit_mode div.cell.selected{border-color:transparent}}.prompt{min-width:14ex;padding:.4em;margin:0;font-family:monospace;text-align:right;line-height:1.21429em}@media (max-width:540px){.prompt{text-align:left}}div.inner_cell{display:-webkit-box;-webkit-box-orient:vertical;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:vertical;-moz-box-align:stretch;display:box;box-orient:vertical;box-align:stretch;display:flex;flex-direction:column;align-items:stretch;-webkit-box-flex:1;-moz-box-flex:1;box-flex:1;flex:1}@-moz-document url-prefix(){div.inner_cell{overflow-x:hidden}}div.input_area{border:1px solid #cfcfcf;border-radius:2px;background:#f7f7f7;line-height:1.21429em}div.prompt:empty{padding-top:0;padding-bottom:0}div.unrecognized_cell{padding:5px 5px 5px 0;display:-webkit-box;-webkit-box-orient:horizontal;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:horizontal;-moz-box-align:stretch;display:box;box-orient:horizontal;box-align:stretch;display:flex;flex-direction:row;align-items:stretch}div.unrecognized_cell .inner_cell{border-radius:2px;padding:5px;font-weight:bold;color:red;border:1px solid #cfcfcf;background:#eaeaea}div.unrecognized_cell .inner_cell a{color:inherit;text-decoration:none}div.unrecognized_cell .inner_cell a:hover{color:inherit;text-decoration:none}@media (max-width:540px){div.unrecognized_cell>div.prompt{display:none}}@media print{div.code_cell{page-break-inside:avoid}}div.input{page-break-inside:avoid;display:-webkit-box;-webkit-box-orient:horizontal;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:horizontal;-moz-box-align:stretch;display:box;box-orient:horizontal;box-align:stretch;display:flex;flex-direction:row;align-items:stretch}@media (max-width:540px){div.input{display:-webkit-box;-webkit-box-orient:vertical;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:vertical;-moz-box-align:stretch;display:box;box-orient:vertical;box-align:stretch;display:flex;flex-direction:column;align-items:stretch}}div.input_prompt{color:navy;border-top:1px solid transparent}div.input_area>div.highlight{margin:.4em;border:none;padding:0;background-color:transparent}div.input_area>div.highlight>pre{margin:0;border:none;padding:0;background-color:transparent}.CodeMirror{line-height:1.21429em;font-size:14px;height:auto;background:none}.CodeMirror-scroll{overflow-y:hidden;overflow-x:auto}.CodeMirror-lines{padding:.4em}.CodeMirror-linenumber{padding:0 8px 0 4px}.CodeMirror-gutters{border-bottom-left-radius:2px;border-top-left-radius:2px}.CodeMirror pre{padding:0;border:0;border-radius:0}.highlight-base{}.highlight-variable{}.highlight-variable-2{color:#1a1a1a}.highlight-variable-3{color:#333}.highlight-string{color:#ba2121}.highlight-comment{color:#408080;font-style:italic}.highlight-number{80}.highlight-atom{color:#88f}.highlight-keyword{8000;font-weight:bold}.highlight-builtin{8000}.highlight-error{color:#f00}.highlight-operator{color:#a2f;font-weight:bold}.highlight-meta{color:#a2f}.highlight-def{f}.highlight-string-2{color:#f50}.highlight-qualifier{color:#555}.highlight-bracket{color:#997}.highlight-tag{color:#170}.highlight-attribute{c}.highlight-header{color:blue}.highlight-quote{90}.highlight-link{c}.cm-s-ipython span.cm-keyword{8000;font-weight:bold}.cm-s-ipython span.cm-atom{color:#88f}.cm-s-ipython span.cm-number{80}.cm-s-ipython span.cm-def{f}.cm-s-ipython span.cm-variable{}.cm-s-ipython span.cm-operator{color:#a2f;font-weight:bold}.cm-s-ipython span.cm-variable-2{color:#1a1a1a}.cm-s-ipython span.cm-variable-3{color:#333}.cm-s-ipython span.cm-comment{color:#408080;font-style:italic}.cm-s-ipython span.cm-string{color:#ba2121}.cm-s-ipython span.cm-string-2{color:#f50}.cm-s-ipython span.cm-meta{color:#a2f}.cm-s-ipython span.cm-qualifier{color:#555}.cm-s-ipython span.cm-builtin{8000}.cm-s-ipython span.cm-bracket{color:#997}.cm-s-ipython span.cm-tag{color:#170}.cm-s-ipython span.cm-attribute{c}.cm-s-ipython span.cm-header{color:blue}.cm-s-ipython span.cm-quote{90}.cm-s-ipython span.cm-link{c}.cm-s-ipython span.cm-error{color:#f00}.cm-s-ipython span.cm-tab{background:url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADAAAAAMCAYAAAAkuj5RAAAAAXNSR0IArs4c6QAAAGFJREFUSMft1LsRQFAQheHPowAKoACx3IgEKtaEHujDjORSgWTH/ZOdnZOcM/sgk/kFFWY0qV8foQwS4MKBCS3qR6ixBJvElOobYAtivseIE120FaowJPN75GMu8j/LfMwNjh4HUpwg4LUAAAAASUVORK5CYII=);background-position:right;background-repeat:no-repeat}div.output_wrapper{position:relative;display:-webkit-box;-webkit-box-orient:vertical;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:vertical;-moz-box-align:stretch;display:box;box-orient:vertical;box-align:stretch;display:flex;flex-direction:column;align-items:stretch}div.output_scroll{height:24em;width:100%;overflow:auto;border-radius:2px;-webkit-box-shadow:inset 0 2px 8px rgba(0,0,0,0.8);box-shadow:inset 0 2px 8px rgba(0,0,0,0.8);display:block}div.output_collapsed{margin:0;padding:0;display:-webkit-box;-webkit-box-orient:vertical;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:vertical;-moz-box-align:stretch;display:box;box-orient:vertical;box-align:stretch;display:flex;flex-direction:column;align-items:stretch}div.out_prompt_overlay{height:100%;padding:0 .4em;position:absolute;border-radius:2px}div.out_prompt_overlay:hover{-webkit-box-shadow:inset 0 0 1px #000;box-shadow:inset 0 0 1px #000;background:rgba(240,240,240,0.5)}div.output_prompt{color:darkred}div.output_area{padding:0;page-break-inside:avoid;display:-webkit-box;-webkit-box-orient:horizontal;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:horizontal;-moz-box-align:stretch;display:box;box-orient:horizontal;box-align:stretch;display:flex;flex-direction:row;align-items:stretch}div.output_area .MathJax_Display{text-align:left !important}div.output_area div.output_area .output{display:-webkit-box;-webkit-box-orient:vertical;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:vertical;-moz-box-align:stretch;display:box;box-orient:vertical;box-align:stretch;display:flex;flex-direction:column;align-items:stretch}@media (max-width:540px){div.output_area{display:-webkit-box;-webkit-box-orient:vertical;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:vertical;-moz-box-align:stretch;display:box;box-orient:vertical;box-align:stretch;display:flex;flex-direction:column;align-items:stretch}}div.output_area pre{margin:0;padding:0;border:0;vertical-align:baseline;color:black;background-color:transparent;border-radius:0}div.output_subarea{padding:.4em;-webkit-box-flex:1;-moz-box-flex:1;box-flex:1;flex:1}div.output_text{text-align:left;line-height:1.21429em}div.output_stderr{background:#fdd}div.output_latex{text-align:left}div.output_javascript:empty{padding:0}.js-error{color:darkred}div.raw_input_container{font-family:monospace;padding-top:5px}input.raw_input{font-family:inherit;font-size:inherit;color:inherit;width:auto;vertical-align:baseline;padding:0 .25em;margin:0 .25em}input.raw_input:focus{box-shadow:none}p.p-space{margin-bottom:10px}div.output_unrecognized{padding:5px;font-weight:bold;color:red}div.output_unrecognized a{color:inherit;text-decoration:none}div.output_unrecognized a:hover{color:inherit;text-decoration:none}.rendered_html{}.rendered_html :link{text-decoration:underline}.rendered_html :visited{text-decoration:underline}.rendered_html h1:first-child{margin-top:.538em}.rendered_html h2:first-child{margin-top:.636em}.rendered_html h3:first-child{margin-top:.777em}.rendered_html h4:first-child{margin-top:1em}.rendered_html h5:first-child{margin-top:1em}.rendered_html h6:first-child{margin-top:1em}.rendered_html *+ul{margin-top:1em}.rendered_html *+ol{margin-top:1em}.rendered_html *+table{margin-top:1em}.rendered_html *+p{margin-top:1em}.rendered_html *+img{margin-top:1em}div.text_cell{display:-webkit-box;-webkit-box-orient:horizontal;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:horizontal;-moz-box-align:stretch;display:box;box-orient:horizontal;box-align:stretch;display:flex;flex-direction:row;align-items:stretch}@media (max-width:540px){div.text_cell>div.prompt{display:none}}div.text_cell_render{outline:none;resize:none;width:inherit;border-style:none;padding:.5em .5em .5em .4em;box-sizing:border-box;-moz-box-sizing:border-box;-webkit-box-sizing:border-box}a.anchor-link:link{text-decoration:none;padding:0 20px;visibility:hidden}h1:hover .anchor-link,h2:hover .anchor-link,h3:hover .anchor-link,h4:hover .anchor-link,h5:hover .anchor-link,h6:hover .anchor-link{visibility:visible}.text_cell.rendered .input_area{display:none}.text_cell.unrendered .text_cell_render{display:none}.cm-header-1,.cm-header-2,.cm-header-3,.cm-header-4,.cm-header-5,.cm-header-6{font-weight:bold;font-family:\"Helvetica Neue\",Helvetica,Arial,sans-serif}.cm-header-1{font-size:185.7%}.cm-header-2{font-size:157.1%}.cm-header-3{font-size:128.6%}.cm-header-4{font-size:110%}.cm-header-5{font-size:100%;font-style:italic}.cm-header-6{font-size:100%;font-style:italic}.widget-interact>div,.widget-interact>input{padding:2.5px}.widget-area{page-break-inside:avoid;display:-webkit-box;-webkit-box-orient:horizontal;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:horizontal;-moz-box-align:stretch;display:box;box-orient:horizontal;box-align:stretch;display:flex;flex-direction:row;align-items:stretch}.widget-area .widget-subarea{padding:.44em .4em .4em 1px;margin-left:6px;box-sizing:border-box;-moz-box-sizing:border-box;-webkit-box-sizing:border-box;display:-webkit-box;-webkit-box-orient:vertical;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:vertical;-moz-box-align:stretch;display:box;box-orient:vertical;box-align:stretch;display:flex;flex-direction:column;align-items:stretch;-webkit-box-flex:2;-moz-box-flex:2;box-flex:2;flex:2;-webkit-box-align:start;-moz-box-align:start;box-align:start;align-items:flex-start}.widget-area.connection-problems .prompt:after{content:\"\\f127\";font-family:'FontAwesome';color:#d9534f;font-size:14px;top:3px;padding:3px}.slide-track{border:1px solid #ccc;background:#fff;border-radius:2px}.widget-hslider{padding-left:8px;padding-right:2px;overflow:visible;width:350px;height:5px;max-height:5px;margin-top:13px;margin-bottom:10px;border:1px solid #ccc;background:#fff;border-radius:2px;display:-webkit-box;-webkit-box-orient:horizontal;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:horizontal;-moz-box-align:stretch;display:box;box-orient:horizontal;box-align:stretch;display:flex;flex-direction:row;align-items:stretch}.widget-hslider .ui-slider{border:0;background:none;display:-webkit-box;-webkit-box-orient:horizontal;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:horizontal;-moz-box-align:stretch;display:box;box-orient:horizontal;box-align:stretch;display:flex;flex-direction:row;align-items:stretch;-webkit-box-flex:1;-moz-box-flex:1;box-flex:1;flex:1}.widget-hslider .ui-slider .ui-slider-handle{width:12px;height:28px;margin-top:-8px;border-radius:2px}.widget-hslider .ui-slider .ui-slider-range{height:12px;margin-top:-4px;background:#eee}.widget-vslider{padding-bottom:5px;overflow:visible;width:5px;max-width:5px;height:250px;margin-left:12px;border:1px solid #ccc;background:#fff;border-radius:2px;display:-webkit-box;-webkit-box-orient:vertical;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:vertical;-moz-box-align:stretch;display:box;box-orient:vertical;box-align:stretch;display:flex;flex-direction:column;align-items:stretch}.widget-vslider .ui-slider{border:0;background:none;margin-left:-4px;margin-top:5px;display:-webkit-box;-webkit-box-orient:vertical;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:vertical;-moz-box-align:stretch;display:box;box-orient:vertical;box-align:stretch;display:flex;flex-direction:column;align-items:stretch;-webkit-box-flex:1;-moz-box-flex:1;box-flex:1;flex:1}.widget-vslider .ui-slider .ui-slider-handle{width:28px;height:12px;margin-left:-9px;border-radius:2px}.widget-vslider .ui-slider .ui-slider-range{width:12px;margin-left:-1px;background:#eee}.widget-text{width:350px;margin:0}.widget-listbox{width:350px;margin-bottom:0}.widget-numeric-text{width:150px;margin:0}.widget-progress{margin-top:6px;min-width:350px}.widget-progress .progress-bar{-webkit-transition:none;-moz-transition:none;-ms-transition:none;-o-transition:none;transition:none}.widget-combo-btn{min-width:125px}.widget_item .dropdown-menu li a{color:inherit}.widget-hbox{display:-webkit-box;-webkit-box-orient:horizontal;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:horizontal;-moz-box-align:stretch;display:box;box-orient:horizontal;box-align:stretch;display:flex;flex-direction:row;align-items:stretch}.widget-hbox input[type=\"checkbox\"]{margin-top:9px;margin-bottom:10px}.widget-hbox .widget-label{min-width:10ex;padding-right:8px;padding-top:5px;text-align:right;vertical-align:text-top}.widget-hbox .widget-readout{padding-left:8px;padding-top:5px;text-align:left;vertical-align:text-top}.widget-vbox{display:-webkit-box;-webkit-box-orient:vertical;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:vertical;-moz-box-align:stretch;display:box;box-orient:vertical;box-align:stretch;display:flex;flex-direction:column;align-items:stretch}.widget-vbox .widget-label{padding-bottom:5px;text-align:center;vertical-align:text-bottom}.widget-vbox .widget-readout{padding-top:5px;text-align:center;vertical-align:text-top}.widget-box{box-sizing:border-box;-moz-box-sizing:border-box;-webkit-box-sizing:border-box;-webkit-box-align:start;-moz-box-align:start;box-align:start;align-items:flex-start}.widget-radio-box{display:-webkit-box;-webkit-box-orient:vertical;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:vertical;-moz-box-align:stretch;display:box;box-orient:vertical;box-align:stretch;display:flex;flex-direction:column;align-items:stretch;box-sizing:border-box;-moz-box-sizing:border-box;-webkit-box-sizing:border-box;padding-top:4px}.widget-radio-box label{margin-top:0}.widget-radio{margin-left:20px} .highlight .hll { background-color: #ffffcc } .highlight { background: #f8f8f8; } .highlight .c { color: #408080; font-style: italic } /* Comment */ .highlight .err { border: 1px solid #FF0000 } /* Error */ .highlight .k { color: #008000; font-weight: bold } /* Keyword */ .highlight .o { color: #666666 } /* Operator */ .highlight .cm { color: #408080; font-style: italic } /* Comment.Multiline */ .highlight .cp { color: #BC7A00 } /* Comment.Preproc */ .highlight .c1 { color: #408080; font-style: italic } /* Comment.Single */ .highlight .cs { color: #408080; font-style: italic } /* Comment.Special */ .highlight .gd { color: #A00000 } /* Generic.Deleted */ .highlight .ge { font-style: italic } /* Generic.Emph */ .highlight .gr { color: #FF0000 } /* Generic.Error */ .highlight .gh { color: #000080; font-weight: bold } /* Generic.Heading */ .highlight .gi { color: #00A000 } /* Generic.Inserted */ .highlight .go { color: #888888 } /* Generic.Output */ .highlight .gp { color: #000080; font-weight: bold } /* Generic.Prompt */ .highlight .gs { font-weight: bold } /* Generic.Strong */ .highlight .gu { color: #800080; font-weight: bold } /* Generic.Subheading */ .highlight .gt { color: #0044DD } /* Generic.Traceback */ .highlight .kc { color: #008000; font-weight: bold } /* Keyword.Constant */ .highlight .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */ .highlight .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */ .highlight .kp { color: #008000 } /* Keyword.Pseudo */ .highlight .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */ .highlight .kt { color: #B00040 } /* Keyword.Type */ .highlight .m { color: #666666 } /* Literal.Number */ .highlight .s { color: #BA2121 } /* Literal.String */ .highlight .na { color: #7D9029 } /* Name.Attribute */ .highlight .nb { color: #008000 } /* Name.Builtin */ .highlight .nc { color: #0000FF; font-weight: bold } /* Name.Class */ .highlight .no { color: #880000 } /* Name.Constant */ .highlight .nd { color: #AA22FF } /* Name.Decorator */ .highlight .ni { color: #999999; font-weight: bold } /* Name.Entity */ .highlight .ne { color: #D2413A; font-weight: bold } /* Name.Exception */ .highlight .nf { color: #0000FF } /* Name.Function */ .highlight .nl { color: #A0A000 } /* Name.Label */ .highlight .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */ .highlight .nt { color: #008000; font-weight: bold } /* Name.Tag */ .highlight .nv { color: #19177C } /* Name.Variable */ .highlight .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */ .highlight .w { color: #bbbbbb } /* Text.Whitespace */ .highlight .mb { color: #666666 } /* Literal.Number.Bin */ .highlight .mf { color: #666666 } /* Literal.Number.Float */ .highlight .mh { color: #666666 } /* Literal.Number.Hex */ .highlight .mi { color: #666666 } /* Literal.Number.Integer */ .highlight .mo { color: #666666 } /* Literal.Number.Oct */ .highlight .sb { color: #BA2121 } /* Literal.String.Backtick */ .highlight .sc { color: #BA2121 } /* Literal.String.Char */ .highlight .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */ .highlight .s2 { color: #BA2121 } /* Literal.String.Double */ .highlight .se { color: #BB6622; font-weight: bold } /* Literal.String.Escape */ .highlight .sh { color: #BA2121 } /* Literal.String.Heredoc */ .highlight .si { color: #BB6688; font-weight: bold } /* Literal.String.Interpol */ .highlight .sx { color: #008000 } /* Literal.String.Other */ .highlight .sr { color: #BB6688 } /* Literal.String.Regex */ .highlight .s1 { color: #BA2121 } /* Literal.String.Single */ .highlight .ss { color: #19177C } /* Literal.String.Symbol */ .highlight .bp { color: #008000 } /* Name.Builtin.Pseudo */ .highlight .vc { color: #19177C } /* Name.Variable.Class */ .highlight .vg { color: #19177C } /* Name.Variable.Global */ .highlight .vi { color: #19177C } /* Name.Variable.Instance */ .highlight .il { color: #666666 } /* Literal.Number.Integer.Long */ Brownian Motion and the Wiener Process Igor Marfin < igor.marfin@unister.de > Abstract The financial statistics claims that Brownian motion can describe the random behaviour of the asset price level $S(t)$ over time. What is the Brownian motion then? More details can be found at https://bitbucket.org/iggy_floyd/tsa-arma-model . Initialization In [1]: import sys sys . path = [ '/usr/local/lib/python2.7/dist-packages' ] + sys . path # to fix the problem with numpy: this replaces 1.6 version by 1.9 % matplotlib inline % pylab inline ion () import os import matplotlib import numpy as np import matplotlib.pyplot as pl import matplotlib as mpl import logging import pymc as pm # a plotter and dataframe modules import seaborn as sns # seaborn to make a nice plots of the data import pandas as pd import scipy.stats as stats # Set up logging. logger = logging . getLogger () logger . setLevel ( logging . INFO ) # styling 1: comment it if you find it not good for you import json s = json . load ( open ( \"styles/my_matplotlibrc.json\" ) ) matplotlib . rcParams . update ( s ) from IPython.core.display import HTML from IPython.display import display , Math , Latex import urllib2 def css_styling (): styles = open ( \"styles/custom_v3.css\" , \"r\" ) . read () return HTML ( styles ) css_styling () #HTML( urllib2.urlopen('http://bit.ly/1Bf5Hft').read() ) ion () # styling 2: comment it if you find it not good for you from book_format import load_style , figsize , set_figsize load_style () Populating the interactive namespace from numpy and matplotlib /usr/local/lib/python2.7/dist-packages/matplotlib/__init__.py:855: UserWarning: svg.embed_char_paths is deprecated and replaced with svg.fonttype; please use the latter. warnings.warn(self.msg_depr % (key, alt_key)) Out[1]: @import url('http://fonts.googleapis.com/css?family=Source+Code+Pro'); @import url('http://fonts.googleapis.com/css?family=Vollkorn'); @import url('http://fonts.googleapis.com/css?family=Arimo'); div.cell{ width: 1200px; margin-left: 0% !important; margin-right: auto; } div.text_cell code { background: transparent; color: #000000; font-weight: 600; font-size: 11pt; font-style: bold; font-family: 'Source Code Pro', Consolas, monocco, monospace; } h1 { font-family: 'Open sans',verdana,arial,sans-serif; } div.input_area { background: #F6F6F9; border: 1px solid #586e75; } .text_cell_render h1 { font-weight: 200; font-size: 30pt; line-height: 100%; color:#c76c0c; margin-bottom: 0.5em; margin-top: 1em; display: block; white-space: wrap; } h2 { font-family: 'Open sans',verdana,arial,sans-serif; } .text_cell_render h2 { font-weight: 200; font-size: 16pt; font-style: italic; line-height: 100%; color:#c76c0c; margin-bottom: 0.5em; margin-top: 1.5em; display: inline; white-space: wrap; } h3 { font-family: 'Open sans',verdana,arial,sans-serif; } .text_cell_render h3 { font-weight: 200; font-size: 14pt; line-height: 100%; color:#d77c0c; margin-bottom: 0.5em; margin-top: 2em; display: block; white-space: nowrap; } h4 { font-family: 'Open sans',verdana,arial,sans-serif; } .text_cell_render h4 { font-weight: 100; font-size: 14pt; color:#d77c0c; margin-bottom: 0.5em; margin-top: 0.5em; display: block; white-space: nowrap; } h5 { font-family: 'Open sans',verdana,arial,sans-serif; } .text_cell_render h5 { font-weight: 200; font-style: normal; color: #1d3b84; font-size: 16pt; margin-bottom: 0em; margin-top: 0.5em; display: block; white-space: nowrap; } div.text_cell_render{ font-family: 'Arimo',verdana,arial,sans-serif; line-height: 125%; font-size: 120%; text-align:justify; text-justify:inter-word; } div.output_subarea.output_text.output_pyout { overflow-x: auto; overflow-y: scroll; max-height: 50000px; } div.output_subarea.output_stream.output_stdout.output_text { overflow-x: auto; overflow-y: scroll; max-height: 50000px; } div.output_wrapper{ margin-top:0.2em; margin-bottom:0.2em; } code{ font-size: 70%; } .rendered_html code{ background-color: transparent; } ul{ margin: 2em; } ul li{ padding-left: 0.5em; margin-bottom: 0.5em; margin-top: 0.5em; } ul li li{ padding-left: 0.2em; margin-bottom: 0.2em; margin-top: 0.2em; } ol{ margin: 2em; } ol li{ padding-left: 0.5em; margin-bottom: 0.5em; margin-top: 0.5em; } ul li{ padding-left: 0.5em; margin-bottom: 0.5em; margin-top: 0.2em; } a:link{ font-weight: bold; color:#447adb; } a:visited{ font-weight: bold; color: #1d3b84; } a:hover{ font-weight: bold; color: #1d3b84; } a:focus{ font-weight: bold; color:#447adb; } a:active{ font-weight: bold; color:#447adb; } .rendered_html :link { text-decoration: underline; } .rendered_html :hover { text-decoration: none; } .rendered_html :visited { text-decoration: none; } .rendered_html :focus { text-decoration: none; } .rendered_html :active { text-decoration: none; } .warning{ color: rgb( 240, 20, 20 ) } hr { color: #f3f3f3; background-color: #f3f3f3; height: 1px; } blockquote{ display:block; background: #fcfcfc; border-left: 5px solid #c76c0c; font-family: 'Open sans',verdana,arial,sans-serif; width:1000px; padding: 10px 10px 10px 10px; text-align:justify; text-justify:inter-word; } blockquote p { margin-bottom: 0; line-height: 125%; font-size: 100%; } MathJax.Hub.Config({ TeX: { extensions: [\"AMSmath.js\"] }, tex2jax: { inlineMath: [ ['$','$'], [\"\\\\(\",\"\\\\)\"] ], displayMath: [ ['$$','$$'], [\"\\\\[\",\"\\\\]\"] ] }, displayAlign: 'center', // Change this to 'center' to center equations. \"HTML-CSS\": { scale:100, availableFonts: [\"Neo-Euler\"], preferredFont: \"Neo-Euler\", webFont: \"Neo-Euler\", styles: {'.MathJax_Display': {\"margin\": 4}} } }); The Markov property of the Brownian motion Definitely, the Brownian motion is the Markov chain of 'stochastic' states which do not have 'memory'. This means that the conditional probability distribution of the future states of the process are independent of any previous state, with the exception of the current state [1] $$\\mathbb{P}(X_n=x_n|X_{n-1}=x_{n-1}, \\dots, X_0=x_0)=\\mathbb{P}(X_n=x_n|X_{n-1}=x_{n-1})$$ A sensible way to introduce the Markov property is through a sequence of random variables $Z_i$, which can take one of two values from the set {1,−1}. This is known as a coin toss. We can calculate the expectations of $Z_i$: $$E(Z_i)=0,\\,\\,E(Z&#94;2_i)=1,\\,\\,E(Z_iZ_k)=0.$$ The key point is that the expectation of $Z_i$ has no dependence on any previous values within the sequence. Let us take the partial sums of our random variables within our coin toss, which we will denote by Si: $$S_i=\\sum_{k=1}&#94;i Z_k.$$ We can now calculate the expectations of our partial sums, using the linearity of the expectation operator: $$E(S_i)=0,\\,\\,E(S&#94;2_i)=E(Z&#94;2_1+2*Z_1Z_2+...)=i.$$ We see that, again, there is no dependence on the expectation of $S_i$ of any previous value within the sequence of partial sums. The only thing important here is that the variance of the partial sum is increasing linearly with the size $i$ of the partial chain. We see that, again, there is no dependence on the expectation of $S_i$ of any previous value within the sequence of partial sums. We can extend this to discuss conditional expectation. Conditional expectation is the expectation of a random variable with respect to some conditional probability distribution. Hence, we can ask that if $i=4$ (i.e. we carry out four coin tosses), what does this mean for the expectation of $S_5$? $$ E(S_5|Z_1,Z_2,Z_3,Z_4)=E(Z_5 + S_4|Z_1,Z_2,Z_3,Z_4) = E(Z_5) + E(S_4|Z_1,Z_2,Z_3,Z_4) = 0 + S_4 = S_4 $$ That is, the expected value of $S_i$ is only dependent upon the previous value $S_{i−1}$, not on any values prior to that. This is known as the Markov Property. Definition of the Brawnian Motion (BM) Consider a continuous real-valued time interval $[0,T], \\, T>0$. In this interval $N$ coin tosses will be carried out, which each take a time $T/N$ and hence are spaced equally. Concurrently, the payoff returned from each coin toss will be modified. The sequence of discrete random variables representing the coin toss is $Z_i∈\\{−1,1\\}$. A further sequence of discrete random variables (DRV), $\\tilde{Z}_i∈\\{\\sqrt{T/N},-\\sqrt{T/N}\\}$, can be defined. The definition of such a sequence of DRVs is used to provide a very specific quadratic variation of the coin toss. The quadratic variation of a sequence of DRVs is defined to be simply the sum of the squared differences of the current and previous terms: $$\\sum_{k=1}&#94;i (S_k - S_{k-1})&#94;2.$$ For $Z_i$, the previous coin toss random variable sequence, the quadratic variation is given by: $$\\sum_{k=1}&#94;i (S_k - S_{k-1})&#94;2=i.$$ For $\\tilde{Z}_i$, the quadratic variation of the partial sums $\\tilde{S}_i$ is: $$\\sum_{k=1}&#94;N (\\tilde{S}_k - \\tilde{S}_{k-1})&#94;2= \\sum_{k=1}&#94;N \\left(\\sqrt{\\frac{T}{N}}\\right)&#94;2 = T $$ Thus, by construction, the quadratic variation of the amended coin toss $\\tilde{Z}_i$ is simply the total duration of all tosses, $T$. Importantly, note that both the Markov properties are retained by $\\tilde{Z}_i$. As $N\\rightarrow \\inf$ the random walk coin toss does not diverge. Definition: A sequence of random variables $B(t)$ (a Markov chain) is a Brownian motion if $B(0)=0$, and for all >$t,s$ such that s < t and $B(t)−B(s)$ is normally distributed with variance $t−s$ and the distribution of $B(t)−B(s)$ is independent of $B(r)$ for $r\\leq s$. An example of the Brownian motion (Wiener Process) is given below. In [2]: from numpy.random import standard_normal from numpy import array , zeros , sqrt , shape from pylab import * def brownian ( x_0 = 1e-9 , T = 100 , dt = 1e0 , mu = 0. , sigma = 1e-2 , N_Sim = 10 , geometric = True ): ''' generate the geometric brownian motion (GBM) ''' Steps = round ( T / dt ); #Steps in years S = zeros ([ N_Sim , Steps ], dtype = float ) x = range ( 0 , int ( Steps ), 1 ) for j in range ( 0 , N_Sim , 1 ): S [ j , 0 ] = x_0 for i in x [: - 1 ]: if ( geometric ): S [ j , i + 1 ] = S [ j , i ] + S [ j , i ] * ( mu - 0.5 * pow ( sigma , 2 )) * dt + sigma * S [ j , i ] * sqrt ( dt ) * standard_normal (); else : S [ j , i + 1 ] = S [ j , i ] + sigma * sqrt ( dt ) * standard_normal () plot ( x , S [ j ], label = 'asset %i ' % j ) title ( ' %d Simulations of %d Days with parameters: (Sigma %.6f , Mu %.6f , S0 %.6f )' % ( int ( N_Sim ), int ( Steps ), sigma , mu , x_0 )) xlabel ( 'time (days)' ) ylabel ( 'stock price' ) legend ( loc = \"upper left\" ) show () sigma = 1e-2 mu = 0. brownian ( x_0 = 1e-4 , T = 100. , dt = 1e0 , mu = mu , sigma = sigma , N_Sim = 5 , geometric = False ) In [91]: % history -f brownian.py -l 1 File u'brownian.py' exists. Overwrite? y Overwriting file. The Brownian motion has a non-zero probability of being negative. This is clearly not a property shared by real-world assets - stock prices cannot be less than zero. Hence, although the stochastic nature of a Brownian motion for our model should be retained, it is necessary to adjust exactly how that randomness is distributed. In particular, the concept of geometric Brownian motion (GBM) can be introduced, which will solve the problem of negative stock prices. Geometric Brownian motion (GBM) Definition (Stochastic Differential Equation) : Let $B(t)$ be a Brownian motion. If $W(t)$ is a sequence of random variables, such that for all t, $$W(t+\\delta t)−W(t)−\\delta t \\mu(t,W(t))−\\sigma (t,B(t))(B(t+\\delta t)−B(t))$$ is a random variable with mean and variance that are O(\\delta t), then: $$dW=\\mu(t,W(t))dt+\\sigma(t,W(t))dB$$ is a stochastic differential equation for the Geometric Brownian motion, W(t),or the Ito drift-diffusion process. It can be seen that $\\mu$ and $\\sigma$ are both functions of $t$ and $W$. $\\mu$ has the interpretation of a non-stochastic drift coefficient, while $\\sigma$ represents the coefficient of volatility - it is multiplied by the stochastic $dB$ term. The drift and volatility functions are simple scales $$ \\mu(t,W(t)) = \\mu\\cdot W(t),\\\\ \\sigma(t,W(t)) = \\sigma\\cdot W(t).$$ In [3]: sigma = 1e-2 mu = 0. # we have changed geometric=False to geometric=True #brownian(x_0=1e-4,T=100.,dt=1e0, mu=mu, sigma=sigma, N_Sim=5,geometric=True) # we can add the trend ~ mu=50*x_0 and increase the volitality in 3/2 times mu =. 5e-2 sigma *= 3. / 2. brownian ( x_0 = 1e-4 , T = 100. , dt = 1e0 , mu = mu , sigma = sigma , N_Sim = 5 , geometric = True ) The main difference between GBM and BM is the 'trend' effect. Mathematicaly, this effect is called Mean Reversion . Autoregressive Timeseries Ok. We have reached this point: why do people work with timeseries of price assets? Definition : Timeseries $X_t=\\{x_t\\}$ is an another representation of the Brownian Motion $$ X_t \\equiv B(t) $$ All modeling the price assets will be done in the notation of $X_t=\\{x_t\\}$. The Mean Reverting timeseries of continuous variable is given by an Ornstein-Uhlenbeck (Ornstein-Uhlenbeck Process) stochastic differential equation [2] : $$dx_t=\\theta(\\mu−x_t)dt+\\sigma dW_t.$$ In the case of the discreete variables, as price assets, the Mean Reverting timeseries is modeled by the combination of Autoregressive and Moving-average models, which both together define the ARMA model ARMA(p,q) [3] : $$ X_t = c + \\varepsilon_t + \\sum_{i=1}&#94;p \\varphi_i X_{t-i} + \\sum_{i=1}&#94;q \\theta_i \\varepsilon_{t-i}.\\, $$ $p$ and $q$ refer to the degrees of the model: p autoregressive terms and q moving-average terms. References [1] Markov property [2] Ornstein–Uhlenbeck process [3] Autoregressive–moving-average model In [ ]: if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML'; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: 'center',\" + \" displayIndent: '0em',\" + \" showMathMenu: true,\" + \" tex2jax: { \" + \" inlineMath: [ ['$','$'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }\" + \" } \" + \"}); \"; (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Statistics","title":"Brownian Motion and the Wiener Process"},{"url":"http://igormarfin.github.io/blog/2016/02/08/money-management-system.-portfolio-optimization-approaches-or-how-to-become-a-millionaire-from-stock-tradings/","text":"/*! * * IPython notebook * */.ansibold{font-weight:bold}.ansiblack{color:black}.ansired{color:darkred}.ansigreen{color:darkgreen}.ansiyellow{color:#c4a000}.ansiblue{color:darkblue}.ansipurple{color:darkviolet}.ansicyan{color:steelblue}.ansigray{color:gray}.ansibgblack{background-color:black}.ansibgred{background-color:red}.ansibggreen{background-color:green}.ansibgyellow{background-color:yellow}.ansibgblue{background-color:blue}.ansibgpurple{background-color:magenta}.ansibgcyan{background-color:cyan}.ansibggray{background-color:gray}div.cell{border:1px solid transparent;display:-webkit-box;-webkit-box-orient:vertical;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:vertical;-moz-box-align:stretch;display:box;box-orient:vertical;box-align:stretch;display:flex;flex-direction:column;align-items:stretch;border-radius:2px;box-sizing:border-box;-moz-box-sizing:border-box;-webkit-box-sizing:border-box;border-width:thin;border-style:solid;width:100%;padding:5px;margin:0;outline:none}div.cell.selected{border-color:#ababab}@media print{div.cell.selected{border-color:transparent}}.edit_mode div.cell.selected{border-color:green}@media print{.edit_mode div.cell.selected{border-color:transparent}}.prompt{min-width:14ex;padding:.4em;margin:0;font-family:monospace;text-align:right;line-height:1.21429em}@media (max-width:540px){.prompt{text-align:left}}div.inner_cell{display:-webkit-box;-webkit-box-orient:vertical;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:vertical;-moz-box-align:stretch;display:box;box-orient:vertical;box-align:stretch;display:flex;flex-direction:column;align-items:stretch;-webkit-box-flex:1;-moz-box-flex:1;box-flex:1;flex:1}@-moz-document url-prefix(){div.inner_cell{overflow-x:hidden}}div.input_area{border:1px solid #cfcfcf;border-radius:2px;background:#f7f7f7;line-height:1.21429em}div.prompt:empty{padding-top:0;padding-bottom:0}div.unrecognized_cell{padding:5px 5px 5px 0;display:-webkit-box;-webkit-box-orient:horizontal;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:horizontal;-moz-box-align:stretch;display:box;box-orient:horizontal;box-align:stretch;display:flex;flex-direction:row;align-items:stretch}div.unrecognized_cell .inner_cell{border-radius:2px;padding:5px;font-weight:bold;color:red;border:1px solid #cfcfcf;background:#eaeaea}div.unrecognized_cell .inner_cell a{color:inherit;text-decoration:none}div.unrecognized_cell .inner_cell a:hover{color:inherit;text-decoration:none}@media (max-width:540px){div.unrecognized_cell>div.prompt{display:none}}@media print{div.code_cell{page-break-inside:avoid}}div.input{page-break-inside:avoid;display:-webkit-box;-webkit-box-orient:horizontal;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:horizontal;-moz-box-align:stretch;display:box;box-orient:horizontal;box-align:stretch;display:flex;flex-direction:row;align-items:stretch}@media (max-width:540px){div.input{display:-webkit-box;-webkit-box-orient:vertical;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:vertical;-moz-box-align:stretch;display:box;box-orient:vertical;box-align:stretch;display:flex;flex-direction:column;align-items:stretch}}div.input_prompt{color:navy;border-top:1px solid transparent}div.input_area>div.highlight{margin:.4em;border:none;padding:0;background-color:transparent}div.input_area>div.highlight>pre{margin:0;border:none;padding:0;background-color:transparent}.CodeMirror{line-height:1.21429em;font-size:14px;height:auto;background:none}.CodeMirror-scroll{overflow-y:hidden;overflow-x:auto}.CodeMirror-lines{padding:.4em}.CodeMirror-linenumber{padding:0 8px 0 4px}.CodeMirror-gutters{border-bottom-left-radius:2px;border-top-left-radius:2px}.CodeMirror pre{padding:0;border:0;border-radius:0}.highlight-base{}.highlight-variable{}.highlight-variable-2{color:#1a1a1a}.highlight-variable-3{color:#333}.highlight-string{color:#ba2121}.highlight-comment{color:#408080;font-style:italic}.highlight-number{80}.highlight-atom{color:#88f}.highlight-keyword{8000;font-weight:bold}.highlight-builtin{8000}.highlight-error{color:#f00}.highlight-operator{color:#a2f;font-weight:bold}.highlight-meta{color:#a2f}.highlight-def{f}.highlight-string-2{color:#f50}.highlight-qualifier{color:#555}.highlight-bracket{color:#997}.highlight-tag{color:#170}.highlight-attribute{c}.highlight-header{color:blue}.highlight-quote{90}.highlight-link{c}.cm-s-ipython span.cm-keyword{8000;font-weight:bold}.cm-s-ipython span.cm-atom{color:#88f}.cm-s-ipython span.cm-number{80}.cm-s-ipython span.cm-def{f}.cm-s-ipython span.cm-variable{}.cm-s-ipython span.cm-operator{color:#a2f;font-weight:bold}.cm-s-ipython span.cm-variable-2{color:#1a1a1a}.cm-s-ipython span.cm-variable-3{color:#333}.cm-s-ipython span.cm-comment{color:#408080;font-style:italic}.cm-s-ipython span.cm-string{color:#ba2121}.cm-s-ipython span.cm-string-2{color:#f50}.cm-s-ipython span.cm-meta{color:#a2f}.cm-s-ipython span.cm-qualifier{color:#555}.cm-s-ipython span.cm-builtin{8000}.cm-s-ipython span.cm-bracket{color:#997}.cm-s-ipython span.cm-tag{color:#170}.cm-s-ipython span.cm-attribute{c}.cm-s-ipython span.cm-header{color:blue}.cm-s-ipython span.cm-quote{90}.cm-s-ipython span.cm-link{c}.cm-s-ipython span.cm-error{color:#f00}.cm-s-ipython span.cm-tab{background:url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADAAAAAMCAYAAAAkuj5RAAAAAXNSR0IArs4c6QAAAGFJREFUSMft1LsRQFAQheHPowAKoACx3IgEKtaEHujDjORSgWTH/ZOdnZOcM/sgk/kFFWY0qV8foQwS4MKBCS3qR6ixBJvElOobYAtivseIE120FaowJPN75GMu8j/LfMwNjh4HUpwg4LUAAAAASUVORK5CYII=);background-position:right;background-repeat:no-repeat}div.output_wrapper{position:relative;display:-webkit-box;-webkit-box-orient:vertical;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:vertical;-moz-box-align:stretch;display:box;box-orient:vertical;box-align:stretch;display:flex;flex-direction:column;align-items:stretch}div.output_scroll{height:24em;width:100%;overflow:auto;border-radius:2px;-webkit-box-shadow:inset 0 2px 8px rgba(0,0,0,0.8);box-shadow:inset 0 2px 8px rgba(0,0,0,0.8);display:block}div.output_collapsed{margin:0;padding:0;display:-webkit-box;-webkit-box-orient:vertical;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:vertical;-moz-box-align:stretch;display:box;box-orient:vertical;box-align:stretch;display:flex;flex-direction:column;align-items:stretch}div.out_prompt_overlay{height:100%;padding:0 .4em;position:absolute;border-radius:2px}div.out_prompt_overlay:hover{-webkit-box-shadow:inset 0 0 1px #000;box-shadow:inset 0 0 1px #000;background:rgba(240,240,240,0.5)}div.output_prompt{color:darkred}div.output_area{padding:0;page-break-inside:avoid;display:-webkit-box;-webkit-box-orient:horizontal;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:horizontal;-moz-box-align:stretch;display:box;box-orient:horizontal;box-align:stretch;display:flex;flex-direction:row;align-items:stretch}div.output_area .MathJax_Display{text-align:left !important}div.output_area div.output_area .output{display:-webkit-box;-webkit-box-orient:vertical;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:vertical;-moz-box-align:stretch;display:box;box-orient:vertical;box-align:stretch;display:flex;flex-direction:column;align-items:stretch}@media (max-width:540px){div.output_area{display:-webkit-box;-webkit-box-orient:vertical;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:vertical;-moz-box-align:stretch;display:box;box-orient:vertical;box-align:stretch;display:flex;flex-direction:column;align-items:stretch}}div.output_area pre{margin:0;padding:0;border:0;vertical-align:baseline;color:black;background-color:transparent;border-radius:0}div.output_subarea{padding:.4em;-webkit-box-flex:1;-moz-box-flex:1;box-flex:1;flex:1}div.output_text{text-align:left;line-height:1.21429em}div.output_stderr{background:#fdd}div.output_latex{text-align:left}div.output_javascript:empty{padding:0}.js-error{color:darkred}div.raw_input_container{font-family:monospace;padding-top:5px}input.raw_input{font-family:inherit;font-size:inherit;color:inherit;width:auto;vertical-align:baseline;padding:0 .25em;margin:0 .25em}input.raw_input:focus{box-shadow:none}p.p-space{margin-bottom:10px}div.output_unrecognized{padding:5px;font-weight:bold;color:red}div.output_unrecognized a{color:inherit;text-decoration:none}div.output_unrecognized a:hover{color:inherit;text-decoration:none}.rendered_html{}.rendered_html :link{text-decoration:underline}.rendered_html :visited{text-decoration:underline}.rendered_html h1:first-child{margin-top:.538em}.rendered_html h2:first-child{margin-top:.636em}.rendered_html h3:first-child{margin-top:.777em}.rendered_html h4:first-child{margin-top:1em}.rendered_html h5:first-child{margin-top:1em}.rendered_html h6:first-child{margin-top:1em}.rendered_html *+ul{margin-top:1em}.rendered_html *+ol{margin-top:1em}.rendered_html *+table{margin-top:1em}.rendered_html *+p{margin-top:1em}.rendered_html *+img{margin-top:1em}div.text_cell{display:-webkit-box;-webkit-box-orient:horizontal;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:horizontal;-moz-box-align:stretch;display:box;box-orient:horizontal;box-align:stretch;display:flex;flex-direction:row;align-items:stretch}@media (max-width:540px){div.text_cell>div.prompt{display:none}}div.text_cell_render{outline:none;resize:none;width:inherit;border-style:none;padding:.5em .5em .5em .4em;box-sizing:border-box;-moz-box-sizing:border-box;-webkit-box-sizing:border-box}a.anchor-link:link{text-decoration:none;padding:0 20px;visibility:hidden}h1:hover .anchor-link,h2:hover .anchor-link,h3:hover .anchor-link,h4:hover .anchor-link,h5:hover .anchor-link,h6:hover .anchor-link{visibility:visible}.text_cell.rendered .input_area{display:none}.text_cell.unrendered .text_cell_render{display:none}.cm-header-1,.cm-header-2,.cm-header-3,.cm-header-4,.cm-header-5,.cm-header-6{font-weight:bold;font-family:\"Helvetica Neue\",Helvetica,Arial,sans-serif}.cm-header-1{font-size:185.7%}.cm-header-2{font-size:157.1%}.cm-header-3{font-size:128.6%}.cm-header-4{font-size:110%}.cm-header-5{font-size:100%;font-style:italic}.cm-header-6{font-size:100%;font-style:italic}.widget-interact>div,.widget-interact>input{padding:2.5px}.widget-area{page-break-inside:avoid;display:-webkit-box;-webkit-box-orient:horizontal;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:horizontal;-moz-box-align:stretch;display:box;box-orient:horizontal;box-align:stretch;display:flex;flex-direction:row;align-items:stretch}.widget-area .widget-subarea{padding:.44em .4em .4em 1px;margin-left:6px;box-sizing:border-box;-moz-box-sizing:border-box;-webkit-box-sizing:border-box;display:-webkit-box;-webkit-box-orient:vertical;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:vertical;-moz-box-align:stretch;display:box;box-orient:vertical;box-align:stretch;display:flex;flex-direction:column;align-items:stretch;-webkit-box-flex:2;-moz-box-flex:2;box-flex:2;flex:2;-webkit-box-align:start;-moz-box-align:start;box-align:start;align-items:flex-start}.widget-area.connection-problems .prompt:after{content:\"\\f127\";font-family:'FontAwesome';color:#d9534f;font-size:14px;top:3px;padding:3px}.slide-track{border:1px solid #ccc;background:#fff;border-radius:2px}.widget-hslider{padding-left:8px;padding-right:2px;overflow:visible;width:350px;height:5px;max-height:5px;margin-top:13px;margin-bottom:10px;border:1px solid #ccc;background:#fff;border-radius:2px;display:-webkit-box;-webkit-box-orient:horizontal;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:horizontal;-moz-box-align:stretch;display:box;box-orient:horizontal;box-align:stretch;display:flex;flex-direction:row;align-items:stretch}.widget-hslider .ui-slider{border:0;background:none;display:-webkit-box;-webkit-box-orient:horizontal;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:horizontal;-moz-box-align:stretch;display:box;box-orient:horizontal;box-align:stretch;display:flex;flex-direction:row;align-items:stretch;-webkit-box-flex:1;-moz-box-flex:1;box-flex:1;flex:1}.widget-hslider .ui-slider .ui-slider-handle{width:12px;height:28px;margin-top:-8px;border-radius:2px}.widget-hslider .ui-slider .ui-slider-range{height:12px;margin-top:-4px;background:#eee}.widget-vslider{padding-bottom:5px;overflow:visible;width:5px;max-width:5px;height:250px;margin-left:12px;border:1px solid #ccc;background:#fff;border-radius:2px;display:-webkit-box;-webkit-box-orient:vertical;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:vertical;-moz-box-align:stretch;display:box;box-orient:vertical;box-align:stretch;display:flex;flex-direction:column;align-items:stretch}.widget-vslider .ui-slider{border:0;background:none;margin-left:-4px;margin-top:5px;display:-webkit-box;-webkit-box-orient:vertical;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:vertical;-moz-box-align:stretch;display:box;box-orient:vertical;box-align:stretch;display:flex;flex-direction:column;align-items:stretch;-webkit-box-flex:1;-moz-box-flex:1;box-flex:1;flex:1}.widget-vslider .ui-slider .ui-slider-handle{width:28px;height:12px;margin-left:-9px;border-radius:2px}.widget-vslider .ui-slider .ui-slider-range{width:12px;margin-left:-1px;background:#eee}.widget-text{width:350px;margin:0}.widget-listbox{width:350px;margin-bottom:0}.widget-numeric-text{width:150px;margin:0}.widget-progress{margin-top:6px;min-width:350px}.widget-progress .progress-bar{-webkit-transition:none;-moz-transition:none;-ms-transition:none;-o-transition:none;transition:none}.widget-combo-btn{min-width:125px}.widget_item .dropdown-menu li a{color:inherit}.widget-hbox{display:-webkit-box;-webkit-box-orient:horizontal;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:horizontal;-moz-box-align:stretch;display:box;box-orient:horizontal;box-align:stretch;display:flex;flex-direction:row;align-items:stretch}.widget-hbox input[type=\"checkbox\"]{margin-top:9px;margin-bottom:10px}.widget-hbox .widget-label{min-width:10ex;padding-right:8px;padding-top:5px;text-align:right;vertical-align:text-top}.widget-hbox .widget-readout{padding-left:8px;padding-top:5px;text-align:left;vertical-align:text-top}.widget-vbox{display:-webkit-box;-webkit-box-orient:vertical;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:vertical;-moz-box-align:stretch;display:box;box-orient:vertical;box-align:stretch;display:flex;flex-direction:column;align-items:stretch}.widget-vbox .widget-label{padding-bottom:5px;text-align:center;vertical-align:text-bottom}.widget-vbox .widget-readout{padding-top:5px;text-align:center;vertical-align:text-top}.widget-box{box-sizing:border-box;-moz-box-sizing:border-box;-webkit-box-sizing:border-box;-webkit-box-align:start;-moz-box-align:start;box-align:start;align-items:flex-start}.widget-radio-box{display:-webkit-box;-webkit-box-orient:vertical;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:vertical;-moz-box-align:stretch;display:box;box-orient:vertical;box-align:stretch;display:flex;flex-direction:column;align-items:stretch;box-sizing:border-box;-moz-box-sizing:border-box;-webkit-box-sizing:border-box;padding-top:4px}.widget-radio-box label{margin-top:0}.widget-radio{margin-left:20px} .highlight .hll { background-color: #ffffcc } .highlight { background: #f8f8f8; } .highlight .c { color: #408080; font-style: italic } /* Comment */ .highlight .err { border: 1px solid #FF0000 } /* Error */ .highlight .k { color: #008000; font-weight: bold } /* Keyword */ .highlight .o { color: #666666 } /* Operator */ .highlight .cm { color: #408080; font-style: italic } /* Comment.Multiline */ .highlight .cp { color: #BC7A00 } /* Comment.Preproc */ .highlight .c1 { color: #408080; font-style: italic } /* Comment.Single */ .highlight .cs { color: #408080; font-style: italic } /* Comment.Special */ .highlight .gd { color: #A00000 } /* Generic.Deleted */ .highlight .ge { font-style: italic } /* Generic.Emph */ .highlight .gr { color: #FF0000 } /* Generic.Error */ .highlight .gh { color: #000080; font-weight: bold } /* Generic.Heading */ .highlight .gi { color: #00A000 } /* Generic.Inserted */ .highlight .go { color: #888888 } /* Generic.Output */ .highlight .gp { color: #000080; font-weight: bold } /* Generic.Prompt */ .highlight .gs { font-weight: bold } /* Generic.Strong */ .highlight .gu { color: #800080; font-weight: bold } /* Generic.Subheading */ .highlight .gt { color: #0044DD } /* Generic.Traceback */ .highlight .kc { color: #008000; font-weight: bold } /* Keyword.Constant */ .highlight .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */ .highlight .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */ .highlight .kp { color: #008000 } /* Keyword.Pseudo */ .highlight .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */ .highlight .kt { color: #B00040 } /* Keyword.Type */ .highlight .m { color: #666666 } /* Literal.Number */ .highlight .s { color: #BA2121 } /* Literal.String */ .highlight .na { color: #7D9029 } /* Name.Attribute */ .highlight .nb { color: #008000 } /* Name.Builtin */ .highlight .nc { color: #0000FF; font-weight: bold } /* Name.Class */ .highlight .no { color: #880000 } /* Name.Constant */ .highlight .nd { color: #AA22FF } /* Name.Decorator */ .highlight .ni { color: #999999; font-weight: bold } /* Name.Entity */ .highlight .ne { color: #D2413A; font-weight: bold } /* Name.Exception */ .highlight .nf { color: #0000FF } /* Name.Function */ .highlight .nl { color: #A0A000 } /* Name.Label */ .highlight .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */ .highlight .nt { color: #008000; font-weight: bold } /* Name.Tag */ .highlight .nv { color: #19177C } /* Name.Variable */ .highlight .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */ .highlight .w { color: #bbbbbb } /* Text.Whitespace */ .highlight .mb { color: #666666 } /* Literal.Number.Bin */ .highlight .mf { color: #666666 } /* Literal.Number.Float */ .highlight .mh { color: #666666 } /* Literal.Number.Hex */ .highlight .mi { color: #666666 } /* Literal.Number.Integer */ .highlight .mo { color: #666666 } /* Literal.Number.Oct */ .highlight .sb { color: #BA2121 } /* Literal.String.Backtick */ .highlight .sc { color: #BA2121 } /* Literal.String.Char */ .highlight .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */ .highlight .s2 { color: #BA2121 } /* Literal.String.Double */ .highlight .se { color: #BB6622; font-weight: bold } /* Literal.String.Escape */ .highlight .sh { color: #BA2121 } /* Literal.String.Heredoc */ .highlight .si { color: #BB6688; font-weight: bold } /* Literal.String.Interpol */ .highlight .sx { color: #008000 } /* Literal.String.Other */ .highlight .sr { color: #BB6688 } /* Literal.String.Regex */ .highlight .s1 { color: #BA2121 } /* Literal.String.Single */ .highlight .ss { color: #19177C } /* Literal.String.Symbol */ .highlight .bp { color: #008000 } /* Name.Builtin.Pseudo */ .highlight .vc { color: #19177C } /* Name.Variable.Class */ .highlight .vg { color: #19177C } /* Name.Variable.Global */ .highlight .vi { color: #19177C } /* Name.Variable.Instance */ .highlight .il { color: #666666 } /* Literal.Number.Integer.Long */ Money Management System: Portfolio Optimization Approaches or How to Become a Millionaire from stock tradings $$\\\\[2pt]$$ Igor Marfin [Unister Gmb@2014] < igor.marfin@unister.de > $$\\\\[40pt]$$ Table of Contents 0.1 Abstract 0.2 Initialization 0.3 Introduction to the topic 0.4 Mean variance optimization 0.4.1 How does the MPT treat the loss function and, as a result, risk? 0.4.2 Get and Clean data from Yahoo! Finance 0.4.3 Trade-off between Mean-variance and Returns 0.5 Portfolio mean-variance calculation and optimization 0.6 Black-Litterman model 0.6.1 Reverse optimization of equilibrium returns 0.6.2 Forward optimization of equilibrium returns without application of investor views 0.6.3 Forward optimization of equilibrium returns with application of investor views 0.7 Kelly Criterion 0.7.1 Leverage definition 0.7.2 Assumptions 0.7.3 Kelly Criterion Formulation 0.7.4 Kelly Criterion in Practice $$\\\\[10pt]$$ 0.1 Abstract The material presented here is a brief introduction to the concepts of Mean-Variance Optimization (MVO), Black-Litterman model,Kelly Criterion and Optimization based on Value at Risk (VaR). More details can be found at my repository https://bitbucket.org/iggy_floyd/money-management-sytem-traiding $$\\\\[5pt]$$ 0.2 Initialization As usual we are going to set up the python environment for the data analysis and make a nicer style of the notebook. Thus we run the following commands in the beginning of our study: In [1]: import sys sys . path = [ '/usr/local/lib/python2.7/dist-packages' ] + sys . path # to fix the problem with numpy: this replaces 1.6 version by 1.9 % matplotlib inline % pylab inline ion () import os import matplotlib import numpy as np import matplotlib.pyplot as pl import matplotlib as mpl import logging import pymc as pm # a plotter and dataframe modules import seaborn as sns # seaborn to make a nice plots of the data import pandas as pd import scipy.stats as stats # Set up logging. logger = logging . getLogger () logger . setLevel ( logging . INFO ) from book_format import load_style , figsize , set_figsize load_style () Populating the interactive namespace from numpy and matplotlib Out[1]: @import url('http://fonts.googleapis.com/css?family=Source+Code+Pro'); @import url('http://fonts.googleapis.com/css?family=Vollkorn'); @import url('http://fonts.googleapis.com/css?family=Arimo'); div.cell{ width: 1200px; margin-left: 0% !important; margin-right: auto; } div.text_cell code { background: transparent; color: #000000; font-weight: 600; font-size: 11pt; font-style: bold; font-family: 'Source Code Pro', Consolas, monocco, monospace; } h1 { font-family: 'Open sans',verdana,arial,sans-serif; } div.input_area { background: #F6F6F9; border: 1px solid #586e75; } .text_cell_render h1 { font-weight: 200; font-size: 30pt; line-height: 100%; color:#c76c0c; margin-bottom: 0.5em; margin-top: 1em; display: block; white-space: wrap; } h2 { font-family: 'Open sans',verdana,arial,sans-serif; } .text_cell_render h2 { font-weight: 200; font-size: 16pt; font-style: italic; line-height: 100%; color:#c76c0c; margin-bottom: 0.5em; margin-top: 1.5em; display: inline; white-space: wrap; } h3 { font-family: 'Open sans',verdana,arial,sans-serif; } .text_cell_render h3 { font-weight: 200; font-size: 14pt; line-height: 100%; color:#d77c0c; margin-bottom: 0.5em; margin-top: 2em; display: block; white-space: nowrap; } h4 { font-family: 'Open sans',verdana,arial,sans-serif; } .text_cell_render h4 { font-weight: 100; font-size: 14pt; color:#d77c0c; margin-bottom: 0.5em; margin-top: 0.5em; display: block; white-space: nowrap; } h5 { font-family: 'Open sans',verdana,arial,sans-serif; } .text_cell_render h5 { font-weight: 200; font-style: normal; color: #1d3b84; font-size: 16pt; margin-bottom: 0em; margin-top: 0.5em; display: block; white-space: nowrap; } div.text_cell_render{ font-family: 'Arimo',verdana,arial,sans-serif; line-height: 125%; font-size: 120%; text-align:justify; text-justify:inter-word; } div.output_subarea.output_text.output_pyout { overflow-x: auto; overflow-y: scroll; max-height: 50000px; } div.output_subarea.output_stream.output_stdout.output_text { overflow-x: auto; overflow-y: scroll; max-height: 50000px; } div.output_wrapper{ margin-top:0.2em; margin-bottom:0.2em; } code{ font-size: 70%; } .rendered_html code{ background-color: transparent; } ul{ margin: 2em; } ul li{ padding-left: 0.5em; margin-bottom: 0.5em; margin-top: 0.5em; } ul li li{ padding-left: 0.2em; margin-bottom: 0.2em; margin-top: 0.2em; } ol{ margin: 2em; } ol li{ padding-left: 0.5em; margin-bottom: 0.5em; margin-top: 0.5em; } ul li{ padding-left: 0.5em; margin-bottom: 0.5em; margin-top: 0.2em; } a:link{ font-weight: bold; color:#447adb; } a:visited{ font-weight: bold; color: #1d3b84; } a:hover{ font-weight: bold; color: #1d3b84; } a:focus{ font-weight: bold; color:#447adb; } a:active{ font-weight: bold; color:#447adb; } .rendered_html :link { text-decoration: underline; } .rendered_html :hover { text-decoration: none; } .rendered_html :visited { text-decoration: none; } .rendered_html :focus { text-decoration: none; } .rendered_html :active { text-decoration: none; } .warning{ color: rgb( 240, 20, 20 ) } hr { color: #f3f3f3; background-color: #f3f3f3; height: 1px; } blockquote{ display:block; background: #fcfcfc; border-left: 5px solid #c76c0c; font-family: 'Open sans',verdana,arial,sans-serif; width:1000px; padding: 10px 10px 10px 10px; text-align:justify; text-justify:inter-word; } blockquote p { margin-bottom: 0; line-height: 125%; font-size: 100%; } MathJax.Hub.Config({ TeX: { extensions: [\"AMSmath.js\"] }, tex2jax: { inlineMath: [ ['$','$'], [\"\\\\(\",\"\\\\)\"] ], displayMath: [ ['$$','$$'], [\"\\\\[\",\"\\\\]\"] ] }, displayAlign: 'center', // Change this to 'center' to center equations. \"HTML-CSS\": { scale:100, availableFonts: [\"Neo-Euler\"], preferredFont: \"Neo-Euler\", webFont: \"Neo-Euler\", styles: {'.MathJax_Display': {\"margin\": 4}} } }); The next Javascript extensions will allow to use the text tools as auto-spelling , table-of-content-generator , BibTex-Reference-generator etc In [5]: %% javascript IPython . load_extensions ( \"calico-spell-check\" , \"calico-document-tools\" , \"calico-cell-tools\" ); <IPython.core.display.Javascript object> $$\\\\[5pt]$$ 0.3 Introduction to the topic The fundamental goal of portfolio management is to optimally allocate your investments between different assets. As it is claimed by the wikipedia: Modern portfolio theory (MPT) is a theory of finance that attempts to maximize portfolio expected return for a given amount of portfolio risk, or equivalently minimize risk for a given level of expected return, by carefully choosing the proportions of various assets. First, I am going to consider a Mean variance optimization (MVO) which is a quantitative tool allowing you to make this allocation by considering the trade-off between risk and return. In conventional single period MVO (this is what we intend to study) you will make your portfolio allocation for a single upcoming period, and the goal will be to maximize your expected return subject to a selected level of risk. Single period MVO was developed in the pioneering work of Markowitz. $$\\\\[5pt]$$ 0.4 Mean variance optimization The MPT assumes that investors are risk averse, meaning that given two portfolios that offer the same expected return, investors will prefer the less risky one. What does it mean? The wikipedia says: An investor will take on increased risk only if compensated by higher expected returns. The term risk is introduced here. Let me address you to my previous tutorial on the Bayesian Risk Management where the risk in the trading was defined by the special loss function for finance applications. This loss function was dependent on prediction (guess) and actual returns if the predicted return > true return , we earn money if the predicted return < true return , we loss money and it is very bad! def stock_loss(true_return, yhat, alpha=100.): ''' defines the loss on the stock price ''' if true_return * yhat < 0: # opposite signs, not good # loss is the function of the squared yhat and linear > true_return return alpha * yhat ** 2 - np.sign(true_return) * yhat \\ + abs(true_return) else: return abs(true_return - yhat) By nature, we don't know true_return in the future and what we can do then, we can guess using our expectation on returns estimated from the past data. As our expectations are more far away from the true_return at the moment, the larger values of the stock_loss are, and of course, more risks we would expect. Conversely, an investor who wants higher expected returns must accept more risk. I hope it is clear from the previous point. The exact trade-off will be the same for all investors, but different investors will evaluate the trade-off differently based on individual risk aversion characteristics. Here is only said that different investors have different stock_loss functions. The implication is that a rational investor will not invest in a portfolio if a second portfolio exists with a more favorable risk-expected return profile – i.e., if for that level of risk an alternative portfolio exists that has better expected returns. This statement claims that any rational investor will want to choose the instrument with the highest expectations of the returns if the level of risk stays under acceptable level. 0.4.1 How does the MPT treat the loss function and, as a result, risk? Well, the risk from classical point of view is a function of the true_return and guess. Also the risk is defined by the shape of this function. In many cases it is not convenient and handful. It would be simpler if the risk would be some numerical constant assigned (empirically) to the concrete asset (instrument). From the construction of the loss function, the unknown a priory true_return will play the most important role in the risk: knowledge of the true_return will automatically tell us the value of our guesses such the loss is minimal So, from this point of view, the uncertainty on true_return could be a nice indicator of the risk. And this point is proposed by the MPT as a stream-line in the risk/loss calculation. OK. Let's summarize all points discussed so far. In order to deal with the portfolio management, we need to choose a predefined set of instruments (stocks/assets) which we are going to play with; calculate the variance of instruments,i.e. risk levels assigned to each asset; calculate the expected returns, i.e. the expected profit in future (say we want to buy this stock now in order to sell them later) and the select the instruments from the list in such way that some merit (not defined yet) of the profit of owing these instruments would be maximal. Only one question remains open: how do we define the merit of the profit from the portfolio? There are several approaches which help to answer this question. We will start from considering the Mean Variance Optimization . In [6]: from IPython.display import Image Image ( filename = 'stock_loss.png' ) Out[6]: $$\\\\[5pt]$$ 0.4.2 Get and Clean data from Yahoo! Finance First, we define the list of instruments which we are going to use in constructions of our portfolio. Then we develop the machinery to get data from the Yahoo! Finance and prepare (clean) the data to the appropriate format. One of the functions below allows to communicate with the Yahoo Financial to get historical data on trading operations. Another one is used to add returns on 'Close' prices to the dataframe. In [7]: ''' Igor Marfin <Unister Gmb, 2014> igor.marfin@unister.de Extraction the financial data from Yahoo! Finance ''' from pandas.io.data import DataReader from datetime import datetime instruments = [ 'XOM' , 'AAPL' , 'MSFT' , 'JNJ' , 'GE' , 'CVX' , 'PG' , 'WFC' ] def getStockPrices ( startdate = datetime ( 1986 , 9 , 1 ), enddate = datetime . now (), stocks = instruments ): '''returns the stock prices''' data = map ( lambda x : DataReader ( x , \"yahoo\" , startdate , enddate ), stocks ) for i , symbol in enumerate ( stocks ): data [ i ][ 'Symbol' ] = stocks [ i ] return data def addPctChange ( dataframe , price = 'Close' ): '''adds the percentage on the change of the price''' # get Series on the interested column _series = dataframe [ price ] _series = _series . pct_change () dataframe [ price + '_pct_change' ] = _series return dataframe # test of the reader and 'PctChange' adder data = getStockPrices () addPctChange ( data [ 5 ]) . dropna () . head () Out[7]: Open High Low Close Volume Adj Close Symbol Close_pct_change Date 1986-09-03 44.125 45.500 44.125 45.500 3149200 3.886914 CVX 0.025352 1986-09-04 45.500 47.125 45.250 46.625 5000000 3.983019 CVX 0.024725 1986-09-05 46.750 47.125 46.625 47.000 5240000 4.015054 CVX 0.008043 1986-09-08 46.750 47.000 45.750 46.125 3881600 3.940306 CVX -0.018617 1986-09-09 46.000 46.125 45.250 45.375 4120000 3.876236 CVX -0.016260 Nice. Also we need to get the market capitalization of the companies those stocks are chosen as instruments. The function below can be used not only to retrieve capitalization but also other interesting parameters of companies. In [8]: ''' Igor Marfin <Unister Gmb, 2014> igor.marfin@unister.de Get Capitalization ''' feature_descriptions = { 'a' : 'Ask' , 'a2' : 'Av. daily volume' , 'a5' : 'Ask size' , 'b' : 'Bid' , 'b2' : 'Ask (real-time)' , 'b3' : 'Bid (real-time)' , 'b4' : 'Book value' , 'b6' : 'Bid size' , 'c' : 'Change' , 'c1' : 'Change' , 'c3' : 'Commission' , 'c6' : 'Change (real-time)' , 'c8' : 'After hours change (real-time)' , 'd' : 'Dividend/share' , 'd1' : 'Last trade date' , 'd2' : 'Trade date' , 'e' : 'Earnings/share' , 'e1' : 'Error indication' , 'e7' : 'EPS est. current year' , 'e8' : 'EPS est. next year' , 'e9' : 'EPS est. next quarter' , 'f6' : 'Float shares' , 'g' : 'Day \\' s low' , 'h' : 'Day \\' s high' , 'j' : '52-week low' , 'k' : '52-week high' , 'g1' : 'Holdings gain' , 'g3' : 'Annualized gain' , 'g4' : 'Holdings gain' , 'g5' : 'Holdings gain (real-time)' , 'g6' : 'Holdings gain (real-time)' , 'i' : 'More info' , 'i5' : 'Order book (real-time)' , 'j1' : 'Market capitalization' , 'j3' : 'Market capitalization (real-time)' , 'j4' : 'EBITDA' , 'j5' : 'Change from 52-week low' , 'j6' : 'Change from 52-week low' , 'k1' : 'Last trade (real-time)' , 'k2' : 'Change (real-time)' , 'k3' : 'Last trade size' , 'k4' : 'Change from 52-week high' , 'k5' : 'Change from 52-week high' , 'l' : 'Last trade' , 'l1' : 'Last trade' , 'l2' : 'High limit' , 'l3' : 'Low limit' , 'm' : 'Day \\' s range' , 'm2' : 'Day \\' s range (real-time)' , 'm3' : '50-day moving average' , 'm4' : '200-day moving average' , 'm5' : 'Change from 200-day moving average' , 'm6' : 'Change from 200-day moving average' , 'm7' : 'Change from 50-day moving average' , 'm8' : 'Change from 50-day moving average' , 'n' : 'Name' , 'n4' : 'Notes' , 'o' : 'Open' , 'p' : 'Previous close' , 'p1' : 'Price paid' , 'p2' : 'Change' , 'p5' : 'Price/sales' , 'p6' : 'Price/book' , 'q' : 'Ex-dividend date' , 'r' : 'P/E ratio' , 'r1' : 'Dividend pay date' , 'r2' : 'P/E ratio (real-time)' , 'r5' : 'PEG ratio' , 'r6' : 'Price/EPS estimate current year' , 'r7' : 'Price/EPS estimate next year' , 's' : 'Symbol' , 's1' : 'Shares owned' , 's7' : 'Short ratio' , 't1' : 'Last trade' , 't6' : 'Trade links' , 't7' : 'Ticker trend' , 't8' : '52-week target estimate' , 'v' : 'Volume' , 'v1' : 'Holding \\' s value' , 'v7' : 'Holding \\' s value (real-time)' , 'w' : '52-week range' , 'w1' : 'Day \\' s value change' , 'w4' : 'Day \\' s value change (real-time)' , 'x' : 'Stock exchange' , 'y' : 'Dividend & yeild' } def getCapitalization ( stocks = instruments , features = [ \"j1\" , \"j3\" , \"r\" ], feature_descriptions = feature_descriptions ): '''returns caps''' _stocks = '+' . join ( stocks ) . lower () url = 'http://finance.yahoo.com/d/quotes.csv?s=' + _stocks + '+&f=' + '' . join ( features ) dataframe = pd . read_csv ( url , names = [ value for key , value in feature_descriptions . iteritems () if key in features ]) if ( \"j1\" in features ): dataframe [ feature_descriptions [ \"j1\" ]] = dataframe [ feature_descriptions [ \"j1\" ]] . apply ( lambda x : float ( x . replace ( 'B' , '' ))) dataframe [ 'Symbol' ] = map ( lambda i : stocks [ i ], dataframe . index . values ) return dataframe caps = getCapitalization () Before we go further and define the optimization procedure, we need to calculate the variance matrix of the 'Close' price to be used later: In [9]: ''' Igor Marfin <Unister Gmb, 2014> igor.marfin@unister.de Get (Annual) Covariance Matrix, Mean (Annual) Returns ''' def GetCovMatrix ( dataframe , stocks = instruments , feature = 'Close' ): '''creates the covariance matrix''' # add returns of the 'Close' to each dataframe of the stocks dataframe = map ( lambda i : addPctChange ( dataframe [ i ], feature ), [ i for i , _ in enumerate ( stocks )]) dataframe = map ( lambda item : item [ feature + \"_pct_change\" ] . as_matrix () . tolist () , dataframe ) dataframe = pd . DataFrame ( np . array ( dataframe ) . T ) #cov_matrix=dataframe.cov(min_periods=None).as_matrix() cov_matrix = dataframe . cov ( min_periods = None ) return cov_matrix def GetCorrMatrix ( dataframe , stocks = instruments , feature = 'Close' ): '''creates the correlation matrix''' # add returns of the 'Close' to each dataframe of the stocks dataframe = map ( lambda i : addPctChange ( dataframe [ i ], feature ), [ i for i , _ in enumerate ( stocks )]) dataframe = map ( lambda item : item [ feature + \"_pct_change\" ] . as_matrix () . tolist () , dataframe ) dataframe = pd . DataFrame ( np . array ( dataframe ) . T ) cov_matrix = dataframe . cov ( min_periods = None ) . as_matrix () V = np . diag ( cov_matrix ) . reshape ( 1 , - 1 ) return pd . DataFrame ( cov_matrix / np . sqrt ( V * V . T )) def GetMeanReturns ( dataframe , stocks = instruments , feature = 'Close' ): '''calculates the mean of the returns for feature''' # add returns of the 'Close' to each dataframe of the stocks columns = stocks dataframe = map ( lambda i : addPctChange ( dataframe [ i ], feature ), [ i for i , _ in enumerate ( stocks )]) dataframe = map ( lambda item : item [ feature + \"_pct_change\" ] . as_matrix () . tolist () , dataframe ) dataframe = pd . DataFrame ( np . array ( dataframe ) . T ) dataframe . columns = columns return dataframe . describe () . loc [ \"mean\" ,:] def GetAnnualExpectedReturns ( returns , period = 250 ): ''' returns the expected returns for the 'period' days''' return returns . apply ( lambda x : ( 1 + x ) ** period - 1 ) def GetAnnualCovMatrix ( dataframe , stocks = instruments , feature = 'Close' , period = 250 ): '''creates the covariance matrix for the 'period' of days ''' # add returns of the 'Close' to each dataframe of the stocks dataframe = map ( lambda i : addPctChange ( dataframe [ i ], feature ), [ i for i , _ in enumerate ( stocks )]) dataframe = map ( lambda item : item [ feature + \"_pct_change\" ] . as_matrix () . tolist () , dataframe ) dataframe = pd . DataFrame ( np . array ( dataframe ) . T ) cov_matrix = dataframe . cov ( min_periods = None ) . as_matrix () return cov_matrix * period ''' the function, which returns from historical dataframe * Name of the instrument * Weight calculated from capitalization * Annual Expected Return * Annual Covariance Matrix Name,Weight, Annual Expected Return are combined in one DataFrame ''' def ProcessHistoricaData ( dataframe , caps , stocks = instruments , feature = 'Close' , period = 250 ): ''' returns processed information from the historical data''' annual_cov_matrix = GetAnnualCovMatrix ( dataframe , stocks , feature , period ) returns = GetMeanReturns ( dataframe , stocks , feature ) annual_expect_returns = GetAnnualExpectedReturns ( returns , period ) total_cap = caps [ \"Market capitalization\" ] . sum () result_dataframe = caps . copy ( True ) result_dataframe = result_dataframe . drop ( \"Market capitalization\" , 1 ) . drop ( \"Market capitalization (real-time)\" , 1 ) #result_dataframe=result_dataframe.drop(\"Market capitalization (real-time)\",1) result_dataframe [ \"Weight\" ] = caps [ \"Market capitalization\" ] . apply ( lambda x : x / total_cap ) result_dataframe [ \"AnnualExpectReturn\" ] = annual_expect_returns . values return result_dataframe , pd . DataFrame ( annual_cov_matrix ) result_dataframe , cov_matrix = ProcessHistoricaData ( data , caps ) result_dataframe Out[9]: P/E ratio Symbol Weight AnnualExpectReturn 0 14.461 XOM 0.133227 0.055751 1 12.920 AAPL 0.250464 0.223660 2 31.610 MSFT 0.147102 0.135760 3 18.610 JNJ 0.105595 0.064417 4 NaN GE 0.111426 0.040283 5 13.950 CVX 0.066873 0.071327 6 30.430 PG 0.079175 0.055048 7 12.800 WFC 0.106138 0.107425 I'd like to say that I use the post published at http://www.quantandfinancial.com/2013/07/mean-variance-portfolio-optimization.html as a reference to check that my calculations are correct. So now I want to compare obtained results with ones presented in the blog. In [10]: '''function to print the obtained processed data I have taken it from http://code.google.com/p/quantandfinancial/source/browse/trunk/example_black_litterman.py to compare the result with http://www.quantandfinancial.com/2013/07/mean-variance-portfolio-optimization.html ''' def print_assets ( names , W , R , C ): print ( \" %-10s %6s %6s %6s \" % ( \"Name\" , \"Weight\" , \"Return\" , \"Dev\" )) for i in range ( len ( names )): print ( \" %-10s %5.1f%% %5.1f%% %5.1f%% \" % ( names [ i ], 100 * W [ i ], 100 * R [ i ], 100 * C [ i , i ] **. 5 )) print_assets ( result_dataframe [ \"Symbol\" ] . values , result_dataframe [ \"Weight\" ] . values , result_dataframe [ \"AnnualExpectReturn\" ] . values , cov_matrix . as_matrix () ) Name Weight Return Dev XOM 13.3% 5.6% 28.9% AAPL 25.0% 22.4% 51.1% MSFT 14.7% 13.6% 43.2% JNJ 10.6% 6.4% 29.3% GE 11.1% 4.0% 34.7% CVX 6.7% 7.1% 28.4% PG 7.9% 5.5% 30.2% WFC 10.6% 10.7% 41.0% $$\\\\[5pt]$$ 0.4.3 Trade-off between Mean-variance and Returns Mean returns are quantitatively measured by geometrically averaging series of daily or weekly returns over a certain period of time, while the uncertainty is expressed by the variance or standard deviation of such series. Naturally more volatile the asset is, higher is also the expected return. This relationship is determined by the supply and demand forces on a capital market and may be mathematically expressed by one of the capital pricing models Most of these pricing models are based on an assumption that only systematic risk is reflected in capital prices and therefore investors shouldn't expect additional risk premium by holding poorly diversified or standalone investments. Assuming normal distribution of asset returns, we can quantitatively measure this diversification benefit by calculating correlation between two price series, which is equal or less than one. Imagine, that we have the portfolio of the portfolio='XOM','CVX' stocks. In case there is no diversification benefit,i.e the two assets doesn't correlate, the risk(asset deviation)-return curve is a simple line connected to points on the space: $ (r_{XOM},\\sigma_{XOM}) $ and $ (r_{CVX},\\sigma_{CVX}). $ Having the portfolio of 50% for XOM' and 50% for 'CVX' stocks, we can annually expect to get 6% of the profit (returns) with the risk (deviations) of 8.2%. However, if the correlation between stocks,which is about 56%, is taken into account, the same portfolio will give us 6% of the profit (returns) and the risk (deviations) of 6%! It is shown on the plot below. In [11]: '''function to plot the same triangle plot as it is shown in the post http://3.bp.blogspot.com/-Yhb7BabPFY8/UeWpZSV8pHI/AAAAAAAABcQ/8cwnKXcckdA/s1600/meanvariance.png ''' def plot_mean_variance_returns ( result_dataframe , cov_matrix , portfolio = [ 'XOM' , 'CVX' ]): assert len ( portfolio ) == 2 , \"portfolio should have only 2 instruments\" returns = result_dataframe . loc [ result_dataframe [ \"Symbol\" ] . isin ( portfolio )][ 'AnnualExpectReturn' ] . values indexes = result_dataframe . loc [ result_dataframe [ \"Symbol\" ] . isin ( portfolio )] . index . values fractions = np . linspace ( 0. , 1. , 75 ) total_std_deviations_no_corr = map ( lambda x : x * cov_matrix . loc [ indexes [ 0 ], indexes [ 0 ]] + ( 1. - x ) * cov_matrix . loc [ indexes [ 1 ], indexes [ 1 ]], fractions ) total_std_deviations_with_corr = map ( lambda x : x * cov_matrix . loc [ indexes [ 0 ], indexes [ 0 ]] + ( 1. - x ) * cov_matrix . loc [ indexes [ 1 ], indexes [ 1 ]] - 2 * x * ( 1. - x ) * cov_matrix . loc [ indexes [ 0 ], indexes [ 1 ]], fractions ) total_returns = map ( lambda x : x * returns [ 0 ] + ( 1. - x ) * returns [ 1 ], fractions ) plt . plot ( total_std_deviations_no_corr , total_returns , label = \"w/o correlation\" , lw = 3 ) plt . plot ( total_std_deviations_with_corr , total_returns , label = \"w/ correlation\" , lw = 3 ) plt . xlabel ( \"Std Deviation\" ) plt . ylabel ( \"Returns\" ) plt . legend () plt . title ( \"Trade-off between Risk and Returs for %r \" % portfolio ); plot_mean_variance_returns ( result_dataframe , cov_matrix ) This plot brings the idea on a robust way to optimize the portfolio if it consists from two assets: Choose such pair of assets which has the minimal risk. All correlations between assets should be taken into account. We call such method the Portfolio Mean-Variance Optimization with the minimum variance frontier ,i.e PMO with MinimumVarianceFrontier . $$\\\\[5pt]$$ 0.5 Portfolio mean-variance calculation and optimization However we could extend our robust proposal by few things. First, we want to replace the merit of optimization: instead of minimizing the variance of the portfolio $\\sigma_P$, we will test some kind of mean-variance utility function, such as a Sharpe ratio: $$ s=\\frac{r_P−r_f}{\\sigma_P} $$ Here $r_P$ and $r_f$ are expected returns of the portfolio and risk-free rate . Second, we will be not limited by the portfolio of two assets, i.e we are going to use all possible combinations between different instruments. Third and last, we need to define $r_P$, $\\sigma_P$ and $r_f$. The return $r_P$ of the portfolio is determined by the formula $$ r_P = W_i R_i, $$ where $W_i$ is the vector of weights and $R_i$ is the vector of annual returns for chosen instruments. The variance of the portfolio is defined as follows: $$ C_P&#94;{2} = W_iC_{ij}&#94;2W_j, $$ where $ C_{ij}&#94;2 $ is the covariance matrix of our assets. This is so-called PMO with the MinimumSharpeRatioFrontier In [12]: ''' Igor Marfin <Unister Gmb, 2014> igor.marfin@unister.de Calculation of the portfolio variance and return ''' def portfolioReturn ( W , R ): ''' R_i*W_i ''' return np . sum ( R * W ) def portfolioVariance ( W , C ): ''' W_i*C_ij*W_j ''' return np . dot ( np . dot ( C , W ), W ) print \"The return of the portfolio consisted of all instruments \\ and weights based on the companies capitalization is %3.2f%% \\n\\n \" % ( 100. * portfolioReturn ( result_dataframe [ \"Weight\" ] . values , result_dataframe [ \"AnnualExpectReturn\" ] . values )) print \"The variance of the portfolio consisted of all instruments \\ and weights based on the companies capitalization is %3.2f%% \" % ( 100. * portfolioVariance ( result_dataframe [ \"Weight\" ] . values , cov_matrix . as_matrix ())) The return of the portfolio consisted of all instruments and weights based on the companies capitalization is 11.52% The variance of the portfolio consisted of all instruments and weights based on the companies capitalization is 5.57% In [13]: ''' Igor Marfin <Unister Gmb, 2014> igor.marfin@unister.de Portfolio mean-variance calculation and optimization with MinimumSharpeRatioFrontier ''' import scipy # Given risk-free rate, assets returns and covariances, this # function calculates weights of tangency portfolio with respect to # sharpe ratio maximization def MinimumSharpeRatioFrontier ( R , C , rf ): def fitness ( W , R , C , rf ): mean , var = portfolioReturn ( W , R ), portfolioVariance ( W , C ) # calculate mean/variance of the portfolio util = ( mean - rf ) / sqrt ( var ) # utility = Sharpe ratio return 1 / util # maximize the utility, minimize its inverse value n = len ( R ) W = ones ([ n ]) / n # start optimization with equal weights b_ = [( 0. , 1. ) for i in range ( n )] # weights for boundaries between 0%..100%. No leverage, no shorting c_ = ({ 'type' : 'eq' , 'fun' : lambda W : sum ( W ) - 1. }) # Sum of weights must be 100% optimized = scipy . optimize . minimize ( fitness , W , ( R , C , rf ), method = 'SLSQP' , constraints = c_ , bounds = b_ ) if not optimized . success : raise BaseException ( optimized . message ) return optimized . x # rf taken from # http://people.stern.nyu.edu/adamodar/podcasts/cfspr15/cfsession6.pdf # rf is 0.02 (only 2%) for US$. Weights_optimal = MinimumSharpeRatioFrontier ( result_dataframe [ \"AnnualExpectReturn\" ] . values , cov_matrix . as_matrix (), 0.02 ) print \"New portfolio after optimization: \\n\\n \" print_assets ( result_dataframe [ \"Symbol\" ] . values , Weights_optimal , result_dataframe [ \"AnnualExpectReturn\" ] . values , cov_matrix . as_matrix () ) print \" \\n\\n \" print \"The return of the portfolio consisted of all instruments \\ and optimal weights is %3.2f%% \\n\\n \" % ( 100. * portfolioReturn ( Weights_optimal , result_dataframe [ \"AnnualExpectReturn\" ] . values )) print \"The variance of the portfolio consisted of all instruments \\ and optimal weights is %3.2f%% \" % ( 100. * portfolioVariance ( Weights_optimal , cov_matrix . as_matrix ())) New portfolio after optimization: Name Weight Return Dev XOM 0.0% 5.6% 28.9% AAPL 42.0% 22.4% 51.1% MSFT 20.4% 13.6% 43.2% JNJ 9.6% 6.4% 29.3% GE -0.0% 4.0% 34.7% CVX 13.2% 7.1% 28.4% PG 0.0% 5.5% 30.2% WFC 14.8% 10.7% 41.0% The return of the portfolio consisted of all instruments and optimal weights is 15.31% The variance of the portfolio consisted of all instruments and optimal weights is 8.71% The functions below plots the contributions of different portfolio constituent pairs and optimized portfolio parameters together: In [14]: def plot_mean_variance_returns ( result_dataframe , cov_matrix , portfolio = [ 'XOM' , 'CVX' ]): assert len ( portfolio ) == 2 , \"portfolio should have only 2 instruments\" returns = result_dataframe . loc [ result_dataframe [ \"Symbol\" ] . isin ( portfolio )][ 'AnnualExpectReturn' ] . values indexes = result_dataframe . loc [ result_dataframe [ \"Symbol\" ] . isin ( portfolio )] . index . values fractions = np . linspace ( 0. , 1. , 75 ) total_std_deviations_no_corr = map ( lambda x : x * cov_matrix . loc [ indexes [ 0 ], indexes [ 0 ]] + ( 1. - x ) * cov_matrix . loc [ indexes [ 1 ], indexes [ 1 ]], fractions ) total_std_deviations_with_corr = map ( lambda x : x * cov_matrix . loc [ indexes [ 0 ], indexes [ 0 ]] + ( 1. - x ) * cov_matrix . loc [ indexes [ 1 ], indexes [ 1 ]] - 2 * x * ( 1. - x ) * cov_matrix . loc [ indexes [ 0 ], indexes [ 1 ]], fractions ) total_returns = map ( lambda x : x * returns [ 0 ] + ( 1. - x ) * returns [ 1 ], fractions ) plt . plot ( total_std_deviations_with_corr , total_returns , label = \" %r \" % portfolio , lw=3) plt . xlabel ( \"Std Deviation\" ) plt . ylabel ( \"Returns\" ) plt . legend () plt . title ( \"Trade-off between Risk and Returs\" ); pairs = [] for i , inst1 in enumerate ( instruments ): for j in range ( i + 1 , len ( instruments )): if ( i < len ( instruments ) - 1 ): pairs += [[ inst1 , instruments [ j ]]] for pair in pairs : plot_mean_variance_returns ( result_dataframe , cov_matrix , portfolio = pair ) pl . scatter ( portfolioVariance ( Weights_optimal , cov_matrix . as_matrix ()), portfolioReturn ( Weights_optimal , result_dataframe [ \"AnnualExpectReturn\" ] . values ), color = \"black\" , s = 42 , label = 'optimized value' ) Out[14]: <matplotlib.collections.PathCollection at 0xc6d458c> OK. It looks optimistic. Would we like to try MinimumVarianceFrontier ? Yes. We implement the MinimumVarianceFrontier with one small improvement: the previous case has considered the utility function, which was the Sharpe ratio having two degrees of freedom - the mean and the variance. This might give us the problem of underestimating the risks. Thus it would be better to find the minimal variance of the portfolio at some levels of investor's risk aversion, represented by required return. So we define these levels (20 equidistant levels) and iterate through them using the optimization algorithm in order to minimize the portfolio variance at each considered level. As a result, we can avoid the use of the risk-free rate in this calculation: the different levels of the return are playing this role. In [15]: ''' Igor Marfin <Unister Gmb, 2014> igor.marfin@unister.de Portfolio mean-variance calculation and optimization with MinimumVarianceFrontier ''' import scipy # Given risk-free rate, assets returns and covariances, this # function calculates weights of tangency portfolio with respect to # variance minimization def MinimumVarianceFrontier ( R , C , num_levels = 20 ): def fitness ( W , R , C , r , penalty_factor = 50. ): # For given level of return r, find weights which minimizes # portfolio variance. mean , var = portfolioReturn ( W , R ), portfolioVariance ( W , C ) # Big penalty for not meeting stated portfolio return effectively serves as optimization constraint penalty = penalty_factor * abs ( mean - r ) return var + penalty frontier_level , frontier_return , frontier_var , frontier_weights = [], [], [], [] n = len ( R ) for r in linspace ( min ( R ), max ( R ), num = num_levels ): # Iterate through the range of returns W = ones ([ n ]) / n # start optimization with equal weights b_ = [( 0. , 1. ) for i in range ( n )] # weights for boundaries between 0%..100%. No leverage, no shorting c_ = ({ 'type' : 'eq' , 'fun' : lambda W : sum ( W ) - 1. }) # Sum of weights must be 100% optimized = scipy . optimize . minimize ( fitness , W , ( R , C , r ), method = 'SLSQP' , constraints = c_ , bounds = b_ ) if not optimized . success : raise BaseException ( optimized . message ) # add point to the min-var frontier [x,y] = [optimized.x, r] frontier_level += [ r ] frontier_return += [ portfolioReturn ( optimized . x , R )] frontier_var += [ portfolioVariance ( optimized . x , C )] frontier_weights += [ optimized . x ] return np . array ( frontier_level ), np . array ( frontier_return ), np . array ( frontier_var ), np . array ( frontier_weights ) frontier_level , frontier_return , frontier_var , frontier_weights = MinimumVarianceFrontier ( result_dataframe [ \"AnnualExpectReturn\" ] . values , cov_matrix . as_matrix ()) for i , weight in enumerate ( frontier_weights ): print \" \\n\\n New portfolio at the level %3.2f%% after optimization: \\n\\n \" % ( 100. * frontier_level [ i ]) print_assets ( result_dataframe [ \"Symbol\" ] . values , weight , result_dataframe [ \"AnnualExpectReturn\" ] . values , cov_matrix . as_matrix () ) print \" \\n\\n \" print \"The return of the portfolio consisted of all instruments and optimal weights is %3.2f%% \\n\\n \" % ( 100. * frontier_return [ i ]) print \"The variance of the portfolio consisted of all instruments and optimal weights is %3.2f%% \" % ( 100. * frontier_var [ i ]) New portfolio at the level 4.03% after optimization: Name Weight Return Dev XOM 0.0% 5.6% 28.9% AAPL -0.0% 22.4% 51.1% MSFT -0.0% 13.6% 43.2% JNJ -0.0% 6.4% 29.3% GE 100.0% 4.0% 34.7% CVX -0.0% 7.1% 28.4% PG 0.0% 5.5% 30.2% WFC -0.0% 10.7% 41.0% The return of the portfolio consisted of all instruments and optimal weights is 4.03% The variance of the portfolio consisted of all instruments and optimal weights is 12.07% New portfolio at the level 4.99% after optimization: Name Weight Return Dev XOM 26.3% 5.6% 28.9% AAPL -0.0% 22.4% 51.1% MSFT 0.0% 13.6% 43.2% JNJ 6.4% 6.4% 29.3% GE 40.0% 4.0% 34.7% CVX 0.0% 7.1% 28.4% PG 27.2% 5.5% 30.2% WFC 0.0% 10.7% 41.0% The return of the portfolio consisted of all instruments and optimal weights is 4.99% The variance of the portfolio consisted of all instruments and optimal weights is 5.09% New portfolio at the level 5.96% after optimization: Name Weight Return Dev XOM 18.3% 5.6% 28.9% AAPL 0.0% 22.4% 51.1% MSFT 0.7% 13.6% 43.2% JNJ 23.9% 6.4% 29.3% GE 14.0% 4.0% 34.7% CVX 18.8% 7.1% 28.4% PG 23.2% 5.5% 30.2% WFC 1.2% 10.7% 41.0% The return of the portfolio consisted of all instruments and optimal weights is 5.96% The variance of the portfolio consisted of all instruments and optimal weights is 3.93% New portfolio at the level 6.92% after optimization: Name Weight Return Dev XOM 16.6% 5.6% 28.9% AAPL 3.4% 22.4% 51.1% MSFT 3.3% 13.6% 43.2% JNJ 23.5% 6.4% 29.3% GE 9.7% 4.0% 34.7% CVX 18.9% 7.1% 28.4% PG 21.0% 5.5% 30.2% WFC 3.5% 10.7% 41.0% The return of the portfolio consisted of all instruments and optimal weights is 6.92% The variance of the portfolio consisted of all instruments and optimal weights is 3.83% New portfolio at the level 7.89% after optimization: Name Weight Return Dev XOM 13.5% 5.6% 28.9% AAPL 7.1% 22.4% 51.1% MSFT 5.5% 13.6% 43.2% JNJ 22.9% 6.4% 29.3% GE 6.0% 4.0% 34.7% CVX 19.9% 7.1% 28.4% PG 19.7% 5.5% 30.2% WFC 5.4% 10.7% 41.0% The return of the portfolio consisted of all instruments and optimal weights is 7.89% The variance of the portfolio consisted of all instruments and optimal weights is 3.84% New portfolio at the level 8.85% after optimization: Name Weight Return Dev XOM 11.3% 5.6% 28.9% AAPL 11.0% 22.4% 51.1% MSFT 7.4% 13.6% 43.2% JNJ 22.2% 6.4% 29.3% GE 2.4% 4.0% 34.7% CVX 20.5% 7.1% 28.4% PG 18.0% 5.5% 30.2% WFC 7.2% 10.7% 41.0% The return of the portfolio consisted of all instruments and optimal weights is 8.85% The variance of the portfolio consisted of all instruments and optimal weights is 3.98% New portfolio at the level 9.82% after optimization: Name Weight Return Dev XOM 8.9% 5.6% 28.9% AAPL 15.2% 22.4% 51.1% MSFT 9.3% 13.6% 43.2% JNJ 21.5% 6.4% 29.3% GE -0.0% 4.0% 34.7% CVX 20.4% 7.1% 28.4% PG 15.8% 5.5% 30.2% WFC 8.9% 10.7% 41.0% The return of the portfolio consisted of all instruments and optimal weights is 9.82% The variance of the portfolio consisted of all instruments and optimal weights is 4.25% New portfolio at the level 10.78% after optimization: Name Weight Return Dev XOM 5.8% 5.6% 28.9% AAPL 19.8% 22.4% 51.1% MSFT 11.2% 13.6% 43.2% JNJ 20.1% 6.4% 29.3% GE 0.0% 4.0% 34.7% CVX 20.2% 7.1% 28.4% PG 13.1% 5.5% 30.2% WFC 9.8% 10.7% 41.0% The return of the portfolio consisted of all instruments and optimal weights is 10.78% The variance of the portfolio consisted of all instruments and optimal weights is 4.66% New portfolio at the level 11.75% after optimization: Name Weight Return Dev XOM 2.2% 5.6% 28.9% AAPL 24.2% 22.4% 51.1% MSFT 13.3% 13.6% 43.2% JNJ 18.2% 6.4% 29.3% GE 0.0% 4.0% 34.7% CVX 20.6% 7.1% 28.4% PG 10.4% 5.5% 30.2% WFC 11.2% 10.7% 41.0% The return of the portfolio consisted of all instruments and optimal weights is 11.75% The variance of the portfolio consisted of all instruments and optimal weights is 5.22% New portfolio at the level 12.71% after optimization: Name Weight Return Dev XOM -0.0% 5.6% 28.9% AAPL 28.4% 22.4% 51.1% MSFT 16.0% 13.6% 43.2% JNJ 17.3% 6.4% 29.3% GE 0.0% 4.0% 34.7% CVX 18.6% 7.1% 28.4% PG 7.1% 5.5% 30.2% WFC 12.7% 10.7% 41.0% The return of the portfolio consisted of all instruments and optimal weights is 12.71% The variance of the portfolio consisted of all instruments and optimal weights is 5.95% New portfolio at the level 13.68% after optimization: Name Weight Return Dev XOM 0.0% 5.6% 28.9% AAPL 33.6% 22.4% 51.1% MSFT 17.3% 13.6% 43.2% JNJ 14.3% 6.4% 29.3% GE -0.0% 4.0% 34.7% CVX 17.4% 7.1% 28.4% PG 4.1% 5.5% 30.2% WFC 13.3% 10.7% 41.0% The return of the portfolio consisted of all instruments and optimal weights is 13.68% The variance of the portfolio consisted of all instruments and optimal weights is 6.83% New portfolio at the level 14.64% after optimization: Name Weight Return Dev XOM 0.0% 5.6% 28.9% AAPL 38.3% 22.4% 51.1% MSFT 19.4% 13.6% 43.2% JNJ 12.0% 6.4% 29.3% GE -0.0% 4.0% 34.7% CVX 15.4% 7.1% 28.4% PG 0.6% 5.5% 30.2% WFC 14.3% 10.7% 41.0% The return of the portfolio consisted of all instruments and optimal weights is 14.64% The variance of the portfolio consisted of all instruments and optimal weights is 7.88% New portfolio at the level 15.61% after optimization: Name Weight Return Dev XOM 0.0% 5.6% 28.9% AAPL 43.5% 22.4% 51.1% MSFT 21.0% 13.6% 43.2% JNJ 8.6% 6.4% 29.3% GE -0.0% 4.0% 34.7% CVX 11.6% 7.1% 28.4% PG 0.0% 5.5% 30.2% WFC 15.2% 10.7% 41.0% The return of the portfolio consisted of all instruments and optimal weights is 15.61% The variance of the portfolio consisted of all instruments and optimal weights is 9.11% New portfolio at the level 16.58% after optimization: Name Weight Return Dev XOM 0.0% 5.6% 28.9% AAPL 48.8% 22.4% 51.1% MSFT 22.8% 13.6% 43.2% JNJ 4.3% 6.4% 29.3% GE 0.0% 4.0% 34.7% CVX 8.3% 7.1% 28.4% PG 0.0% 5.5% 30.2% WFC 15.8% 10.7% 41.0% The return of the portfolio consisted of all instruments and optimal weights is 16.58% The variance of the portfolio consisted of all instruments and optimal weights is 10.53% New portfolio at the level 17.54% after optimization: Name Weight Return Dev XOM -0.0% 5.6% 28.9% AAPL 54.0% 22.4% 51.1% MSFT 24.8% 13.6% 43.2% JNJ 0.6% 6.4% 29.3% GE -0.0% 4.0% 34.7% CVX 4.3% 7.1% 28.4% PG 0.0% 5.5% 30.2% WFC 16.3% 10.7% 41.0% The return of the portfolio consisted of all instruments and optimal weights is 17.54% The variance of the portfolio consisted of all instruments and optimal weights is 12.14% New portfolio at the level 18.51% after optimization: Name Weight Return Dev XOM -0.0% 5.6% 28.9% AAPL 60.8% 22.4% 51.1% MSFT 24.7% 13.6% 43.2% JNJ 0.0% 6.4% 29.3% GE -0.0% 4.0% 34.7% CVX 0.0% 7.1% 28.4% PG 0.0% 5.5% 30.2% WFC 14.6% 10.7% 41.0% The return of the portfolio consisted of all instruments and optimal weights is 18.51% The variance of the portfolio consisted of all instruments and optimal weights is 13.96% New portfolio at the level 19.47% after optimization: Name Weight Return Dev XOM 0.0% 5.6% 28.9% AAPL 69.6% 22.4% 51.1% MSFT 22.4% 13.6% 43.2% JNJ 0.0% 6.4% 29.3% GE -0.0% 4.0% 34.7% CVX 0.0% 7.1% 28.4% PG 0.0% 5.5% 30.2% WFC 8.0% 10.7% 41.0% The return of the portfolio consisted of all instruments and optimal weights is 19.47% The variance of the portfolio consisted of all instruments and optimal weights is 16.18% New portfolio at the level 20.44% after optimization: Name Weight Return Dev XOM -0.0% 5.6% 28.9% AAPL 78.5% 22.4% 51.1% MSFT 20.0% 13.6% 43.2% JNJ 0.0% 6.4% 29.3% GE -0.0% 4.0% 34.7% CVX -0.0% 7.1% 28.4% PG 0.0% 5.5% 30.2% WFC 1.5% 10.7% 41.0% The return of the portfolio consisted of all instruments and optimal weights is 20.44% The variance of the portfolio consisted of all instruments and optimal weights is 18.86% New portfolio at the level 21.40% after optimization: Name Weight Return Dev XOM 0.0% 5.6% 28.9% AAPL 89.0% 22.4% 51.1% MSFT 11.0% 13.6% 43.2% JNJ 0.0% 6.4% 29.3% GE -0.0% 4.0% 34.7% CVX -0.0% 7.1% 28.4% PG 0.0% 5.5% 30.2% WFC 0.0% 10.7% 41.0% The return of the portfolio consisted of all instruments and optimal weights is 21.40% The variance of the portfolio consisted of all instruments and optimal weights is 22.09% New portfolio at the level 22.37% after optimization: Name Weight Return Dev XOM 0.0% 5.6% 28.9% AAPL 100.0% 22.4% 51.1% MSFT 0.0% 13.6% 43.2% JNJ 0.0% 6.4% 29.3% GE -0.0% 4.0% 34.7% CVX 0.0% 7.1% 28.4% PG 0.0% 5.5% 30.2% WFC 0.0% 10.7% 41.0% The return of the portfolio consisted of all instruments and optimal weights is 22.37% The variance of the portfolio consisted of all instruments and optimal weights is 26.09% The plot below illustrates the behavior of the portfolio risk at the different levels of the profit (aversion). The optimized portfolio shows that, first, the risk is decreased with increase of the profit such that someone can achieve 3 times large return's volume for the optimized portfolio at the unchanged risk level which is below 12%! When the highest values of the profit is reached, the portfolio risk is also high and it consist of 25%. Also what can be interesting for us is the fact that the point obtained by MinimumSharpeRatioFrontier lies on the curve given by MinimumVarianceFrontier , i.e the MinimumVarianceFrontier provides the general solutions to the problem of the portfolio optimization. In [16]: '''function to plot the optimzed portfolio risks vs aversion levels ''' def plot_optimized_portfolio_at_levels ( frontier_level , frontier_return , frontier_var , frontier_weights ): plt . plot ( frontier_var , frontier_return , label = \"optimized by MinimumVarianceFrontier\" , linestyle = '--' , color = \"b\" , lw = 10 ) plt . xlabel ( \"Std Deviation\" ) plt . ylabel ( \"Returns\" ) plt . legend () plt . title ( \"Trade-off between Risk and Returs for the portfolio optimized by MinimumVarianceFrontier\" ); pl . scatter ( portfolioVariance ( Weights_optimal , cov_matrix . as_matrix ()), portfolioReturn ( Weights_optimal , result_dataframe [ \"AnnualExpectReturn\" ] . values ), color = \"red\" , s = 92 , label = 'optimized by MinimumSharpeRatioFrontier' ) plot_optimized_portfolio_at_levels ( frontier_level , frontier_return , frontier_var , frontier_weights ) Another plot demonstrates performance,i.e aversion levels and variances, of the portfolio optimized by MinimumVarianceFrosntier and different return-variance curves for pairs of the consistuents of the portfolio. In [17]: def plot_mean_variance_returns ( result_dataframe , cov_matrix , portfolio = [ 'XOM' , 'CVX' ]): assert len ( portfolio ) == 2 , \"portfolio should have only 2 instruments\" returns = result_dataframe . loc [ result_dataframe [ \"Symbol\" ] . isin ( portfolio )][ 'AnnualExpectReturn' ] . values indexes = result_dataframe . loc [ result_dataframe [ \"Symbol\" ] . isin ( portfolio )] . index . values fractions = np . linspace ( 0. , 1. , 75 ) total_std_deviations_no_corr = map ( lambda x : x * cov_matrix . loc [ indexes [ 0 ], indexes [ 0 ]] + ( 1. - x ) * cov_matrix . loc [ indexes [ 1 ], indexes [ 1 ]], fractions ) total_std_deviations_with_corr = map ( lambda x : x * cov_matrix . loc [ indexes [ 0 ], indexes [ 0 ]] + ( 1. - x ) * cov_matrix . loc [ indexes [ 1 ], indexes [ 1 ]] - 2 * x * ( 1. - x ) * cov_matrix . loc [ indexes [ 0 ], indexes [ 1 ]], fractions ) total_returns = map ( lambda x : x * returns [ 0 ] + ( 1. - x ) * returns [ 1 ], fractions ) plt . plot ( total_std_deviations_with_corr , total_returns , label = \" %r \" % portfolio , lw=3) plt . xlabel ( \"Std Deviation\" ) plt . ylabel ( \"Returns\" ) plt . legend () plt . title ( \"Trade-off between Risk and Returs\" ); pairs = [] for i , inst1 in enumerate ( instruments ): for j in range ( i + 1 , len ( instruments )): if ( i < len ( instruments ) - 1 ): pairs += [[ inst1 , instruments [ j ]]] for pair in pairs : plot_mean_variance_returns ( result_dataframe , cov_matrix , portfolio = pair ) plot_optimized_portfolio_at_levels ( frontier_level , frontier_return , frontier_var , frontier_weights ) $$\\\\[5pt]$$ 0.6 Black-Litterman model The application of the Black-Litterman model is known as the technique of reverse optimization. This technique is aimed to improve results obtained from the Mean-Variance Optimization Methods discussed above. One can ask the naive question: What is wrong with the Mean-Variance Optimization ? Well, the empirical studies show that expected asset returns have a stochastic nature. This explains the vast majority of optimal portfolio weightings and the fact that extrapolation of past returns into the future doesn't work well. That is why the achievement of optimal and stable asset allocation would be a primary goal, rather than trying to predict future market returns. I would like to address you to this post (OndrejMartinsky, 2013) used by me as a sketch in this study. I will follow it to develop the machinery of the Black-Litterman model . Here are the main points related to the application of the Black-Litterman model in optimization of the portfolio. They explain the reason of this approach, and they are consisted in the following: The world of efficient markets To maximize investor's utility function, Modern Portfolio Theory suggests holding the combination of a risk-free asset and optimal risky portfolio (ORP) lying on a tangency of the efficient frontier and capital market line. Modern Portfolio Theory also suggests that optimal risky portfolio is a market portfolio (e.g. capitalization-weighted index). If we assume markets are fully efficient and all assets are fairly priced, we don't have any reason to deviate from the market portfolio in our asset allocation. In such case, we don't even need to know equilibrium asset returns nor perform any kind of portfolio optimization, as outlined in the previous article. An optimization based on equilibrium asset returns would lead back to the same market portfolio anyway. So, these statements claim that if we believe to the market prices, we have to use the capitalization-based weights in our portfolio. That's it, and no optimization is needed anymore! An active investor's view The different situation is when investor believes the market as a whole is efficient, but has concerns about the performance of specific assets or asset classes due to the possession of material non-public information, resulting for example from superior fundamental analysis. This case covers the problem of the non-public information. If such information is available, why not to use it in the portfolio building? And this is exactly what the Black-Litterman model cares about. Here is the schematic view of all processes incorporated in the Black-Litterman model The model consists from two parts: the forward-optimization part; the reverse optimization part. The forward-optimization part in this model is the same as in the classic MVO process (boxes $b, g, i, j, k, l$). However, the thing which differs is a way how we are observing expected asset returns, which is one of the inputs into the forward optimizer ($g$). Previously we have used historical asset returns as a proxy. Now, as we can see in the diagram, equilibrium excess asset returns are used instead ($d$ or $g$). Equilibrium asset returns ($d$) are returns implied from the market capitalization weights of individual assets or asset classes ($a$) and historical asset covariances ($b$) in a process known as reverse optimization ($c$). Forward ($h$) and reverse ($c$) optimizations are mutually inverse functions! Therefore, forward-optimizing equilibrium asset returns ($d$) would yield to the optimal risky portfolio ($j$) with exactly the same weights as the weights observed from assets' capitalization ($a$). However, the beauty of the Black-Litterman model comes with the ability to adjust equilibrium market returns ($f$) by incorporating views into it and therefore to get optimal risky portfolio ($j$) reflecting those views. This ORP portfolio may be therefore different to the initial market cap weights ($a$). $$\\\\[5pt]$$ 0.6.1 Reverse optimization of equilibrium returns The novel part of this approach is the reverse optimization and equilibrium returns. First, let's define the equilibrium excess returns . Equilibrium excess market returns are those implied by the capitalization weights of market constituents. They represent more reasonable estimates than those derived from historical performance. This means that the equilibrium excess returns is a function of the capitalizations. Also, please, pay attention on the words \"equilibrium excess\" . This equilibrium excess says that we will use the \"efficient\" returns $\\hat R$, constructed as, $$\\hat R \\sim Equilibrium\\_excess\\_returns + r_f $$ as objects of further optimizations, i.e the $R$ should and will be replaced by $\\hat R$ everywhere in optimization algorithms. Equilibrium excess returns $\\pi_P$ are derived in Black-Litterman using this formula: $$\\lambda_P=\\frac{r_P−r_f}{\\sigma_P&#94;2},\\\\$$$$ r_P = W_iR_i, \\\\$$$$\\pi_{P,i} = \\lambda_{P}C_{ij}W_j,$$ where $W_i$,$R_i$, $r_R$ and $C_{ij}$ have already been introduced in the discussion made for Mean-Variance Optimization. The new quantity here is only the the equilibrium returns $\\pi_{P,i}$. In [18]: ''' Igor Marfin <Unister Gmb, 2014> igor.marfin@unister.de Calculation of the equilibrium excess returns ''' def EquilibriumExcessReturn ( MeanP , VarP , Rf , W , C ): ''' pi_i = \\lambda_{P}C_{ij}W_j ''' lambdaP = ( MeanP - Rf ) / VarP return np . dot ( np . dot ( lambdaP , C ), W ) def print_assets_2 ( names , W , R , Pi , C ): print ( \" %-10s %6s %6s %6s %6s \" % ( \"Name\" , \"Weight\" , \"Return\" , \"Pi\" , \"Dev\" )) for i in range ( len ( names )): print ( \" %-10s %5.1f%% %5.1f%% %5.1f%% %5.1f%% \" % ( names [ i ], 100 * W [ i ], 100 * R [ i ], 100 * Pi [ i ] , 100 * C [ i , i ] **. 5 )) # calculation of the equilibrium excess returns MeanP = portfolioReturn ( result_dataframe [ \"Weight\" ] . values , result_dataframe [ \"AnnualExpectReturn\" ] . values ) VarP = portfolioVariance ( result_dataframe [ \"Weight\" ] . values , cov_matrix . as_matrix ()) equil_excess_returns = EquilibriumExcessReturn ( MeanP , VarP , 0.02 , result_dataframe [ \"Weight\" ] . values , cov_matrix . as_matrix ()) print_assets_2 ( result_dataframe [ \"Symbol\" ] . values , result_dataframe [ \"Weight\" ] . values , result_dataframe [ \"AnnualExpectReturn\" ] . values , equil_excess_returns , cov_matrix . as_matrix () ) Name Weight Return Pi Dev XOM 13.3% 5.6% 6.2% 28.9% AAPL 25.0% 22.4% 15.8% 51.1% MSFT 14.7% 13.6% 10.7% 43.2% JNJ 10.6% 6.4% 5.2% 29.3% GE 11.1% 4.0% 8.1% 34.7% CVX 6.7% 7.1% 5.5% 28.4% PG 7.9% 5.5% 5.2% 30.2% WFC 10.6% 10.7% 8.8% 41.0% The table above shows the overall tendency: as it is clear from the definitions, $\\pi_{P,i}$ will be proportional to the product of the capitalization weight and standard deviation of the asset. Also it is interesting to notice that the procedure EquilibriumExcessReturn will decrease the level of statistical fluctuations in returns: In [19]: x = np . linspace ( 0 , len ( result_dataframe [ \"AnnualExpectReturn\" ] . values ), len ( result_dataframe [ \"AnnualExpectReturn\" ] . values )) pl . plot ( x , result_dataframe [ \"AnnualExpectReturn\" ] . values , label = \"Annual Historical Returns\" ) pl . plot ( x , equil_excess_returns , label = \"Equilibrium Excess Returns\" ) pl . xticks ( x , result_dataframe [ \"Symbol\" ] . values ) pl . legend () Out[19]: <matplotlib.legend.Legend at 0xaa60168c> Now let's compare the mean-variance characteristics of some asset pair before and after application of the equilibrium excess : In [20]: def plot_mean_variance_equilibrium_excess_returns ( result_dataframe , cov_matrix , equil_excess_returns , portfolio = [ 'XOM' , 'CVX' ], rf =. 0 ): assert len ( portfolio ) == 2 , \"portfolio should have only 2 instruments\" returns = result_dataframe . loc [ result_dataframe [ \"Symbol\" ] . isin ( portfolio )][ 'AnnualExpectReturn' ] . values indexes = result_dataframe . loc [ result_dataframe [ \"Symbol\" ] . isin ( portfolio )] . index . values eq_returns = [ equil_excess_returns [ indexes [ 0 ]] + rf , equil_excess_returns [ indexes [ 1 ]] + rf ] fractions = np . linspace ( 0. , 1. , 75 ) total_std_deviations_with_corr = map ( lambda x : x * cov_matrix . loc [ indexes [ 0 ], indexes [ 0 ]] + ( 1. - x ) * cov_matrix . loc [ indexes [ 1 ], indexes [ 1 ]] - 2 * x * ( 1. - x ) * cov_matrix . loc [ indexes [ 0 ], indexes [ 1 ]], fractions ) total_returns = map ( lambda x : x * returns [ 0 ] + ( 1. - x ) * returns [ 1 ], fractions ) equilibrium_returns = map ( lambda x : x * eq_returns [ 0 ] + ( 1. - x ) * eq_returns [ 1 ], fractions ) plt . plot ( total_std_deviations_with_corr , total_returns , label = \"historical data\" , lw = 3 , color = \"green\" ) plt . plot ( total_std_deviations_with_corr , equilibrium_returns , label = \"eqilibrium excess\" , lw = 3 , color = \"blue\" ) for i , label in enumerate ( portfolio ): pl . scatter ([ cov_matrix . loc [ indexes [ i ], indexes [ i ]]], [ returns [ i ]], marker = 'x' , s = 202 , lw = 3 , color = \"green\" ) # draw assets pl . text ( cov_matrix . loc [ indexes [ i ], indexes [ i ]], returns [ i ], ' %s ' % label , verticalalignment='center', color=\"green\") pl . scatter ([ cov_matrix . loc [ indexes [ i ], indexes [ i ]]], [ eq_returns [ i ] ], marker = 'x' , s = 202 , lw = 3 , color = \"blue\" ) # draw assets pl . text ( cov_matrix . loc [ indexes [ i ], indexes [ i ]], eq_returns [ i ], ' %s ' % label , verticalalignment='center', color=\"blue\") plt . xlabel ( \"Std Deviation\" ) plt . ylabel ( \"Returns\" ) plt . legend () plt . title ( \"Trade-off between Risk and Returs for %r \" % portfolio ); plot_mean_variance_equilibrium_excess_returns ( result_dataframe , cov_matrix , equil_excess_returns ) From the plot above, we see that equilibrium excess calculation makes the returns of two assets to be closer to each other in a two-asset portfolio model. $$\\\\[5pt]$$ 0.6.2 Forward optimization of equilibrium returns without application of investor views Just for fun, imagine, that we don't have private information, and consequently, we can skip application of views $(e)$ and $(f)$ in the workflow of the model. Let's make a Forward optimization with a help of the MinimumVarianceFrontier then. In [21]: rf =. 02 frontier_level , frontier_return_equilibr , frontier_var_equilibr , frontier_weights_equilibr = MinimumVarianceFrontier ( equil_excess_returns + rf , cov_matrix . as_matrix ()) '''function to plot the optimzed portfolio risks vs aversion levels ''' def plot_optimized_portfolio_at_levels ( frontier_level , frontier_return , frontier_var , frontier_weights , linestyle = '--' , color = \"b\" , label = \"optimized by MinimumVarianceFrontier\" ): plt . plot ( frontier_var , frontier_return , linestyle = linestyle , color = color , label = label , lw = 3 ) plt . xlabel ( \"Std Deviation\" ) plt . ylabel ( \"Returns\" ) plt . legend () plt . title ( \"Trade-off between Risk and Returs for the portfolio optimized by MinimumVarianceFrontier\" ); plot_optimized_portfolio_at_levels ( frontier_level , frontier_return , frontier_var , frontier_weights , color = \"b\" , label = \"historical data\" ) plot_optimized_portfolio_at_levels ( frontier_level , frontier_return_equilibr , frontier_var_equilibr , frontier_weights_equilibr , linestyle = '-.' , color = \"r\" , label = \"eqilibrium data\" ) for i , label in enumerate ( result_dataframe [ \"Symbol\" ] . values ): pl . scatter ([ cov_matrix . loc [ i , i ]], [ result_dataframe [ 'AnnualExpectReturn' ] . values [ i ]], marker = 'o' , s = 22 , lw = 3 , color = \"blue\" ) # draw assets pl . text ( cov_matrix . loc [ i , i ], result_dataframe [ 'AnnualExpectReturn' ] . values [ i ], ' %s ' % label , verticalalignment='center', color=\"blue\") pl . scatter ([ cov_matrix . loc [ i , i ]], [ equil_excess_returns [ i ] + rf ], marker = 'x' , s = 32 , lw = 2 , color = \"red\" ) # draw assets pl . text ( cov_matrix . loc [ i , i ], equil_excess_returns [ i ] + rf , ' %s ' % label , verticalalignment='center', color=\"red\") $$\\\\[5pt]$$ 0.6.3 Forward optimization of equilibrium returns with application of investor views As we have already mentioned, the whole idea behind the reverse and forward optimization in Black-Litterman model is a process of incorporating custom views to the returns. The good news is that the whole process can be expressed as mathematical formula, while the bad news is that the steps to derive this formula are beyond the scope of this article. Given the views and links matrices $Q$ and $P$, we calculate view-adjusted excess returns $\\pi&#94;{\\prime}_{P,ij}$ as follows: $$\\tau=0.025,\\\\$$$$\\Omega_{im}=\\tau⋅P_{ij}\\cdot C_{jk}\\cdot P&#94;{T}_{km},\\\\$$$$\\pi&#94;{\\prime}_{P,i}=\\left([(\\tau\\cdot C)&#94;{-1} + P&#94;T\\cdot \\Omega&#94;{-1} \\cdot P]&#94;{-1}\\right)_{ij} \\left([(\\tau\\cdot C)&#94;{-1}\\cdot \\pi_{P} + P&#94;T\\cdot \\Omega&#94;{-1} \\cdot Q&#94;T]&#94;{-1}\\right)_{j},$$ where $\\tau$ is scaling factor, $\\Omega_{im}$ is uncertainty matrix and $\\pi&#94;{\\prime}_{P,i}$ is a vector of view-adjusted equilibrium excess returns. The matrix $P$ is a link matrix having (0,1,-1) in rows. The matrix $Q$ is a view matrix providing us some private information. The matrix $P$ usually indicates our understanding on connections between shares,instruments,assets etc obtained from the private information. $$\\\\[3pt]$$ What kind of the information does the view matrix $Q$ can handle? $$\\\\[3pt]$$ Usually, this information is about \"outperformance\" and \"underperformance\" of one asset over another asset. From http://dictionary.cambridge.org/dictionary/english/outperformance : Outperformance -- the fact of producing more money for investors than other shares, bonds, etc. of a similar type. The Underperfomance can be formulated similar. So if somebody tells us as a secret: Microsoft (MSFT) will outperform General Electric (GE) by 5% Then, using our definition of instruments instruments=['XOM', 'AAPL', 'MSFT', 'JNJ', 'GE', 'CVX', 'PG', 'WFC'] we can easily define the view and links matrices as follows: $$Q_{1\\times 1} = \\begin{pmatrix} 0.05 \\end{pmatrix},\\\\ $$$$P_{1\\times 8} = \\begin{pmatrix} 0 & 0 & 1 & 0 & -1 & 0 & 0 & 0 \\end{pmatrix}.\\\\ $$ If somebody tells us as another secret: Microsoft (MSFT) will outperform General Electric (GE) by 5% and Apple (AAPL) will under-perform Johnson & Johnson (JNJ) by 6% the corresponding matrices take the forms $$Q_{1\\times 2} = \\begin{pmatrix} 0.05 & 0.06 \\end{pmatrix},\\\\$$$$P_{2\\times 8} = \\begin{pmatrix} 0 & 0 & 1 & 0 & -1 & 0 & 0 & 0 \\\\ 0 & -1 & 0 & 1 & 0 & 0 & 0 & 0 \\\\ \\end{pmatrix} \\\\ $$$$\\\\[2.4pt]$$ Please note that rather than specifying uncertainty matrix $\\Omega$ of views P explicitly by the investor, we derive it from the formula above. It's because we believe that volatility of assets represented by covariance matrix $C$ is affecting certainty of both returns and views in a similar way! Only the factor $\\tau$ explicitly controls a precision of the investor views. Let's try to code the mechanism of links $P$ and views $Q$. In [22]: ''' Igor Marfin <Unister Gmb, 2014> igor.marfin@unister.de Calculation of the equilibrium excess returns with links and views provided by an invsetor ''' from numpy.linalg import inv def EquilibriumExcessReturnPrime ( pi , tau , P , Q , C ): ''' calculates the \\pi_i&#94;{\\prime} ''' ## calculation of the omega - uncertainty matrix about views ## $$\\Omega_{im}=\\tau⋅P_{ij}\\cdot C_{jk}\\cdot P&#94;{T}_{km},\\\\$$ Omega = np . dot ( np . dot ( np . dot ( tau , P ), C ), P . T ) # 0.025 * P * C * transpose(P) # calculation of equilibrium excess returns with views incorporated # $$\\pi&#94;{\\prime}_{P,i}=\\left([(\\tau\\cdot C)&#94;{-1} + P&#94;T\\cdot \\Omega&#94;{-1} \\cdot P]&#94;{-1}\\right)_{ij} # \\left([(\\tau\\cdot C)&#94;{-1}\\cdot \\pi_{P} + P&#94;T\\cdot \\Omega&#94;{-1} \\cdot Q]&#94;{-1}\\right)_{j},$$ sub_a = inv ( np . dot ( tau , C )) sub_b = np . dot ( np . dot ( P . T , inv ( Omega )), P ) sub_c = np . dot ( inv ( np . dot ( tau , C )), pi ) sub_d = np . dot ( np . dot ( P . T , inv ( Omega )), Q . T ) sub_d = np . reshape ( sub_d , len ( pi )) pi_prime = np . dot ( inv ( sub_a + sub_b ), ( sub_c + sub_d )) return pi_prime def print_assets_3 ( names , W , R , Pi , Pi_prime , C ): print ( \" %-10s %6s %6s %6s %6s %6s \" % ( \"Name\" , \"Weight\" , \"Return\" , \"Pi\" , \"Pi_prime\" , \"Dev\" )) for i in range ( len ( names )): print ( \" %-10s %5.1f%% %5.1f%% %5.1f%% %5.1f%% %5.1f%% \" % ( names [ i ], 100 * W [ i ], 100 * R [ i ], 100 * Pi [ i ], 100. * Pi_prime [ i ] , 100 * C [ i , i ] **. 5 )) # A link matrix P = np . array ([ [ 0. , 0. , 1. , 0. , - 1. , 0. , 0. , 0. ], [ 0. , - 1. , 0. , 1. , 0. , 0. , 0. , 0. ], ]) print \" \\n\\n The shape of the Link Matrix: \" , P . shape , ' \\n\\n ' # A view matrix Q = np . array ([ [ 0.05 , 0.06 ], ]) print \" \\n\\n The shape of the View Matrix: \" , Q . shape , ' \\n\\n ' tau = 0.025 # a precision parameter #Omega = np.dot(np.dot(np.dot(tau, P), cov_matrix.as_matrix()), P.T) # 0.025 * P * C * transpose(P) #print Omega pi_prime = EquilibriumExcessReturnPrime ( equil_excess_returns , tau , P , Q , cov_matrix . as_matrix ()) The shape of the Link Matrix: (2, 8) The shape of the View Matrix: (1, 2) It seems that introducing the private information wee have to relocate the variance of instruments having the new correlations in a head: In [23]: # covariance matrix from private information Omega = np . dot ( np . dot ( np . dot ( tau , P ), cov_matrix . as_matrix ()), P . T ) # function to correct old C covariance function. def correct_cov_matrix_view ( Omega , C , P ): for i in range ( len ( P )): indexes = np . where ( P [ i ] != 0. ) for j , indx in enumerate ( indexes [ 0 ]): C [ indx , indx ] += Omega [ i ][ j ] return C cov_matrix_2 = correct_cov_matrix_view ( Omega , cov_matrix . as_matrix (), P ) In [24]: print_assets_3 ( result_dataframe [ \"Symbol\" ] . values , result_dataframe [ \"Weight\" ] . values , result_dataframe [ \"AnnualExpectReturn\" ] . values , equil_excess_returns , pi_prime , cov_matrix_2 ) Name Weight Return Pi Pi_prime Dev XOM 13.3% 5.6% 6.2% 6.1% 28.9% AAPL 25.0% 22.4% 15.8% 9.3% 51.0% MSFT 14.7% 13.6% 10.7% 10.8% 43.8% JNJ 10.6% 6.4% 5.2% 7.0% 30.6% GE 11.1% 4.0% 8.1% 7.0% 34.7% CVX 6.7% 7.1% 5.5% 5.4% 28.4% PG 7.9% 5.5% 5.2% 5.3% 30.2% WFC 10.6% 10.7% 8.8% 8.2% 41.0% Wow! It looks like we have done in the right direction. The outperformance of MSFT coded in the $P$ and $Q$ matrices increases its equilibrium excess return from 10.7% to 10.9% while GE value was decreased from 8.1% to 7.1%. Ok. Now we would like to make the last exercise with equilibrium excess returns: the forward optimization of the $\\pi_{P,i}&#94;{\\prime}$ with the MinimumVarianceFrontier . In [25]: rf =. 02 frontier_level , frontier_return_equilibr , frontier_var_equilibr , frontier_weights_equilibr = MinimumVarianceFrontier ( equil_excess_returns + rf , cov_matrix . as_matrix ()) frontier_level , frontier_return_equilibr_prime , frontier_var_equilibr_prime , frontier_weights_equilibr_prime = MinimumVarianceFrontier ( pi_prime + rf , cov_matrix_2 ) plot_optimized_portfolio_at_levels ( frontier_level , frontier_return , frontier_var , frontier_weights , color = \"b\" , label = \"historical data\" ) plot_optimized_portfolio_at_levels ( frontier_level , frontier_return_equilibr , frontier_var_equilibr , frontier_weights_equilibr , linestyle = '-.' , color = \"r\" , label = \"eqilibrium data\" ) plot_optimized_portfolio_at_levels ( frontier_level , frontier_var_equilibr_prime , frontier_var_equilibr , frontier_weights_equilibr_prime , linestyle = '-' , color = \"g\" , label = \"eqilibrium data with investor views\" ) for i , label in enumerate ( result_dataframe [ \"Symbol\" ] . values ): pl . scatter ([ cov_matrix . loc [ i , i ]], [ result_dataframe [ 'AnnualExpectReturn' ] . values [ i ]], marker = 'o' , s = 22 , lw = 3 , color = \"blue\" ) # draw assets pl . text ( cov_matrix . loc [ i , i ], result_dataframe [ 'AnnualExpectReturn' ] . values [ i ], ' %s ' % label , verticalalignment='center', color=\"blue\") pl . scatter ([ cov_matrix . loc [ i , i ]], [ equil_excess_returns [ i ] + rf ], marker = 'x' , s = 32 , lw = 2 , color = \"red\" ) # draw assets pl . text ( cov_matrix . loc [ i , i ], equil_excess_returns [ i ] + rf , ' %s ' % label , verticalalignment='center', color=\"red\") pl . scatter ([ cov_matrix . loc [ i , i ]], [ pi_prime [ i ] + rf ], marker = 'x' , s = 32 , lw = 2 , color = \"green\" ) # draw assets pl . text ( cov_matrix . loc [ i , i ], pi_prime [ i ] + rf , ' %s ' % label , verticalalignment='center', color=\"green\") $$\\\\[5pt]$$ 0.7 Kelly Criterion So far, we have considered the case when the investors could buy shares of companies taken in the portfolio. The key point \"buy\" can have two meanings: investors \"buy\" something using only fonds and money which belong them; they \"buy\" something using fonds and money which belong them + a loan provided by a broker We have understood the first meaning of the \"buy\" in our discussions early. Now we want to study the case when an external \"leverage\" is possible. What does the leverage mean? $$\\\\[5pt]$$ 0.7.1 Leverage definition We will define leverage as the ratio of the size of a portfolio to the actual account equity within that portfolio. Here is an example of the purchasing a house with a mortgage taken from the post https://www.quantstart.com/articles/Money-Management-via-the-Kelly-Criterion : To make this clear we can use the analogy of purchasing a house with a mortgage. A mortgage loan, also referred to as a mortgage, is used by purchasers of real property to raise capital to buy real estate; or by existing property owners to raise funds for any purpose while putting a lien on the property being mortgaged. The loan is \"secured\" on the borrower's property. Your down payment (or \"deposit\" for those of us in the UK!) constitutes your account equity, while the down payment plus the mortgage value constitutes the equivalent of the size of a portfolio. Thus a down payment of 50,000 USD on a 200,000 USD house (with a mortgage of 150,000 USD) constitutes a leverage of (150000+50000)/50000=4. Thus in this instance you would be 4x leveraged on the house. $$\\\\[5pt]$$ 0.7.2 Assumptions Before we state the Kelly Criterion specifically let's outline the assumptions that go into its derivation: returns are normally distributed (i.e. Gaussian). Further, each instrument has its own fixed mean and standard deviation of returns. The formula assumes that these mean and std values do not change, i.e. that they are same in the past as in the future. This is clearly not the case with most strategies, so be aware of this assumption. The returns being considered here are excess returns, which means they are net of all financing costs such as interest paid on margin and transaction costs. All of the trading profits are reinvested and no withdrawals of equity are carried out. This is clearly not as applicable in an institutional setting where the above mentioned management fees are taken out and investors often make withdrawals. All of the strategies are statistically independent (there is no correlation between strategies) and thus the covariance matrix between strategy returns is diagonal! $$\\\\[5pt]$$ 0.7.3 Kelly Criterion Formulation Let's imagine that we have a portfolio of $N$ instruments and we wish to determine both how to apply optimal leverage per instrument in order to maximize growth rate (but minimize drawdowns) and how to allocate capital between each asset. If we denote the allocation between each asset $i$ as a vector $f$ of length $N$, s.t. $f=(f_1,...,f_N)$, then the Kelly Criterion for optimal allocation to each instrument $f_i$ is given by: $$f_i=\\mu_i/\\sigma&#94;2_i,\\\\$$ Where $\\mu_i$ are the mean excess returns and $\\sigma_i$ are the standard deviation of excess returns for a instrument $i$. This formula essentially describes the optimal leverage that should be applied to each instrument. Let's get a little bit practice with this criterion. In [26]: ''' Igor Marfin <Unister Gmb, 2014> igor.marfin@unister.de Calculation of the leverage using Kelly Criterion ''' def KellyCriterionLeverage ( means , rf , C , W ): ''' returns the leverages, fractions of the fond to be assigned to each instrument, and expected long-term compounded growth rate, g''' meanP , varP = portfolioReturn ( W , means ), portfolioVariance ( W , C ) # calculate mean/variance of the portfolio sigmas = np . diag ( C ) means_without_rf = means - rf #total_leverage=sum(means_without_rf/sigmas**2) total_leverage = sum ( means_without_rf / sigmas ) # sharpe ratio of the portfolio sP = ( meanP - rf ) / sqrt ( varP ) #growth rate g = rf + sP ** 2 / 2. #return means_without_rf/sigmas**2,means_without_rf/sigmas**2/total_leverage,g return means_without_rf / sigmas , means_without_rf / sigmas / total_leverage , g rf =. 02 print KellyCriterionLeverage ( result_dataframe [ \"AnnualExpectReturn\" ] . values , rf , cov_matrix . as_matrix (), result_dataframe [ \"Weight\" ] . values ) (array([ 0.42673468, 0.78165834, 0.60258079, 0.47425392, 0.16859565, 0.63641134, 0.38497619, 0.51948301]), array([ 0.10682538, 0.19567415, 0.1508453 , 0.11872097, 0.0422049 , 0.15931417, 0.09637189, 0.13004326]), 0.10111899315930686) $$\\\\[5pt]$$ 0.7.4 Kelly Criterion in Practice: LET'S MAKE YOURSELF A MILLIONARE It is important to be aware that the Kelly Criterion requires a continuous rebalancing of capital allocation in order to remain valid. Clearly this is not possible in the discrete setting of actual trading and so an approximation must be made. The standard \"rule of thumb\" here is to update the Kelly allocation once a day. Further, the Kelly Criterion itself should be recalculated periodically, using a trailing mean and standard deviation with a lookback window. So, in next sections of the notebook, I want to simulate the Kelly Criterion Machinery: rebalancing of capital allocation according to the Kelly Criterion recalculating the Kelly Criterion itself using the trailing mean and standard deviations with a lookback window of the 6 months. In [27]: ''' Igor Marfin <Unister Gmb, 2014> igor.marfin@unister.de Simulation of the trading operation using Kelly Criterion ''' from dateutil.relativedelta import relativedelta from datetime import timedelta import math # Free-Risk rate rf =. 02 # period of the simulations start_date_simulations = datetime ( 2011 , 9 , 1 ) end_date_simulations = datetime ( 2015 , 10 , 13 ) # today # money of the investor i # 100000$ or 1000 $ ? account_equity = 100000 # 100K $ #account_equity=1000 # 1K $ # period of kelly rebalancing: 1 day. We recalculate our portfolio each day period_kelly_rebalancing = relativedelta ( days =+ 1 ) # period of kelly recalculation: 15 days period_kelly_recalculation =+ 15 # a lookback window: 6 month lookback_window = relativedelta ( months =- 6 ) # portfolio fractions: fractions on which we split our investment: portfolio_fractions = result_dataframe [ \"Weight\" ] . values # using capitalization def SplitAccountEquity ( stocks = instruments , fractions = portfolio_fractions , account_equity = account_equity , leverages = None ): ''' returns the account_equity splited into fraction according to the weights of instruments ''' splited_account_equity = {} if leverages is None : for i , stock in enumerate ( instruments ): splited_account_equity . update ({ stock : account_equity * fractions [ i ]}) return splited_account_equity , fractions else : new_fractions = [] for i , leverage in enumerate ( leverages ): new_fractions += [ fractions [ i ] if leverage > 0. else 0. ] #new_fractions+=[ fractions[i] if leverage>1. else 0. ] new_fractions = np . array ( new_fractions ) / sum ( new_fractions ) new_fractions = new_fractions . tolist () for i , stock in enumerate ( instruments ): splited_account_equity . update ({ stock : account_equity * new_fractions [ i ]}) return splited_account_equity , new_fractions def getBrokerMargin ( leverage , invest_money ): ''' returns the marging of the broker and the total size of a portfolio. ''' #assert float(invest_money) !=0., 'Do you have an investment fond?' #assert float(leverage) >=1., 'leverage should not be less than 1' broker_margin = float ( invest_money ) * ( float ( leverage ) - 1. ) if ( broker_margin < 0. ): broker_margin = 0. total_invest = broker_margin + float ( invest_money ) if ( leverage < 0. ): total_invest = 0. return broker_margin , total_invest def LeverageOfPortfolio ( splited_account_equity , means , rf , C , W = portfolio_fractions , stocks = instruments ): ''' calculates the leverages of the instruments using the Kelly Criterion and returns them, total leverage, total size of the portfolio and assigned budgets for trading operations on each instrument ''' leverages , _ , _ = KellyCriterionLeverage ( means , rf , C , W ) leverages = map ( lambda x : max ( 1.1 , x ) if x > 0. else float ( x ), leverages ) #leverages = map(lambda x: 1. if float(x)<1. else float(x),leverages) total_broker_margin = 0. total_portfoio_size = 0. total_account_equity = sum ( splited_account_equity . values ()) assert total_account_equity != 0. , 'Do you have money?' assigned_budgets = {} for i , stock in enumerate ( stocks ): broker_margin , assigned_budget = getBrokerMargin ( leverages [ i ], splited_account_equity [ stock ]) total_broker_margin += broker_margin total_portfoio_size += assigned_budget assigned_budgets . update ({ stock : assigned_budget }) return leverages ,( total_account_equity + total_broker_margin ) / total_account_equity , total_portfoio_size , total_broker_margin , assigned_budgets def ExtractHistoricalDataInPeriod ( dataframe , start_date , end_date ): ''' returns the data in the period ''' return [ data . ix [ ( data . index >= start_date ) & ( data . index <= end_date ) ] for data in dataframe ] def NumberOfSharesToBuy ( data , date , budget , stocks = instruments , feature = 'Close' ): ''' returns the number of shares to buy for each asset''' data = ExtractHistoricalDataInPeriod ( data , date , date ) prices = map ( lambda item : item [ feature ] . values [ 0 ] , data ) return np . array ([ math . floor ( budget [ stock ] / prices [ i ]) for i , stock in enumerate ( stocks )]), prices def profit ( data , stock_holdings , stocks = instruments , feature = 'Close' ): ''' returns the number of obtained profit ''' returns = map ( lambda item : item [ feature + '_pct_change' ] . values [ 0 ] , data ) today_prices = map ( lambda item : item [ feature ] . values [ 0 ] , data ) yesterday_prices = [ today_prices [ i ] / ( 1. + rtn ) for i , rtn in enumerate ( returns ) ] portfolio_profit = { k :( today_prices [ i ] - yesterday_prices [ i ]) * stock_holdings [ k ] for i , k in enumerate ( stocks ) } return portfolio_profit , today_prices def broker_marging_cost ( broker_magin , rf , period = 250 ): ''' return the cost of the broker margin at this moment ''' period = 250. if period <= 1. else period return rf / period * broker_magin def KellyRebalancing ( leverages , profit , splited_account_equity , assigned_budgets , today_prices , stocks = instruments ): ''' performs the rebalancing of the portfolio according to the Kelly Criterion''' decision_about_portfolio = { k :( 0. , 0. ) for k in stocks } # here we calculate new assigned_budgets for i , stock in enumerate ( stocks ): if float ( splited_account_equity [ stock ]) < 1e-6 : continue old_brocker_margin = float ( assigned_budgets [ stock ] - splited_account_equity [ stock ] ) old_splited_account_equity = splited_account_equity [ stock ] splited_account_equity [ stock ] += float ( profit [ stock ]) # will be returned back old_leverage = leverages [ i ] new_leverage = float ( assigned_budgets [ stock ] + profit [ stock ] ) / float ( splited_account_equity [ stock ]) new_brocker_margin , new_assigned_budget = getBrokerMargin ( leverages [ i ], splited_account_equity [ stock ]) delta_brocker_margin = float ( new_brocker_margin - old_brocker_margin ) assigned_budgets [ stock ] = new_assigned_budget # will be returned back today_price = float ( today_prices [ i ]) num_items_to_operate = np . sign ( delta_brocker_margin ) * np . ceil ( np . abs ( delta_brocker_margin / today_price )) budget_to_return_ask_broker = float ( num_items_to_operate * today_price ) assigned_budgets [ stock ] += float ( budget_to_return_ask_broker - delta_brocker_margin ) decision_about_portfolio [ stock ] = ( num_items_to_operate , budget_to_return_ask_broker ) # here we return a recalculated portfolio return decision_about_portfolio , splited_account_equity , assigned_budgets def StartTrading ( data , caps , start_date , account_equity = account_equity , fractions = portfolio_fractions , stocks = instruments , min_leverage = 1.1 , lookback_window = lookback_window , rf = rf , feature = 'Close' ): ''' performs all preporations to the trading ''' # 1th day: We start at the end of the 'start_day' and want to buy shares. # We will work with the 'Close' prises. # the variable stores the current portfolio status: numbers of the bought shares of different assets portfolio_status = { k : 0 for k in stocks } # calculate the summary on historical data within the lookback window data_lookback_window = ExtractHistoricalDataInPeriod ( data , start_date + lookback_window , start_date ) lookback_window_result_dataframe , lookback_window_cov_matrix = ProcessHistoricaData ( data_lookback_window , caps ) # get leverages frome the Kelly Criterion. We will use this leverages values # to calculate the fractions of the assets in our portfolio, and assigned our fonds for trading operations # Negative leverages will tell us not to spend our fonds on payments of these assets. leverages , _ , _ = KellyCriterionLeverage ( lookback_window_result_dataframe [ \"AnnualExpectReturn\" ] . values , rf , lookback_window_cov_matrix . as_matrix (), lookback_window_result_dataframe [ \"Weight\" ] . values ) leverages = map ( lambda x : max ( min_leverage , x ) if x > 0. else x , leverages ) # Now we split our budget accordingly to the leverages and calculate new fractions splited_account_equity , fractions = SplitAccountEquity ( stocks = stocks , fractions = fractions , account_equity = account_equity , leverages = leverages ) # Then we again calculate the leverage and the assigned budgets where the broker margins are taken # into account leverages , total_leverage , total_portfolio_size , total_broker_margin , assigned_budgets = LeverageOfPortfolio ( splited_account_equity , lookback_window_result_dataframe [ \"AnnualExpectReturn\" ] . values , rf , lookback_window_cov_matrix . as_matrix (), W = fractions , stocks = instruments ) # current_account_equity keeps the value of our account_equity (without broker margins) at the moment total_account_equity = total_portfolio_size - total_broker_margin active_account_equity = total_portfolio_size # total_broker_margin_cost will trace the total amount of costs of the broker margins total_broker_margin_cost = broker_marging_cost ( total_broker_margin , rf , period = 250. ) # Now let's define the amount of different shares we are going to buy at the end of the start day what_to_buy , prices = NumberOfSharesToBuy ( data , start_date , assigned_budgets , feature = feature ) # buy it # for item , value in enumerate ( what_to_buy ): active_account_equity -= value * prices [ item ] portfolio_status [ stocks [ item ]] += value return leverages , splited_account_equity , assigned_budgets , total_account_equity , \\ total_portfolio_size , total_broker_margin , total_broker_margin_cost , \\ active_account_equity , portfolio_status def CloseTrading ( data , date , portfolio_status , total_broker_margin_cost , total_broker_margin , stocks , feature ): '''closes trading operations ''' data = ExtractHistoricalDataInPeriod ( data , date , date ) prices = map ( lambda item : item [ feature ] . values [ 0 ] , data ) total_money_from_selling = 0. for i , stock in enumerate ( stocks ): total_money_from_selling += portfolio_status [ stock ] * prices [ i ] # return back broker margins total_money_from_selling -= ( total_broker_margin + total_broker_margin_cost ) # clean portfolio_status portfolio_status . clear () return total_money_from_selling def CanWeStartTrading ( portfolio_status ): ''' returns the flag if we are allowed to start preparation to the trading''' return len ( portfolio_status ) == 0. def KellyCriterionRecalculation ( data , date , lookback_window , min_leverage ): ''' makes the recalculation of the Kelly Criterion ''' # calculate the summary on historical data within the lookback window data_lookback_window = ExtractHistoricalDataInPeriod ( data , date + lookback_window , date ) lookback_window_result_dataframe , lookback_window_cov_matrix = ProcessHistoricaData ( data_lookback_window , caps ) # get leverages frome the Kelly Criterion. We will use this leverages values # to calculate the fractions of the assets in our portfolio, and assigned our fonds for trading operations # Negative leverages will tell us not to spend our fonds on payments of these assets. leverages , _ , _ = KellyCriterionLeverage ( lookback_window_result_dataframe [ \"AnnualExpectReturn\" ] . values , rf , lookback_window_cov_matrix . as_matrix (), lookback_window_result_dataframe [ \"Weight\" ] . values ) leverages = map ( lambda x : max ( min_leverage , x ) if x > 0. else x , leverages ) return leverages import copy def simulation ( data , start_date , end_date , caps , stocks = instruments , fractions = portfolio_fractions , account_equity = account_equity , period_kelly_rebalancing = period_kelly_rebalancing , min_leverage = 1.1 , lookback_window = lookback_window , rf = rf , feature = 'Close' , min_allowed_fracprofit_no_rebalance =- 0.1 , delta_in_days_for_kelly_recalculation =+ 15 ): ''' makes simulations of the Kelly Criterion Work ''' portfolio_status = {} active_account_equity = 0. day_of_kelly_recalculation = start_date initial_account_equity = account_equity increase_in_percentage_account_equity_to_stop = 0.2 decrease_in_percentage_account_equity_to_stop = 0.5 # will have the following result_header = [ \"Date\" , \"Account_Equity\" , \"Profit\" , \"Margin\" , \"Margin_Cost\" ] result = [] count = ( end_date - start_date ) . days while ( count > 0 ): count -= 1 # it is time make a decision if (( start_date - day_of_kelly_recalculation ) > timedelta ( days = delta_in_days_for_kelly_recalculation )): new_data = ExtractHistoricalDataInPeriod ( data , start_date , start_date ) if ( 0 in map ( lambda x : len ( x ), new_data )): start_date += period_kelly_rebalancing continue day_of_kelly_recalculation = start_date _portfolio_status = copy . deepcopy ( portfolio_status ) total_money_from_selling = CloseTrading ( data , day_of_kelly_recalculation , portfolio_status , total_broker_margin_cost , total_broker_margin , stocks , feature ) account_equity = total_money_from_selling + active_account_equity splited_account_equity = {} portfolio_status = {} total_broker_margin_cost = 0. total_broker_margin = 0. total_portfolio_size = 0. total_account_equity = account_equity if (( account_equity - initial_account_equity ) > increase_in_percentage_account_equity_to_stop * initial_account_equity ): result += [( start_date , account_equity , 0. , total_broker_margin , total_broker_margin_cost )] # let's add portfolio status result [ - 1 ] += tuple ([ _portfolio_status [ stock ] for stock in stocks ]) dataframe = pd . DataFrame ( result ) dataframe . columns = result_header + [ \"instrument_\" + stock for stock in stocks ] return dataframe , account_equity - initial_account_equity , start_date + period_kelly_rebalancing , False if (( account_equity - initial_account_equity ) < ( - 1. ) * decrease_in_percentage_account_equity_to_stop * initial_account_equity ): result += [( start_date , account_equity , 0. , total_broker_margin , total_broker_margin_cost )] # let's add portfolio status result [ - 1 ] += tuple ([ _portfolio_status [ stock ] for stock in stocks ]) dataframe = pd . DataFrame ( result ) dataframe . columns = result_header + [ \"instrument_\" + stock for stock in stocks ] return dataframe , account_equity - initial_account_equity , start_date + period_kelly_rebalancing , False if ( CanWeStartTrading ( portfolio_status )): new_data = ExtractHistoricalDataInPeriod ( data , start_date , start_date ) if ( 0 in map ( lambda x : len ( x ), new_data )): start_date += period_kelly_rebalancing continue leverages , splited_account_equity , assigned_budgets , total_account_equity , \\ total_portfolio_size , total_broker_margin , total_broker_margin_cost , \\ _active_account_equity , portfolio_status = StartTrading ( data , caps , start_date , account_equity , fractions , stocks , min_leverage , lookback_window , rf = rf , feature = 'Close' ) active_account_equity += _active_account_equity result += [( start_date , total_account_equity , 0. , total_broker_margin , total_broker_margin_cost )] result [ - 1 ] += tuple ([ portfolio_status [ stock ] for stock in stocks ]) start_date += period_kelly_rebalancing new_data = ExtractHistoricalDataInPeriod ( data , start_date , start_date ) if ( 0 in map ( lambda x : len ( x ), new_data )): continue portfolio_profit , today_prices = profit ( new_data , portfolio_status , stocks = stocks , feature = feature ) current_account_equity = sum ( assigned_budgets . values ()) - total_broker_margin total_portfolio_profit = float ( sum ( portfolio_profit . values ())) frac_total_portfolio_profit = total_portfolio_profit / float ( current_account_equity ) if (( frac_total_portfolio_profit < 0. ) and ( frac_total_portfolio_profit > min_allowed_fracprofit_no_rebalance )): for i , stock in enumerate ( stocks ): splited_account_equity [ stock ] += float ( portfolio_profit [ stock ]) # will be returned back assigned_budgets [ stock ] += float ( portfolio_profit [ stock ]) result += [( start_date , current_account_equity , total_portfolio_profit , total_broker_margin , total_broker_margin_cost )] result [ - 1 ] += tuple ([ portfolio_status [ stock ] for stock in stocks ]) continue decision_about_portfolio , new_splited_account_equity , new_assigned_budgets = KellyRebalancing ( leverages , portfolio_profit , splited_account_equity , assigned_budgets , today_prices , stocks ) for key , decision in decision_about_portfolio . items (): # let's perform a decision given by Kelly Criterion shares_amount_to_operate = decision [ 0 ] money_amount_to_operate = decision [ 1 ] if ( shares_amount_to_operate < 0. ): if ( np . abs ( portfolio_status [ key ]) < np . abs ( shares_amount_to_operate )): money_amount_to_operate = ( - 1. ) * money_amount_to_operate * np . abs ( portfolio_status [ key ]) / shares_amount_to_operate shares_amount_to_operate = ( - 1. ) * np . abs ( portfolio_status [ key ]) #if (shares_amount_to_operate<0): # print \"We are selling amount %2.3f of %s\"%(shares_amount_to_operate,key) # print \"We will return %3.2f $ back to the broker\"% np.abs(money_amount_to_operate) #if (shares_amount_to_operate>0.): # print \"We are buying amount %2.3f of %s\"%(shares_amount_to_operate,key) # print \"We will ask %3.2f $ from the broker\"% np.abs(money_amount_to_operate) if ( np . abs ( money_amount_to_operate ) > 0. ): # here we update our tracking variables total_broker_margin += money_amount_to_operate portfolio_status [ key ] += shares_amount_to_operate total_portfolio_size = sum ( new_assigned_budgets . values ()) current_account_equity = total_portfolio_size - total_broker_margin total_broker_margin_cost += broker_marging_cost ( total_broker_margin , rf , period = 250. ) result += [( start_date , current_account_equity , total_portfolio_profit , total_broker_margin , total_broker_margin_cost )] result [ - 1 ] += tuple ([ portfolio_status [ stock ] for stock in stocks ]) if ( len ( result ) > 0 ): dataframe = pd . DataFrame ( result ) dataframe . columns = result_header + [ \"instrument_\" + stock for stock in stocks ] return dataframe , current_account_equity - initial_account_equity , start_date + period_kelly_rebalancing , True else : dataframe = pd . DataFrame () return dataframe , 0. , start_date + period_kelly_rebalancing , True So, we are ready to start our simulations. Let's look at the Kelly Criterion in practice. But before,we run the code determined above, we should understand that not all our instruments are statistically independent! Some trading profits are NOT reinvested and there are withdrawals of equity time to time (because we want to get some money now ... :-) ) The last point of this list is coded as my_earning in the code below. All these modifications can degradate the overall performance of the Kelly Criterion. In [28]: # Simulations are starting... my_earning = [] log_of_operations = [] timeout_in_days =+ 20 # if we got loss in the last tradings, let's wait timeout_in_days ... # I want to follow to short-time fluctuations, then I recalculate the leverage each 10 days # using the lookback window of 6 months period_kelly_recalculation =+ 10 # it is my small income current_earning = 0. END = False start_date = start_date_simulations while not END : dataframe , earned_money , start_date , END = simulation ( data , start_date , end_date_simulations , caps , min_allowed_fracprofit_no_rebalance =- 0.10 , delta_in_days_for_kelly_recalculation = period_kelly_recalculation ) print \"At this day %s we have earned %f $\" % ( str ( start_date ), earned_money ) log_of_operations += [ dataframe ] my_earning += [( start_date + relativedelta ( days =- 1 ), earned_money )] #Ups, if I have no more money to recove this loss, exit from my tradings operations if ( ( earned_money < 0. ) and ( current_earning < np . abs ( earned_money ))): current_earning += earned_money break ; else : current_earning += earned_money if (( earned_money < 0. ) and ( timeout_in_days > 0. ) ): start_date += relativedelta ( days = timeout_in_days ) if not (( end_date_simulations - start_date ) . days > 0 ): start_date = end_date_simulations #if I have got losses two times in row and I want to exit from my tradings operations if (( len ( my_earning ) > 1 ) and ( my_earning [ - 2 ][ 1 ] < 0. ) and ( my_earning [ - 1 ][ 1 ] < 0. )): break print \"total earnings: %f \" % current_earning At this day 2012-02-25 00:00:00 we have earned 45928.833729$ At this day 2012-03-20 00:00:00 we have earned 112763.560604$ At this day 2012-04-14 00:00:00 we have earned -54694.262967$ At this day 2012-08-21 00:00:00 we have earned 20349.806350$ At this day 2012-09-18 00:00:00 we have earned 25710.073194$ At this day 2013-02-06 00:00:00 we have earned 20562.971367$ At this day 2013-03-16 00:00:00 we have earned 35034.955393$ At this day 2013-03-28 00:00:00 we have earned 35251.499653$ At this day 2013-04-20 00:00:00 we have earned 32706.859199$ At this day 2013-12-10 00:00:00 we have earned 41932.492853$ At this day 2014-05-14 00:00:00 we have earned 20706.852526$ At this day 2014-06-10 00:00:00 we have earned -191394.058177$ At this day 2014-09-20 00:00:00 we have earned 39335.777488$ At this day 2015-07-21 00:00:00 we have earned 34009.145349$ At this day 2015-08-26 00:00:00 we have earned -54016.425119$ At this day 2015-10-14 00:00:00 we have earned 12132.128638$ total earnings: 176320.210078 /usr/local/lib/python2.7/dist-packages/IPython/kernel/__main__.py:28: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy In [29]: # how many days the simulations has covered?< print ( end_date_simulations - start_date_simulations ) 1503 days, 0:00:00 Wow! Having 100K\\$ in the pocket, I was able to earn about 130K\\$ within 1503 days starting from 2011-09-01! Let's imagine that I don't have 100K\\$ but I have 1K\\$ (it is my current situation :-) ) In [30]: # Simulations are starting... account_equity = 1000 # only 1K$ my_earning = [] log_of_operations = [] timeout_in_days =+ 20 # if we got loss in the last tradings, let's wait timeout_in_days ... # I want to follow to short-time fluctuations, then I recalculate the leverage each 10 days # using the lookback window of 6 months period_kelly_recalculation =+ 10 # it is my small income current_earning = 0. END = False start_date = start_date_simulations while not END : dataframe , earned_money , start_date , END = simulation ( data , start_date , end_date_simulations , caps , account_equity = account_equity , min_allowed_fracprofit_no_rebalance =- 0.10 , delta_in_days_for_kelly_recalculation = period_kelly_recalculation ) print \"At this day %s we have earned %f $\" % ( str ( start_date ), earned_money ) log_of_operations += [ dataframe ] if ( np . isnan ( earned_money )): earned_money = 0. my_earning += [( start_date + relativedelta ( days =- 1 ), earned_money )] #Ups, if I have no more money to recove this loss, exit from my tradings operations if ( ( earned_money < 0. ) and ( current_earning < np . abs ( earned_money ))): current_earning += earned_money break ; else : current_earning += earned_money if (( earned_money < 0. ) and ( timeout_in_days > 0. ) ): start_date += relativedelta ( days = timeout_in_days ) if not (( end_date_simulations - start_date ) . days > 0 ): start_date = end_date_simulations #if I have got losses two times in row and I want to exit from my tradings operations if (( len ( my_earning ) > 1 ) and ( my_earning [ - 2 ][ 1 ] < 0. ) and ( my_earning [ - 1 ][ 1 ] < 0. )): break print \"total earnings: %f \" % current_earning At this day 2011-10-05 00:00:00 we have earned 374.189708$ At this day 2011-11-09 00:00:00 we have earned 770.145786$ At this day 2011-12-03 00:00:00 we have earned 413.912956$ At this day 2011-12-28 00:00:00 we have earned 426.439717$ At this day 2012-01-21 00:00:00 we have earned 344.434323$ At this day 2012-02-14 00:00:00 we have earned 521.177178$ At this day 2012-03-10 00:00:00 we have earned 861.460366$ At this day 2012-03-22 00:00:00 we have earned 635.907127$ At this day 2012-04-03 00:00:00 we have earned 261.490364$ At this day 2012-04-28 00:00:00 we have earned 250.138907$ At this day 2012-06-02 00:00:00 we have earned 876.920552$ At this day 2012-06-26 00:00:00 we have earned 474.062371$ At this day 2012-07-10 00:00:00 we have earned 526.890764$ At this day 2012-08-04 00:00:00 we have earned 287.289043$ At this day 2012-08-28 00:00:00 we have earned 802.073854$ At this day 2012-09-22 00:00:00 we have earned 697.623315$ At this day 2012-10-16 00:00:00 we have earned 633.938520$ At this day 2012-11-13 00:00:00 we have earned 458.420275$ At this day 2012-12-08 00:00:00 we have earned 236.768612$ At this day 2013-01-12 00:00:00 we have earned 555.520033$ At this day 2013-02-05 00:00:00 we have earned 464.600069$ At this day 2013-02-20 00:00:00 we have earned 230.723015$ At this day 2013-03-16 00:00:00 we have earned 411.392502$ At this day 2013-03-28 00:00:00 we have earned 351.189547$ At this day 2013-04-20 00:00:00 we have earned 480.889286$ At this day 2013-05-25 00:00:00 we have earned 728.093010$ At this day 2013-07-10 00:00:00 we have earned 774.309248$ At this day 2013-08-03 00:00:00 we have earned 228.524601$ At this day 2013-08-27 00:00:00 we have earned 331.478208$ At this day 2013-09-21 00:00:00 we have earned 340.138511$ At this day 2013-10-15 00:00:00 we have earned 244.750869$ At this day 2013-10-29 00:00:00 we have earned 252.951040$ At this day 2013-11-23 00:00:00 we have earned 755.663897$ At this day 2013-12-17 00:00:00 we have earned 577.515155$ At this day 2014-01-11 00:00:00 we have earned 418.938885$ At this day 2014-02-15 00:00:00 we have earned 1112.380757$ At this day 2014-03-11 00:00:00 we have earned 466.890577$ At this day 2014-04-05 00:00:00 we have earned 330.065268$ At this day 2014-04-29 00:00:00 we have earned 823.577360$ At this day 2014-05-24 00:00:00 we have earned 600.806076$ At this day 2014-06-28 00:00:00 we have earned 1278.445958$ At this day 2014-07-22 00:00:00 we have earned 360.687250$ At this day 2014-08-27 00:00:00 we have earned 566.329206$ At this day 2014-09-20 00:00:00 we have earned 698.657084$ At this day 2014-11-05 00:00:00 we have earned 968.564509$ At this day 2014-11-29 00:00:00 we have earned 228.345470$ At this day 2015-01-14 00:00:00 we have earned 715.658220$ At this day 2015-02-18 00:00:00 we have earned 1029.103939$ At this day 2015-04-07 00:00:00 we have earned 384.163437$ At this day 2015-05-02 00:00:00 we have earned 212.347789$ At this day 2015-06-09 00:00:00 we have earned 346.414808$ At this day 2015-07-18 00:00:00 we have earned 653.665870$ At this day 2015-10-14 00:00:00 we have earned nan$ total earnings: 27776.065193 /usr/local/lib/python2.7/dist-packages/IPython/kernel/__main__.py:28: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy Wow again! I have got ~27K now! There is only problem here: where can we find the broker with 3% rate to provide us leverage for such small initial investment. In [38]: # Here is how I have earned 27K$ :-) fig , ax1 = plt . subplots () ax1 . plot ( map ( lambda x : x [ 0 ], my_earning ), map ( lambda x : x [ 1 ], my_earning ), color = \"b\" , label = \" \\n\\n\\n income per operation \\n\\n\\n \" ) legend () ax2 = ax1 . twinx () ax2 . plot ( map ( lambda x : x [ 0 ], my_earning ), np . cumsum ( map ( lambda x : x [ 1 ], my_earning )), color = \"r\" , label = \"total income\" ) legend () Out[38]: <matplotlib.legend.Legend at 0xc825e0c> In [51]: log_of_operations [ - 2 ][[ 'Date' , u'Account_Equity' , u'Profit' , u'Margin' , u'Margin_Cost' ]] Out[51]: Date Account_Equity Profit Margin Margin_Cost 0 2015-06-09 1000.000000 0.000000 2661.606658 0.212929 1 2015-06-10 1038.460083 38.460083 2875.326661 0.442955 2 2015-06-11 1038.460083 -8.420152 2875.326661 0.442955 3 2015-06-12 1030.039931 -31.869972 2875.326661 0.442955 4 2015-06-15 998.169959 -10.780000 2875.326661 0.442955 5 2015-06-16 1002.749968 15.360009 2636.066664 0.653840 6 2015-06-17 1002.749968 -4.499908 2636.066664 0.653840 7 2015-06-18 1015.530002 17.279942 2876.386664 0.883951 8 2015-06-19 1015.530002 -33.810027 2876.386664 0.883951 9 2015-06-22 1012.806090 0.000000 3584.681653 0.286775 10 2015-06-23 1012.806090 -9.550086 3584.681653 0.286775 11 2015-06-24 1013.936105 10.680101 3627.991653 0.577014 12 2015-06-25 1013.936105 -24.930039 3627.991653 0.577014 13 2015-06-26 989.006066 -12.170012 3627.991653 0.577014 14 2015-06-29 976.836054 -82.080021 3627.991653 0.577014 15 2015-06-30 916.156096 21.400063 3168.891651 0.830525 16 2015-07-01 950.516030 34.359934 3562.571647 1.115531 17 2015-07-02 950.516030 -2.949856 3562.571647 1.115531 18 2015-07-06 1096.587189 0.000000 6573.573275 0.525886 19 2015-07-07 1096.587189 -10.249895 6573.573275 0.525886 20 2015-07-08 913.257218 -173.080076 5294.343278 0.949433 21 2015-07-09 913.257218 -63.419959 5294.343278 0.949433 22 2015-07-10 970.887279 121.050020 5787.953282 1.412470 23 2015-07-13 1074.327395 103.440116 6430.543301 1.926913 24 2015-07-14 1096.377272 22.049877 6388.843300 2.438021 25 2015-07-15 1160.217206 63.839934 7038.453298 3.001097 26 2015-07-16 1251.687025 91.469819 7536.233283 3.603995 27 2015-07-17 1300.287010 48.599985 7894.393269 4.235547 28 2015-07-17 1653.665870 0.000000 0.000000 0.000000 In [50]: filter ( lambda x : \"instrument_\" in x , log_of_operations [ - 1 ] . columns ) log_of_operations [ - 2 ][[ \"Date\" ] + filter ( lambda x : \"instrument_\" in x , log_of_operations [ - 1 ] . columns )] Out[50]: Date instrument_XOM instrument_AAPL instrument_MSFT instrument_JNJ instrument_GE instrument_CVX instrument_PG instrument_WFC 0 2015-06-09 0 18 0 0 27 0 0 8 1 2015-06-10 0 19 0 0 28 0 0 9 2 2015-06-11 0 19 0 0 28 0 0 9 3 2015-06-12 0 19 0 0 28 0 0 9 4 2015-06-15 0 19 0 0 28 0 0 9 5 2015-06-16 0 18 0 0 26 0 0 8 6 2015-06-17 0 18 0 0 26 0 0 8 7 2015-06-18 0 19 0 0 28 0 0 9 8 2015-06-19 0 19 0 0 28 0 0 9 9 2015-06-22 0 22 0 0 27 0 0 15 10 2015-06-23 0 22 0 0 27 0 0 15 11 2015-06-24 0 23 0 0 26 0 0 14 12 2015-06-25 0 23 0 0 26 0 0 14 13 2015-06-26 0 23 0 0 26 0 0 14 14 2015-06-29 0 23 0 0 26 0 0 14 15 2015-06-30 0 20 0 0 25 0 0 13 16 2015-07-01 0 22 0 0 26 0 0 15 17 2015-07-02 0 22 0 0 26 0 0 15 18 2015-07-06 0 38 0 0 44 0 0 29 19 2015-07-07 0 38 0 0 44 0 0 29 20 2015-07-08 0 30 0 0 41 0 0 25 21 2015-07-09 0 30 0 0 41 0 0 25 22 2015-07-10 0 32 0 0 44 0 0 28 23 2015-07-13 0 36 0 0 45 0 0 30 24 2015-07-14 0 35 0 0 46 0 0 31 25 2015-07-15 0 39 0 0 47 0 0 33 26 2015-07-16 0 42 0 0 49 0 0 34 27 2015-07-17 0 45 0 0 50 0 0 33 28 2015-07-17 0 45 0 0 50 0 0 33 In [32]: ### Do not delete the following Markdown section! ### This is the BibTeX references! References &#94; Ondrej Martinsky,. 2013. Portfolio Optimization II : Black-Litterman model . URL if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML'; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: 'center',\" + \" displayIndent: '0em',\" + \" showMathMenu: true,\" + \" tex2jax: { \" + \" inlineMath: [ ['$','$'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }\" + \" } \" + \"}); \"; (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Statistics","title":"Money Management System. Portfolio Optimization Approaches or How to Become a Millionaire from stock tradings"},{"url":"http://igormarfin.github.io/blog/2016/02/08/stochastic-kalman-filter-in-the-finance/","text":"/*! * * IPython notebook * */.ansibold{font-weight:bold}.ansiblack{color:black}.ansired{color:darkred}.ansigreen{color:darkgreen}.ansiyellow{color:#c4a000}.ansiblue{color:darkblue}.ansipurple{color:darkviolet}.ansicyan{color:steelblue}.ansigray{color:gray}.ansibgblack{background-color:black}.ansibgred{background-color:red}.ansibggreen{background-color:green}.ansibgyellow{background-color:yellow}.ansibgblue{background-color:blue}.ansibgpurple{background-color:magenta}.ansibgcyan{background-color:cyan}.ansibggray{background-color:gray}div.cell{border:1px solid transparent;display:-webkit-box;-webkit-box-orient:vertical;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:vertical;-moz-box-align:stretch;display:box;box-orient:vertical;box-align:stretch;display:flex;flex-direction:column;align-items:stretch;border-radius:2px;box-sizing:border-box;-moz-box-sizing:border-box;-webkit-box-sizing:border-box;border-width:thin;border-style:solid;width:100%;padding:5px;margin:0;outline:none}div.cell.selected{border-color:#ababab}@media print{div.cell.selected{border-color:transparent}}.edit_mode div.cell.selected{border-color:green}@media print{.edit_mode div.cell.selected{border-color:transparent}}.prompt{min-width:14ex;padding:.4em;margin:0;font-family:monospace;text-align:right;line-height:1.21429em}@media (max-width:540px){.prompt{text-align:left}}div.inner_cell{display:-webkit-box;-webkit-box-orient:vertical;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:vertical;-moz-box-align:stretch;display:box;box-orient:vertical;box-align:stretch;display:flex;flex-direction:column;align-items:stretch;-webkit-box-flex:1;-moz-box-flex:1;box-flex:1;flex:1}@-moz-document url-prefix(){div.inner_cell{overflow-x:hidden}}div.input_area{border:1px solid #cfcfcf;border-radius:2px;background:#f7f7f7;line-height:1.21429em}div.prompt:empty{padding-top:0;padding-bottom:0}div.unrecognized_cell{padding:5px 5px 5px 0;display:-webkit-box;-webkit-box-orient:horizontal;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:horizontal;-moz-box-align:stretch;display:box;box-orient:horizontal;box-align:stretch;display:flex;flex-direction:row;align-items:stretch}div.unrecognized_cell .inner_cell{border-radius:2px;padding:5px;font-weight:bold;color:red;border:1px solid #cfcfcf;background:#eaeaea}div.unrecognized_cell .inner_cell a{color:inherit;text-decoration:none}div.unrecognized_cell .inner_cell a:hover{color:inherit;text-decoration:none}@media (max-width:540px){div.unrecognized_cell>div.prompt{display:none}}@media print{div.code_cell{page-break-inside:avoid}}div.input{page-break-inside:avoid;display:-webkit-box;-webkit-box-orient:horizontal;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:horizontal;-moz-box-align:stretch;display:box;box-orient:horizontal;box-align:stretch;display:flex;flex-direction:row;align-items:stretch}@media (max-width:540px){div.input{display:-webkit-box;-webkit-box-orient:vertical;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:vertical;-moz-box-align:stretch;display:box;box-orient:vertical;box-align:stretch;display:flex;flex-direction:column;align-items:stretch}}div.input_prompt{color:navy;border-top:1px solid transparent}div.input_area>div.highlight{margin:.4em;border:none;padding:0;background-color:transparent}div.input_area>div.highlight>pre{margin:0;border:none;padding:0;background-color:transparent}.CodeMirror{line-height:1.21429em;font-size:14px;height:auto;background:none}.CodeMirror-scroll{overflow-y:hidden;overflow-x:auto}.CodeMirror-lines{padding:.4em}.CodeMirror-linenumber{padding:0 8px 0 4px}.CodeMirror-gutters{border-bottom-left-radius:2px;border-top-left-radius:2px}.CodeMirror pre{padding:0;border:0;border-radius:0}.highlight-base{}.highlight-variable{}.highlight-variable-2{color:#1a1a1a}.highlight-variable-3{color:#333}.highlight-string{color:#ba2121}.highlight-comment{color:#408080;font-style:italic}.highlight-number{80}.highlight-atom{color:#88f}.highlight-keyword{8000;font-weight:bold}.highlight-builtin{8000}.highlight-error{color:#f00}.highlight-operator{color:#a2f;font-weight:bold}.highlight-meta{color:#a2f}.highlight-def{f}.highlight-string-2{color:#f50}.highlight-qualifier{color:#555}.highlight-bracket{color:#997}.highlight-tag{color:#170}.highlight-attribute{c}.highlight-header{color:blue}.highlight-quote{90}.highlight-link{c}.cm-s-ipython span.cm-keyword{8000;font-weight:bold}.cm-s-ipython span.cm-atom{color:#88f}.cm-s-ipython span.cm-number{80}.cm-s-ipython span.cm-def{f}.cm-s-ipython span.cm-variable{}.cm-s-ipython span.cm-operator{color:#a2f;font-weight:bold}.cm-s-ipython span.cm-variable-2{color:#1a1a1a}.cm-s-ipython span.cm-variable-3{color:#333}.cm-s-ipython span.cm-comment{color:#408080;font-style:italic}.cm-s-ipython span.cm-string{color:#ba2121}.cm-s-ipython span.cm-string-2{color:#f50}.cm-s-ipython span.cm-meta{color:#a2f}.cm-s-ipython span.cm-qualifier{color:#555}.cm-s-ipython span.cm-builtin{8000}.cm-s-ipython span.cm-bracket{color:#997}.cm-s-ipython span.cm-tag{color:#170}.cm-s-ipython span.cm-attribute{c}.cm-s-ipython span.cm-header{color:blue}.cm-s-ipython span.cm-quote{90}.cm-s-ipython span.cm-link{c}.cm-s-ipython span.cm-error{color:#f00}.cm-s-ipython span.cm-tab{background:url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADAAAAAMCAYAAAAkuj5RAAAAAXNSR0IArs4c6QAAAGFJREFUSMft1LsRQFAQheHPowAKoACx3IgEKtaEHujDjORSgWTH/ZOdnZOcM/sgk/kFFWY0qV8foQwS4MKBCS3qR6ixBJvElOobYAtivseIE120FaowJPN75GMu8j/LfMwNjh4HUpwg4LUAAAAASUVORK5CYII=);background-position:right;background-repeat:no-repeat}div.output_wrapper{position:relative;display:-webkit-box;-webkit-box-orient:vertical;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:vertical;-moz-box-align:stretch;display:box;box-orient:vertical;box-align:stretch;display:flex;flex-direction:column;align-items:stretch}div.output_scroll{height:24em;width:100%;overflow:auto;border-radius:2px;-webkit-box-shadow:inset 0 2px 8px rgba(0,0,0,0.8);box-shadow:inset 0 2px 8px rgba(0,0,0,0.8);display:block}div.output_collapsed{margin:0;padding:0;display:-webkit-box;-webkit-box-orient:vertical;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:vertical;-moz-box-align:stretch;display:box;box-orient:vertical;box-align:stretch;display:flex;flex-direction:column;align-items:stretch}div.out_prompt_overlay{height:100%;padding:0 .4em;position:absolute;border-radius:2px}div.out_prompt_overlay:hover{-webkit-box-shadow:inset 0 0 1px #000;box-shadow:inset 0 0 1px #000;background:rgba(240,240,240,0.5)}div.output_prompt{color:darkred}div.output_area{padding:0;page-break-inside:avoid;display:-webkit-box;-webkit-box-orient:horizontal;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:horizontal;-moz-box-align:stretch;display:box;box-orient:horizontal;box-align:stretch;display:flex;flex-direction:row;align-items:stretch}div.output_area .MathJax_Display{text-align:left !important}div.output_area div.output_area .output{display:-webkit-box;-webkit-box-orient:vertical;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:vertical;-moz-box-align:stretch;display:box;box-orient:vertical;box-align:stretch;display:flex;flex-direction:column;align-items:stretch}@media (max-width:540px){div.output_area{display:-webkit-box;-webkit-box-orient:vertical;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:vertical;-moz-box-align:stretch;display:box;box-orient:vertical;box-align:stretch;display:flex;flex-direction:column;align-items:stretch}}div.output_area pre{margin:0;padding:0;border:0;vertical-align:baseline;color:black;background-color:transparent;border-radius:0}div.output_subarea{padding:.4em;-webkit-box-flex:1;-moz-box-flex:1;box-flex:1;flex:1}div.output_text{text-align:left;line-height:1.21429em}div.output_stderr{background:#fdd}div.output_latex{text-align:left}div.output_javascript:empty{padding:0}.js-error{color:darkred}div.raw_input_container{font-family:monospace;padding-top:5px}input.raw_input{font-family:inherit;font-size:inherit;color:inherit;width:auto;vertical-align:baseline;padding:0 .25em;margin:0 .25em}input.raw_input:focus{box-shadow:none}p.p-space{margin-bottom:10px}div.output_unrecognized{padding:5px;font-weight:bold;color:red}div.output_unrecognized a{color:inherit;text-decoration:none}div.output_unrecognized a:hover{color:inherit;text-decoration:none}.rendered_html{}.rendered_html :link{text-decoration:underline}.rendered_html :visited{text-decoration:underline}.rendered_html h1:first-child{margin-top:.538em}.rendered_html h2:first-child{margin-top:.636em}.rendered_html h3:first-child{margin-top:.777em}.rendered_html h4:first-child{margin-top:1em}.rendered_html h5:first-child{margin-top:1em}.rendered_html h6:first-child{margin-top:1em}.rendered_html *+ul{margin-top:1em}.rendered_html *+ol{margin-top:1em}.rendered_html *+table{margin-top:1em}.rendered_html *+p{margin-top:1em}.rendered_html *+img{margin-top:1em}div.text_cell{display:-webkit-box;-webkit-box-orient:horizontal;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:horizontal;-moz-box-align:stretch;display:box;box-orient:horizontal;box-align:stretch;display:flex;flex-direction:row;align-items:stretch}@media (max-width:540px){div.text_cell>div.prompt{display:none}}div.text_cell_render{outline:none;resize:none;width:inherit;border-style:none;padding:.5em .5em .5em .4em;box-sizing:border-box;-moz-box-sizing:border-box;-webkit-box-sizing:border-box}a.anchor-link:link{text-decoration:none;padding:0 20px;visibility:hidden}h1:hover .anchor-link,h2:hover .anchor-link,h3:hover .anchor-link,h4:hover .anchor-link,h5:hover .anchor-link,h6:hover .anchor-link{visibility:visible}.text_cell.rendered .input_area{display:none}.text_cell.unrendered .text_cell_render{display:none}.cm-header-1,.cm-header-2,.cm-header-3,.cm-header-4,.cm-header-5,.cm-header-6{font-weight:bold;font-family:\"Helvetica Neue\",Helvetica,Arial,sans-serif}.cm-header-1{font-size:185.7%}.cm-header-2{font-size:157.1%}.cm-header-3{font-size:128.6%}.cm-header-4{font-size:110%}.cm-header-5{font-size:100%;font-style:italic}.cm-header-6{font-size:100%;font-style:italic}.widget-interact>div,.widget-interact>input{padding:2.5px}.widget-area{page-break-inside:avoid;display:-webkit-box;-webkit-box-orient:horizontal;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:horizontal;-moz-box-align:stretch;display:box;box-orient:horizontal;box-align:stretch;display:flex;flex-direction:row;align-items:stretch}.widget-area .widget-subarea{padding:.44em .4em .4em 1px;margin-left:6px;box-sizing:border-box;-moz-box-sizing:border-box;-webkit-box-sizing:border-box;display:-webkit-box;-webkit-box-orient:vertical;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:vertical;-moz-box-align:stretch;display:box;box-orient:vertical;box-align:stretch;display:flex;flex-direction:column;align-items:stretch;-webkit-box-flex:2;-moz-box-flex:2;box-flex:2;flex:2;-webkit-box-align:start;-moz-box-align:start;box-align:start;align-items:flex-start}.widget-area.connection-problems .prompt:after{content:\"\\f127\";font-family:'FontAwesome';color:#d9534f;font-size:14px;top:3px;padding:3px}.slide-track{border:1px solid #ccc;background:#fff;border-radius:2px}.widget-hslider{padding-left:8px;padding-right:2px;overflow:visible;width:350px;height:5px;max-height:5px;margin-top:13px;margin-bottom:10px;border:1px solid #ccc;background:#fff;border-radius:2px;display:-webkit-box;-webkit-box-orient:horizontal;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:horizontal;-moz-box-align:stretch;display:box;box-orient:horizontal;box-align:stretch;display:flex;flex-direction:row;align-items:stretch}.widget-hslider .ui-slider{border:0;background:none;display:-webkit-box;-webkit-box-orient:horizontal;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:horizontal;-moz-box-align:stretch;display:box;box-orient:horizontal;box-align:stretch;display:flex;flex-direction:row;align-items:stretch;-webkit-box-flex:1;-moz-box-flex:1;box-flex:1;flex:1}.widget-hslider .ui-slider .ui-slider-handle{width:12px;height:28px;margin-top:-8px;border-radius:2px}.widget-hslider .ui-slider .ui-slider-range{height:12px;margin-top:-4px;background:#eee}.widget-vslider{padding-bottom:5px;overflow:visible;width:5px;max-width:5px;height:250px;margin-left:12px;border:1px solid #ccc;background:#fff;border-radius:2px;display:-webkit-box;-webkit-box-orient:vertical;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:vertical;-moz-box-align:stretch;display:box;box-orient:vertical;box-align:stretch;display:flex;flex-direction:column;align-items:stretch}.widget-vslider .ui-slider{border:0;background:none;margin-left:-4px;margin-top:5px;display:-webkit-box;-webkit-box-orient:vertical;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:vertical;-moz-box-align:stretch;display:box;box-orient:vertical;box-align:stretch;display:flex;flex-direction:column;align-items:stretch;-webkit-box-flex:1;-moz-box-flex:1;box-flex:1;flex:1}.widget-vslider .ui-slider .ui-slider-handle{width:28px;height:12px;margin-left:-9px;border-radius:2px}.widget-vslider .ui-slider .ui-slider-range{width:12px;margin-left:-1px;background:#eee}.widget-text{width:350px;margin:0}.widget-listbox{width:350px;margin-bottom:0}.widget-numeric-text{width:150px;margin:0}.widget-progress{margin-top:6px;min-width:350px}.widget-progress .progress-bar{-webkit-transition:none;-moz-transition:none;-ms-transition:none;-o-transition:none;transition:none}.widget-combo-btn{min-width:125px}.widget_item .dropdown-menu li a{color:inherit}.widget-hbox{display:-webkit-box;-webkit-box-orient:horizontal;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:horizontal;-moz-box-align:stretch;display:box;box-orient:horizontal;box-align:stretch;display:flex;flex-direction:row;align-items:stretch}.widget-hbox input[type=\"checkbox\"]{margin-top:9px;margin-bottom:10px}.widget-hbox .widget-label{min-width:10ex;padding-right:8px;padding-top:5px;text-align:right;vertical-align:text-top}.widget-hbox .widget-readout{padding-left:8px;padding-top:5px;text-align:left;vertical-align:text-top}.widget-vbox{display:-webkit-box;-webkit-box-orient:vertical;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:vertical;-moz-box-align:stretch;display:box;box-orient:vertical;box-align:stretch;display:flex;flex-direction:column;align-items:stretch}.widget-vbox .widget-label{padding-bottom:5px;text-align:center;vertical-align:text-bottom}.widget-vbox .widget-readout{padding-top:5px;text-align:center;vertical-align:text-top}.widget-box{box-sizing:border-box;-moz-box-sizing:border-box;-webkit-box-sizing:border-box;-webkit-box-align:start;-moz-box-align:start;box-align:start;align-items:flex-start}.widget-radio-box{display:-webkit-box;-webkit-box-orient:vertical;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:vertical;-moz-box-align:stretch;display:box;box-orient:vertical;box-align:stretch;display:flex;flex-direction:column;align-items:stretch;box-sizing:border-box;-moz-box-sizing:border-box;-webkit-box-sizing:border-box;padding-top:4px}.widget-radio-box label{margin-top:0}.widget-radio{margin-left:20px} .highlight .hll { background-color: #ffffcc } .highlight { background: #f8f8f8; } .highlight .c { color: #408080; font-style: italic } /* Comment */ .highlight .err { border: 1px solid #FF0000 } /* Error */ .highlight .k { color: #008000; font-weight: bold } /* Keyword */ .highlight .o { color: #666666 } /* Operator */ .highlight .cm { color: #408080; font-style: italic } /* Comment.Multiline */ .highlight .cp { color: #BC7A00 } /* Comment.Preproc */ .highlight .c1 { color: #408080; font-style: italic } /* Comment.Single */ .highlight .cs { color: #408080; font-style: italic } /* Comment.Special */ .highlight .gd { color: #A00000 } /* Generic.Deleted */ .highlight .ge { font-style: italic } /* Generic.Emph */ .highlight .gr { color: #FF0000 } /* Generic.Error */ .highlight .gh { color: #000080; font-weight: bold } /* Generic.Heading */ .highlight .gi { color: #00A000 } /* Generic.Inserted */ .highlight .go { color: #888888 } /* Generic.Output */ .highlight .gp { color: #000080; font-weight: bold } /* Generic.Prompt */ .highlight .gs { font-weight: bold } /* Generic.Strong */ .highlight .gu { color: #800080; font-weight: bold } /* Generic.Subheading */ .highlight .gt { color: #0044DD } /* Generic.Traceback */ .highlight .kc { color: #008000; font-weight: bold } /* Keyword.Constant */ .highlight .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */ .highlight .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */ .highlight .kp { color: #008000 } /* Keyword.Pseudo */ .highlight .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */ .highlight .kt { color: #B00040 } /* Keyword.Type */ .highlight .m { color: #666666 } /* Literal.Number */ .highlight .s { color: #BA2121 } /* Literal.String */ .highlight .na { color: #7D9029 } /* Name.Attribute */ .highlight .nb { color: #008000 } /* Name.Builtin */ .highlight .nc { color: #0000FF; font-weight: bold } /* Name.Class */ .highlight .no { color: #880000 } /* Name.Constant */ .highlight .nd { color: #AA22FF } /* Name.Decorator */ .highlight .ni { color: #999999; font-weight: bold } /* Name.Entity */ .highlight .ne { color: #D2413A; font-weight: bold } /* Name.Exception */ .highlight .nf { color: #0000FF } /* Name.Function */ .highlight .nl { color: #A0A000 } /* Name.Label */ .highlight .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */ .highlight .nt { color: #008000; font-weight: bold } /* Name.Tag */ .highlight .nv { color: #19177C } /* Name.Variable */ .highlight .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */ .highlight .w { color: #bbbbbb } /* Text.Whitespace */ .highlight .mb { color: #666666 } /* Literal.Number.Bin */ .highlight .mf { color: #666666 } /* Literal.Number.Float */ .highlight .mh { color: #666666 } /* Literal.Number.Hex */ .highlight .mi { color: #666666 } /* Literal.Number.Integer */ .highlight .mo { color: #666666 } /* Literal.Number.Oct */ .highlight .sb { color: #BA2121 } /* Literal.String.Backtick */ .highlight .sc { color: #BA2121 } /* Literal.String.Char */ .highlight .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */ .highlight .s2 { color: #BA2121 } /* Literal.String.Double */ .highlight .se { color: #BB6622; font-weight: bold } /* Literal.String.Escape */ .highlight .sh { color: #BA2121 } /* Literal.String.Heredoc */ .highlight .si { color: #BB6688; font-weight: bold } /* Literal.String.Interpol */ .highlight .sx { color: #008000 } /* Literal.String.Other */ .highlight .sr { color: #BB6688 } /* Literal.String.Regex */ .highlight .s1 { color: #BA2121 } /* Literal.String.Single */ .highlight .ss { color: #19177C } /* Literal.String.Symbol */ .highlight .bp { color: #008000 } /* Name.Builtin.Pseudo */ .highlight .vc { color: #19177C } /* Name.Variable.Class */ .highlight .vg { color: #19177C } /* Name.Variable.Global */ .highlight .vi { color: #19177C } /* Name.Variable.Instance */ .highlight .il { color: #666666 } /* Literal.Number.Integer.Long */ Kalman Filter in the Finance Igor Marfin < igor.marfin@unister.de > Abstract Here are a few examples of Kalman Filters given to illustrate the prediction mechanism of the Kalman Filtering. Motivation As motivation of this tutorial, let me, to quote Michael Halls-Moore from QuantStart.com , who, in his presenation AN INTRODUCTION TO BACKTESTING WITH PYTHON AND PANDAS , has summarized key points of QUANTITATIVE TRADING : Identification statistically signiﬁcant and repeatable market behaviour that can be exploited to generate proﬁts Forecasting methods attempt to predict the direction or value of an instrument in subsequent future time periods based on certain historical factors. Therefore, today, I will focus on Forecasting of the values of instruments (stocks prices), which plays the significant role in the quantitative trading. Initialization In [1]: import sys sys . path = [ '/usr/local/lib/python2.7/dist-packages' ] + sys . path # to fix the problem with numpy: this replaces 1.6 version by 1.9 % matplotlib inline % pylab inline import os import matplotlib import numpy as np import matplotlib.pyplot as pl import matplotlib as mpl import logging import pymc as pm # a plotter and dataframe modules import seaborn as sns # seaborn to make a nice plots of the data import pandas as pd import scipy.stats as stats # use a nice style for plots and the notebook import json s = json . load ( open ( \"styles/my_matplotlibrc.json\" ) ) matplotlib . rcParams . update ( s ) from IPython.core.display import HTML from IPython.display import display , Math , Latex import urllib2 def css_styling (): styles = open ( \"styles/custom_v3.css\" , \"r\" ) . read () return HTML ( styles ) css_styling () #HTML( urllib2.urlopen('http://bit.ly/1Bf5Hft').read() ) ion () # Set up logging. logger = logging . getLogger () logger . setLevel ( logging . INFO ) Populating the interactive namespace from numpy and matplotlib /usr/local/lib/python2.7/dist-packages/matplotlib/__init__.py:855: UserWarning: svg.embed_char_paths is deprecated and replaced with svg.fonttype; please use the latter. warnings.warn(self.msg_depr % (key, alt_key)) Stock Prices For this exercise, we will be examining the daily 'closed' prices of the GOOG (google). In [2]: import datetime import ystockquote as ysq stocks = [ \"GOOG\" ] enddate = datetime . datetime . now () . strftime ( \"%Y-%m- %d \" ) # today's date. startdate = \"2014-09-01\" #startdate = \"2015-06-05\" stock_closes = {} import operator for stock in stocks : data = np . array ( sorted ( ysq . get_historical_prices ( stock , startdate , enddate ) . items (), key = operator . itemgetter ( 0 ))) x = data [:, 1 , None ] stock_closes [ stock ] = np . apply_along_axis ( lambda x : float ( x [ 0 ][ 'Close' ]), 1 , x ) n_observations = len ( stock_closes [ stock ]) dates = map ( lambda x : datetime . datetime . strptime ( x , \"%Y-%m- %d \" ), data [:, 0 ][ 1 : n_observations + 1 ]) figsize ( 12.5 , 4 ) plt . xticks ( np . arange ( n_observations )[:: 8 ], map ( lambda x : datetime . datetime . strftime ( x , \"%Y-%m- %d \" ), dates [:: 8 ]), rotation = 60 ); plt . plot ( stock_closes [ 'GOOG' ], label = 'GOOGLE Close Prices' ) plt . legend ( loc = \"upper left\" ) plt . title ( \"Close Prices\" ) plt . ylabel ( \"in USA($)\" ); pl . savefig ( 'plots/google_prices.png' ) Realization of the Kalman Filtering model (Part 1) In this part we will develop the kalman filter for the time series of the stock price GOOG . We will follow the prescription given in the wiki about Kalman Filter application . First, we introduce the model as follows Consider a price as a truck on frictionless, straight rails. Initially the price is stationary at some position (say, stock_closes['GOOG'].mean() ), but it is buffeted this way and that by random uncontrolled forces. We measure the position of the price every Δt days, but these measurements are imprecise; we want to maintain a model of where the price is and what its velocity is. We show here how we derive the model from which we create our Kalman filter. The position and velocity of the price are described by the linear state space $$\\mathbf{x}_{k} = \\begin{bmatrix} x_k \\\\ \\dot{x}_k \\end{bmatrix} $$ where $\\dot{x}_k$ is the velocity, that is, the derivative of price with respect to time. We assume that between the $(k − 1)$ and $k$ timestep uncontrolled forces cause a constant acceleration of $a_k$ that is normally distributed, with mean 0 and standard deviation $\\sigma_a$. Let's asume that the analogue of Newton's laws of motion can be applied $$\\mathbf{x}_{k} = \\mathbf{F} \\mathbf{x}_{k-1} + \\mathbf{G}a_{k}.$$ We assume that $a_k$ is the effect of an unknown input and $\\mathbf{G}$ applies that effect to the state vector. Here $$\\mathbf{F} = \\begin{bmatrix} 1 & \\Delta t \\\\ 0 & 1 \\end{bmatrix}$$ and $$\\mathbf{G} = \\begin{bmatrix} \\frac{\\Delta t&#94;2}{2} \\\\[6pt] \\Delta t \\end{bmatrix} $$ Thus the price state $x_k$ is defined as $$\\mathbf{x}_{k} = \\mathbf{F} \\mathbf{x}_{k-1} + \\mathbf{w}_{k},$$ where $$\\mathbf{w}_{k} \\sim N(0, \\mathbf{Q})$$ and $$\\mathbf{Q}=\\mathbf{G}\\mathbf{G}&#94;{\\text{T}}\\sigma_a&#94;2 =\\begin{bmatrix} \\frac{\\Delta t&#94;4}{4} & \\frac{\\Delta t&#94;3}{2} \\\\[6pt] \\frac{\\Delta t&#94;3}{2} & \\Delta t&#94;2 \\end{bmatrix}\\sigma_a&#94;2.$$ At each time step, a noisy measurement of the true position of the price is made. Let us suppose the measurement noise $v_k$ is also normally distributed, with mean 0 and standard deviation $\\sigma_v$: $$\\mathbf{z}_{k} = \\mathbf{H x}_{k} + \\mathbf{v}_{k},$$ where $H$ is the transformation matrix (telling us that we measure only the price but not the velocity) $$\\mathbf{H} = \\begin{bmatrix} 1 & 0 \\end{bmatrix}, $$ and $$\\mathbf{R} = \\textrm{E}[\\mathbf{v}_k \\mathbf{v}_k&#94;{\\text{T}}] = \\begin{bmatrix} \\sigma_v&#94;2 \\end{bmatrix}.$$ The KalmanFilter class defines our model. In [3]: class BasicKalmanFilter ( object ): ''' Stochastic Kalman Filter to model 'Close' prices of the stock market ''' def __init__ ( self , data , F_init , G_init , H_init , P_init , x_init , acc_variance = 100. , obs_variance = 100. , delta_t = 0.5 ): # Specify the model. logging . info ( 'building the KalmanFilter model' ) # data to be tested self . data = data # a state transition model: it's stable and doesn't depend on x_k self . F = F_init # Effect from unknown forces a_k self . G = G_init # x is our state vector self . x = x_init # acc_variance is variance of the unknown forces a_k self . acc_variance = acc_variance # obs_variance is variance of the measurement of the 'close' price self . obs_variance = obs_variance # delta_t is the typical period of the action of the force during the trading day: # delta_t = 0.5 -> means that the force acts on the price during the first half of the trading day. self . delta_t = delta_t # a dim. of the hidden state self . dim = x_init . shape [ 0 ] # a control input: this Kalman Filter is designed w/o use of the control input self . B = np . zeros (( self . dim , self . dim )) self . u = np . zeros (( self . dim , 1 )) # the Q matrix: the covariance matrix of the state transition model, i.e Q=G&#94;T.G*\\sigma_a&#94;2 # explicitelly , it is #self.Q = np.array([ [self.delta_t**4/4, self.delta_t**3/2], [self.delta_t**3/2, self.delta_t**2]])*self.acc_variance self . Q = self . G . T * self . G * self . acc_variance # the H matrix: the observation model self . H = H_init # Predicted (a priori) estimate covariance self . P = P_init # the matrix R is the covariance of measurement self . R = np . array ( [[ self . obs_variance ]]) logging . info ( 'done building the KalmanFilter model' ) def predict ( self ): raise NotImplementedError ( 'KalmanFilter prediction is not implemented ' ) def update ( self , z ): raise NotImplementedError ( 'KalmanFilter update is not implemented ' ) def __str__ ( self ): return self . __class__ . __name__ I have called this section as 'Part 1' because, we want to test the Kalman Filter on timeseries. In [4]: # Kalman Filter of the stochastic price process as a function of the time class KalmanFilter ( BasicKalmanFilter ): pass In [5]: # Test of Kalaman Filter Class # a transition model: We measure the state each day -> delta_t = 1. delta_t = 1. F_init = np . array ([[ 1 , delta_t ],[ 0 , 1 ]]) # an 'acting-force' model G_init = np . array ([[ delta_t ** 2 / 2 ], [ delta_t ]]) # an observation model H_init = np . array ([[ 1 , 0 ]]) # an initial state: the price and its velocity x_init = np . array ([[ stock_closes [ 'GOOG' ][ - 100 ]],[ 0. ]]) # a priory covariance matrix # If the initial price and velocity are not known perfectly # the covariance matrix should be initialized with a suitably large number, say L, on its diagonal. P_init = np . array ([[ 1 , 0 ],[ 0 , 100. ]]) # L=100. kf = KalmanFilter ( stock_closes [ 'GOOG' ][ - 100 :], F_init , G_init , H_init , P_init , x_init , acc_variance = 100. , obs_variance = 100. , delta_t = delta_t ) ? kf INFO:root:building the KalmanFilter model INFO:root:done building the KalmanFilter model Let's add predict/update support to our model. In [6]: from numpy.linalg import inv def _predict ( kalman_model ): ''' predicts the price using kalman_model ''' # the state vector should be of the one-column type assert kalman_model . x . shape [ 1 ] == 1 # check the consistency of the transition model assert kalman_model . F . shape [ 0 ] == kalman_model . x . shape [ 0 ] # check the consistency of the control-input model assert kalman_model . u . shape == kalman_model . x . shape assert kalman_model . B . shape [ 0 ] == kalman_model . u . shape [ 0 ] # check the consistency of the control-input model assert kalman_model . F . shape [ 1 ] == kalman_model . P . shape [ 0 ] assert kalman_model . Q . shape == kalman_model . P . shape x_p = np . dot ( kalman_model . F , kalman_model . x ) + np . dot ( kalman_model . B , kalman_model . u ) P_p = np . dot ( kalman_model . F , kalman_model . P ) . dot ( kalman_model . F . T ) + kalman_model . Q assert kalman_model . x . shape == x_p . shape assert P_p . shape == kalman_model . P . shape return x_p , P_p def _update ( kalman_model , z ): ''' updates the price using kalman_model ''' # make a prediction x_p , P_p = kalman_model . predict () assert kalman_model . H . shape [ 1 ] == x_p . shape [ 0 ] assert kalman_model . H . shape [ 1 ] == P_p . shape [ 0 ] assert kalman_model . R . shape [ 0 ] == kalman_model . H . shape [ 0 ] # check the consistency of the measured data assert z . shape [ 1 ] == 1 assert z . shape [ 0 ] == kalman_model . H . shape [ 0 ] y = z - dot ( kalman_model . H , x_p ) S = np . dot ( kalman_model . H , P_p ) . dot ( kalman_model . H . T ) + kalman_model . R K = np . dot ( P_p , kalman_model . H . T ) . dot ( inv ( S )) x_u = x_p + np . dot ( K , y ) P_u = ( np . eye ( K . shape [ 0 ]) - np . dot ( K , kalman_model . H )) . dot ( P_p ) # save updated parametes of the model kalman_model . x = x_u kalman_model . P = P_u return x_u , P_u # Update our class with the predict/update infrastructure KalmanFilter . predict = _predict KalmanFilter . update = _update In [7]: # predict and update the first two data inputs: # timestamp 0 x_p , P_p = kf . predict () print '0: price_pred=' , x_p print '0: Cov_Mat:' , kf . P print '0: Cov_Mat_pred=' , P_p # timestamp 0 x_u , P_u = kf . update ( np . array ([[ stock_closes [ 'GOOG' ][ - 100 ]]])) print '0: price_update=' , x_u print '0: Cov_Mat_update=' , P_u 0: price_pred= [[ 635.140015] [ 0. ]] 0: Cov_Mat: [[ 1. 0.] [ 0. 100.]] 0: Cov_Mat_pred= [[ 126. 150.] [ 150. 200.]] 0: price_update= [[ 635.140015] [ 0. ]] 0: Cov_Mat_update= [[ 55.75221239 66.37168142] [ 66.37168142 100.44247788]] In [8]: # timestamp 1 x_p , P_p = kf . predict () print '1: price_pred=' , x_p print '1: Cov_Mat:' , kf . P print '1: Cov_Mat_pred=' , P_p # timestamp 1 x_u , P_u = kf . update ( np . array ([[ stock_closes [ 'GOOG' ][ - 99 ]]])) print '1: price_update=' , x_u print '1: Cov_Mat_update=' , P_u 1: price_pred= [[ 635.140015] [ 0. ]] 1: Cov_Mat: [[ 55.75221239 66.37168142] [ 66.37168142 100.44247788]] 1: Cov_Mat_pred= [[ 313.9380531 216.81415929] [ 216.81415929 200.44247788]] 1: price_update= [[ 6.35777060e+02] [ 4.39960289e-01]] 1: Cov_Mat_update= [[ 75.84179583 52.37840727] [ 52.37840727 86.87867451]] Now, let's try our KalmanFilter on the whole dataseries. In [9]: def _draw_predicted ( kalman_model , xticks = None , steps_jump = 8 ): ''' draws predicted prices ''' time_steps = len ( kalman_model . data ) price_guesses = np . zeros (( 2 , time_steps )) price_predicted = np . zeros (( 2 , time_steps )) price_actual = np . zeros (( 2 , time_steps )) covariance_matrix = np . zeros ( time_steps ) for i in range ( time_steps ): x_p , P_p = kalman_model . predict () x_g , P_g = kalman_model . update ( np . array ([[ kalman_model . data [ i ]]])) price_guesses [:, i ] = x_g [:, 0 ] covariance_matrix [ i ] = P_p [ 0 ][ 0 ] price_predicted [:, i ] = x_p [:, 0 ] price_actual [:, i ] = np . array ([[ kalman_model . data [ i ]],[ 0. ]])[:, 0 ] # Here 0. --> observed velocity :-) guesses , actual , predicted , std = price_guesses [ 0 , 2 :], price_actual [ 0 , 2 :], price_predicted [ 0 , 2 :], covariance_matrix [ 2 :] plt . plot ( actual , label = 'actual' ) plt . plot ( guesses , label = 'guesses' ) plt . plot ( predicted , label = 'predicted' ) std = np . sqrt ( std ) plt . plot ( predicted + std , c = 'k' , ls = ':' , lw = 2 , label = '68% CL' ) plt . plot ( predicted - std , c = 'k' , ls = ':' , lw = 2 ) plt . fill_between ( range ( len ( predicted )), predicted + std , predicted - std , facecolor = 'yellow' , alpha = 0.2 , label = '68% CL' ) if ( xticks is not None ): plt . xticks ( np . arange ( time_steps )[:: steps_jump ], xticks , rotation = 60 ) plt . legend ( loc = \"upper left\" ) plt . title ( \"Close Prices\" ) plt . ylabel ( \"in USA($)\" ); # update the KalmanFilter class definition KalmanFilter . draw_predicted = _draw_predicted You can test different initilal prices of the model. They initial point doesn't make an influence on the convergence. In [11]: # it seems that it doesn't depend much on the initial conditions #P_init = np.array([[10000, 0],[0,10000.]]) # L=10000. # Here the true init values for a price is used kf = KalmanFilter ( stock_closes [ 'GOOG' ][ - 100 :], F_init , G_init , H_init , P_init , x_init , acc_variance = 1. , obs_variance = 100. , delta_t = delta_t ) # If we don't know the initial price, assume it is zero #kf = KalmanFilter(stock_closes['GOOG'][-100:],F_init,G_init,H_init,P_init,np.zeros((2,1)),acc_variance=10000., obs_variance=10000.,delta_t = 200*100) steps_jump = 8 xticks = map ( lambda x : datetime . datetime . strftime ( x , \"%Y-%m- %d \" ), dates [:: steps_jump ]) kf . draw_predicted ( xticks , steps_jump ) INFO:root:building the KalmanFilter model INFO:root:done building the KalmanFilter model Please, you can play with the covariance matrix of the state transition model 'acc_variance' and the covariant matrix of the observation model 'obs_variance' to see the change of the error band on the predicted price. Also, it is interesting to plot the convergence of the hidden state $x_k$, the convergence is charachterized by 'cov[0,0]' : In [12]: def _draw_cov ( kalman_model , xticks = None , steps_jump = 8 ): ''' draws predicted prices ''' time_steps = len ( kalman_model . data ) covariance_matrix = np . zeros ( time_steps ) for i in range ( time_steps ): x_p , P_p = kalman_model . predict () x_g , P_g = kalman_model . update ( np . array ([[ kalman_model . data [ i ]]])) covariance_matrix [ i ] = P_p [ 0 ][ 0 ] covariance_matrix = np . sqrt ( covariance_matrix ) plt . plot ( covariance_matrix ) if ( xticks is not None ): plt . xticks ( np . arange ( time_steps )[:: steps_jump ], xticks , rotation = 60 ) plt . legend ( loc = \"upper left\" ) plt . title ( \"Divergence of the hidden state in Close Prices\" ) plt . ylabel ( \"in USA($)\" ); # update the KalmanFilter class definition KalmanFilter . draw_cov = _draw_cov kf = KalmanFilter ( stock_closes [ 'GOOG' ][ - 100 :], F_init , G_init , H_init , P_init , np . array ([[ 1e-9 ],[ 0. ]]), acc_variance = 1. , obs_variance = 100. , delta_t = delta_t ) steps_jump = 8 xticks = map ( lambda x : datetime . datetime . strftime ( x , \"%Y-%m- %d \" ), dates [:: steps_jump ]) kf . draw_cov ( xticks , steps_jump ) INFO:root:building the KalmanFilter model INFO:root:done building the KalmanFilter model /usr/local/lib/python2.7/dist-packages/matplotlib/axes/_axes.py:475: UserWarning: No labelled objects found. Use label='...' kwarg on individual plots. warnings.warn(\"No labelled objects found. \" It would be nice to introduce the metric which would give us the quality of the KalmanFilter. Such metric is MASE: MEAN-ABSOLUTE SCALED ERROR In [13]: # Here we introduce the metric we want to test def MASE ( training_series , testing_series , prediction_series ): \"\"\" Computes the MEAN-ABSOLUTE SCALED ERROR forcast error for univariate time series prediction. See \"Another look at measures of forecast accuracy\", Rob J Hyndman parameters: training_series: the series used to train the model, 1d numpy array testing_series: the test series to predict, 1d numpy array or float prediction_series: the prediction of testing_series, 1d numpy array (same size as testing_series) or float absolute: \"squares\" to use sum of squares and root the result, \"absolute\" to use absolute values. \"\"\" n = training_series . shape [ 0 ] d = np . abs ( np . diff ( training_series ) ) . sum () / ( n - 1 ) errors = np . abs ( testing_series - prediction_series ) #errors = testing_series - prediction_series return errors . mean () / d , errors In [14]: # Check the metric with our model kf = KalmanFilter ( stock_closes [ 'GOOG' ][ - 100 :], F_init , G_init , H_init , P_init , x_init , acc_variance = 10000. , obs_variance = 10000. , delta_t = 200 * 100 ) time_steps = len ( kf . data ) price_guesses = np . zeros (( 2 , time_steps )) price_actual = np . zeros (( 2 , time_steps )) for i in range ( time_steps ): x_g , P_g = kf . update ( np . array ([[ kf . data [ i ]]])) price_guesses [:, i ] = x_g [:, 0 ] price_actual [:, i ] = np . array ([[ kf . data [ i ]],[ 0. ]])[:, 0 ] # Here 0. --> observed velocity :-) guesses , actual = price_guesses [ 0 ,:], price_actual [ 0 ,:] print \"MASE=\" , MASE ( actual , actual , guesses )[ 0 ] print 'MAX MASE=' , np . max ( MASE ( actual , actual , guesses )[ 1 ]), ' ' , np . argmax ( MASE ( actual , actual , guesses )[ 1 ]) print 'MIN MASE=' , np . min ( MASE ( actual , actual , guesses )[ 1 ]), ' ' , np . argmin ( MASE ( actual , actual , guesses )[ 1 ]) print 'MEAN MASE=' , MASE ( actual , actual , guesses )[ 1 ] . mean () plot ( MASE ( actual , actual , guesses )[ 1 ]) INFO:root:building the KalmanFilter model INFO:root:done building the KalmanFilter model MASE= 0.291192291092 MAX MASE= 13.8442899469 28 MIN MASE= 0.0 0 MEAN MASE= 2.70623565356 Out[14]: [<matplotlib.lines.Line2D at 0xd5c4d6c>] OK. It looks like we do in the right direction. We can try to simplify the transition model of the Kalman Filter removing the velocity of the price, and test again. In [15]: # Kalman Filter of the stochastic price process as a function of the time class KalmanFilterSimplified ( BasicKalmanFilter ): pass # Update our class with the predict/update infrastructure KalmanFilterSimplified . predict = _predict KalmanFilterSimplified . update = _update In [16]: # a transition model: We measure the state each day -> delta_t = 1. delta_t = 1. F_init = np . array ([[ 1 ]]) # an 'acting-force' model G_init = np . array ([[ delta_t ** 2 / 2 ]]) # an observation model H_init = np . array ([[ 1 ]]) # an initial state: the price x_init = np . array ([[ stock_closes [ 'GOOG' ][ - 100 ]]]) # a priory covariance matrix # If the initial price and velocity are not known perfectly # the covariance matrix should be initialized with a suitably large number, say L, on its diagonal. P_init = np . array ([[ 1 ]]) # kf_simple = KalmanFilterSimplified ( stock_closes [ 'GOOG' ][ - 100 :], F_init , G_init , H_init , P_init , x_init , acc_variance = 100. , obs_variance = 100. , delta_t = delta_t ) ? kf_simple INFO:root:building the KalmanFilter model INFO:root:done building the KalmanFilter model If we use the same covariance of the transition model, i.e acc_variance=1. , the KF will do the bad job. It's clear from the plot belows: In [17]: # predict and update the first two data inputs: KalmanFilterSimplified . draw_predicted = _draw_predicted # compare acc_variance=1. and acc_variance=100. #kf_simple = KalmanFilterSimplified(stock_closes['GOOG'][-100:],F_init,G_init,H_init,P_init,x_init,acc_variance=100., obs_variance=100.,delta_t =delta_t) kf_simple = KalmanFilterSimplified ( stock_closes [ 'GOOG' ][ - 100 :], F_init , G_init , H_init , P_init , x_init , acc_variance = 1. , obs_variance = 100. , delta_t = delta_t ) # If we don't know the initial price, assume it is zero #kf = KalmanFilter(stock_closes['GOOG'][-100:],F_init,G_init,H_init,P_init,np.zeros((2,1)),acc_variance=10000., obs_variance=10000.,delta_t = 200*100) steps_jump = 8 xticks = map ( lambda x : datetime . datetime . strftime ( x , \"%Y-%m- %d \" ), dates [:: steps_jump ]) kf_simple . draw_predicted ( xticks , steps_jump ) INFO:root:building the KalmanFilter model INFO:root:done building the KalmanFilter model The MASE is degreded by the factor 5. In [18]: # Check the metric with our model kf_simple = KalmanFilterSimplified ( stock_closes [ 'GOOG' ][ - 100 :], F_init , G_init , H_init , P_init , x_init , acc_variance = 1. , obs_variance = 100. , delta_t = delta_t ) time_steps = len ( kf_simple . data ) price_guesses = np . zeros (( 2 , time_steps )) price_actual = np . zeros (( 2 , time_steps )) for i in range ( time_steps ): x_g , P_g = kf_simple . update ( np . array ([[ kf_simple . data [ i ]]])) price_guesses [:, i ] = x_g [:, 0 ] price_actual [:, i ] = np . array ([[ kf_simple . data [ i ]],[ 0. ]])[:, 0 ] # Here 0. --> observed velocity :-) guesses , actual = price_guesses [ 0 ,:], price_actual [ 0 ,:] print \"MASE=\" , MASE ( actual , actual , guesses )[ 0 ] print 'MAX MASE=' , np . max ( MASE ( actual , actual , guesses )[ 1 ]), ' ' , np . argmax ( MASE ( actual , actual , guesses )[ 1 ]) print 'MIN MASE=' , np . min ( MASE ( actual , actual , guesses )[ 1 ]), ' ' , np . argmin ( MASE ( actual , actual , guesses )[ 1 ]) print 'MEAN MASE=' , MASE ( actual , actual , guesses )[ 1 ] . mean () plot ( MASE ( actual , actual , guesses )[ 1 ]) INFO:root:building the KalmanFilter model INFO:root:done building the KalmanFilter model MASE= 3.07087289708 MAX MASE= 67.497310677 29 MIN MASE= 0.0 0 MEAN MASE= 28.5395801189 Out[18]: [<matplotlib.lines.Line2D at 0xaa15ac6c>] In [19]: # update the KalmanFilter class definition KalmanFilterSimplified . draw_cov = _draw_cov kf_simple = KalmanFilterSimplified ( stock_closes [ 'GOOG' ][ - 100 :], F_init , G_init , H_init , P_init , x_init , acc_variance = 1. , obs_variance = 100. , delta_t = delta_t ) steps_jump = 8 xticks = map ( lambda x : datetime . datetime . strftime ( x , \"%Y-%m- %d \" ), dates [:: steps_jump ]) kf_simple . draw_cov ( xticks , steps_jump ) INFO:root:building the KalmanFilter model INFO:root:done building the KalmanFilter model Realization of the Kalman Filtering model (Part 2) Another way of the use of the Kalman Filtering technique is the Online Linear Regression. Linear regression is useful for many financial applications such as finding the hedge ratio between two assests in a pair trade. In a perfect world, the realtionship between assests would remain constant along with the slope and intercet of a linear regression. Unfortutanely this is usually the exception rather than the rule. In this part, I'm going to show you how to use a Kalman filter for online linear regression that calculates the time-varying slope and intercept. For this example, I use two related ETF's, the iShares MSCI Australia (EWA) and iShares MSCI Canada (EWC). Let's plot them for the requested period. In [20]: from pandas.io.data import DataReader stocks = [ \"EWA\" , \"EWC\" ] enddate = datetime . datetime . now () . strftime ( \"%Y-%m- %d \" ) # today's date. startdate = \"2014-09-01\" data = DataReader ( stocks , 'yahoo' , startdate , enddate )[ 'Close' ] # visualize the correlation between assest prices over time cm = plt . cm . get_cmap ( 'jet' ) dates = [ str ( p . date ()) for p in data [:: len ( data ) // 10 ] . index ] colors = np . linspace ( 0.1 , 1 , len ( data )) sc = plt . scatter ( data [ data . columns [ 0 ]], data [ data . columns [ 1 ]], s = 30 , c = colors , cmap = cm , edgecolor = 'k' , alpha = 0.7 ) cb = plt . colorbar ( sc ) cb . ax . set_yticklabels ([ str ( p . date ()) for p in data [:: len ( data ) // 9 ] . index ]); plt . xlabel ( data . columns [ 0 ]) plt . ylabel ( data . columns [ 1 ]) pl . savefig ( 'plots/ewa_ewc_prices.png' ) In [21]: stocks = [ \"EWA\" , \"EWC\" ] enddate = datetime . datetime . now () . strftime ( \"%Y-%m- %d \" ) # today's date. startdate = \"2014-09-01\" #startdate = \"2015-06-05\" stock_closes = {} import operator for stock in stocks : data = np . array ( sorted ( ysq . get_historical_prices ( stock , startdate , enddate ) . items (), key = operator . itemgetter ( 0 ))) x = data [:, 1 , None ] stock_closes [ stock ] = np . apply_along_axis ( lambda x : float ( x [ 0 ][ 'Close' ]), 1 , x ) n_observations = len ( stock_closes [ stock ]) dates = map ( lambda x : datetime . datetime . strptime ( x , \"%Y-%m- %d \" ), data [:, 0 ][ 1 : n_observations + 1 ]) What is the idea behind of application of the KF as an online Linear Regression? I will follow this discussion and this post where the topic was investigated. Before we continue with the Kalman filter, recall the equation for a linear regression $$ a_k=\\beta \\cdot b_k+\\alpha $$ where $a_k$ and $b_k$ is the closing price of EWC and EWA respectively and $\\beta$ and $\\alpha$ is the slope and intercept. The Kalman Model is the following: $$\\mathbf{x}_{k} = \\begin{bmatrix} \\beta_k \\\\ \\alpha_k \\end{bmatrix}, $$$$\\mathbf{x}_{k} = \\mathbf{F} \\mathbf{x}_{k-1} + \\mathbf{w}_{k},$$ where $$\\mathbf{w}_{k} \\sim N(0, \\mathbf{Q}).$$ The transition matrix $F_k$ is defined as $$\\mathbf{F}_{k} = \\begin{bmatrix} \\hat{I}, \\,\\, \\hat{I} \\end{bmatrix}, $$ The observation matrix $H_k$ is also quite easy: $$\\mathbf{H}_{k} = \\begin{bmatrix} price&#94;{EWA}_k, \\,\\, 1 \\end{bmatrix}, $$ finally, the observation $z_k$ is the price of EWC $$\\mathbf{z}_{k} = \\mathbf{H x}_{k} + \\mathbf{v}_{k},$$ and $$ \\mathbf{w}_{k} \\sim N(0, \\mathbf{R}) \\\\ \\mathbf{R} = \\textrm{E}[\\mathbf{v}_k \\mathbf{v}_k&#94;{\\text{T}}] = \\begin{bmatrix} \\sigma_v&#94;2 \\end{bmatrix}.$$ WARNING: Here we should understand that $\\mathbf{H}_{k} $ observation matrix is not constan and it develops in timeseries. That means that we need to update $\\mathbf{H}_{k} $ each time. The new Kalman Filter class for Online Linear Regression is inherited from the basic one. The only difference here is the update method. In [22]: from numpy.linalg import inv def _update ( kalman_model , z , Hk ): ''' updates the price using kalman_model ''' # make a prediction x_p , P_p = kalman_model . predict () # a new feature is here # assign a new observation matrix Hk to the observation model kalman_model . H = Hk assert kalman_model . H . shape [ 1 ] == x_p . shape [ 0 ] assert kalman_model . H . shape [ 1 ] == P_p . shape [ 0 ] assert kalman_model . R . shape [ 0 ] == kalman_model . H . shape [ 0 ] # check the consistency of the measured data assert z . shape [ 1 ] == 1 assert z . shape [ 0 ] == kalman_model . H . shape [ 0 ] y = z - dot ( kalman_model . H , x_p ) S = np . dot ( kalman_model . H , P_p ) . dot ( kalman_model . H . T ) + kalman_model . R K = np . dot ( P_p , kalman_model . H . T ) . dot ( inv ( S )) x_u = x_p + np . dot ( K , y ) P_u = ( np . eye ( K . shape [ 0 ]) - np . dot ( K , kalman_model . H )) . dot ( P_p ) # save updated parametes of the model kalman_model . x = x_u kalman_model . P = P_u return x_u , P_u # Online Linear Regression of the stochastic price process as a function of the another price class OLR ( BasicKalmanFilter ): pass # Update our class with the predict/update infrastructure OLR . predict = _predict OLR . update = _update In [23]: # Test of OLR Class # a transition model: delta_t is obsolete now. But it's kept for compatibility. delta_t = 1. F_init = np . array ([[ 1 , 0 ],[ 0 , 1 ]]) # G matrix does n't depend on delta_t G_init = np . array ([[ 1 ], [ 1 ]]) # an observation model H_init = np . array ([[ stock_closes [ 'EWA' ][ - 100 ], 1 ]]) # an initial state: let's assume zeros for a slope and a intercept. x_init = np . array ([[ 0. ],[ 0. ]]) # a priory covariance matrix P_init = np . array ([[ 1. , 0. ],[ 0 , 1. ]]) olr = OLR ( stock_closes [ 'EWC' ][ - 100 :], F_init , G_init , H_init , P_init , x_init , acc_variance = 100. , obs_variance = 100. , delta_t = delta_t ) # a hack : use the symmetric matrix Q olr . Q = np . array ([[ 100 , 0 ],[ 0 , 100 ]]) ? olr INFO:root:building the KalmanFilter model INFO:root:done building the KalmanFilter model In [24]: # predict and update the first two data inputs in a 'manual' mode: # timestamp 0 x_p , P_p = olr . predict () print '0: price_pred=' , x_p print '0: Cov_Mat:' , olr . P print '0: Cov_Mat_pred=' , P_p # timestamp 0 x_u , P_u = olr . update ( np . array ([[ stock_closes [ 'EWC' ][ - 100 ]]]), np . array ([[ stock_closes [ 'EWA' ][ - 100 ], 1 ]]) ) print '0: price_update=' , x_u print '0: Cov_Mat_update=' , P_u 0: price_pred= [[ 0.] [ 0.]] 0: Cov_Mat: [[ 1. 0.] [ 0. 1.]] 0: Cov_Mat_pred= [[ 101. 0.] [ 0. 101.]] 0: price_update= [[ 1.26510958] [ 0.0686813 ]] 0: Cov_Mat_update= [[ 0.588948 -5.45119718] [ -5.45119718 100.70406096]] In [25]: # timestamp 1 x_p , P_p = olr . predict () print '1: price_pred=' , x_p print '1: Cov_Mat:' , olr . P print '1: Cov_Mat_pred=' , P_p # timestamp 1 x_u , P_u = olr . update ( np . array ([[ stock_closes [ 'EWC' ][ - 99 ]]]), np . array ([[ stock_closes [ 'EWA' ][ - 99 ], 1 ]]) ) print '1: price_update=' , x_u print '1: Cov_Mat_update=' , P_u 1: price_pred= [[ 1.26510958] [ 0.0686813 ]] 1: Cov_Mat: [[ 0.588948 -5.45119718] [ -5.45119718 100.70406096]] 1: Cov_Mat_pred= [[ 100.588948 -5.45119718] [ -5.45119718 200.70406096]] 1: price_update= [[ 1.27462047] [ 0.06917136]] 1: Cov_Mat_update= [[ 0.83877663 -10.59090508] [ -10.59090508 200.43923337]] Now, let's try our new KalmanFilter on the whole dataseries. We introduce the draw function for this purpose. In [26]: def _draw_LR ( kalman_model , Hk , slope = True , xticks = None , steps_jump = 8 ): ''' to draw the linear regression parameters over the timeseries ''' time_steps = len ( kalman_model . data ) params_guesses = np . zeros (( time_steps , 2 )) params_predicted = np . zeros (( time_steps , 2 )) for i in range ( time_steps ): x_p , P_p = kalman_model . predict () x_g , P_g = kalman_model . update ( np . array ([[ kalman_model . data [ i ]]]), np . array ([[ Hk [ i ], 1 ]])) params_guesses [ i ] = x_g [:, 0 ] params_predicted [ i ] = x_p [:, 0 ] title = 'slope' index = 0 if ( not slope ): title = 'intercept' index = 1 figsize ( 12.5 , 4 ) plt . plot ( params_guesses [ 2 :, index ], label = 'guess' ) plt . plot ( params_predicted [ 2 :, index ], label = 'predicted' ) if ( xticks is not None ): plt . xticks ( np . arange ( time_steps )[:: steps_jump ], xticks , rotation = 60 ) plt . legend ( loc = \"upper left\" ) plt . title ( title ) # Update our class OLR . draw_LR = _draw_LR In [27]: steps_jump = 8 xticks = map ( lambda x : datetime . datetime . strftime ( x , \"%Y-%m- %d \" ), dates [:: steps_jump ]) olr . draw_LR ( stock_closes [ 'EWA' ][ - 100 :], True , xticks , steps_jump ) In [28]: olr . draw_LR ( stock_closes [ 'EWA' ][ - 100 :], False , xticks , steps_jump ) You can compare the plots with results shown in this post http://nbviewer.ipython.org/github/aidoom/aidoom.github.io/blob/master/notebooks/2014-08-13-online_linear_regression_kalman_filter.ipynb The reason of Kalman Filter in the Finance: It is an important part of the trading strategies. It is time to show the simple backtesting the trading strategy on the stock market. I'm going to use the pybacktest to compare a KalmanFilter-based strategy with the strategy based on the Momentum or trend following Simple Moving Average ( SMA )of the price. First, we need to download pybacktest : In [29]: ! git clone https://github.com/ematvey/pybacktest.git fatal: destination path 'pybacktest' already exists and is not an empty directory. In [30]: sys . path = [ os . getcwd () + '/pybacktest' ] + sys . path import pybacktest # get data: GOOG prices ohlc = pybacktest . load_from_yahoo ( 'GOOG' , start = \"2014-09-01\" ) ohlc . tail () Out[30]: O H L C V AC Date 2016-02-01 750.460022 757.859985 743.270020 752.000000 4801800 752.000000 2016-02-02 784.500000 789.869995 764.650024 764.650024 6332400 764.650024 2016-02-03 770.219971 774.500000 720.500000 726.950012 6162300 726.950012 2016-02-04 722.809998 727.000000 701.859985 708.010010 5145900 708.010010 2016-02-05 703.869995 703.989990 680.150024 683.570007 5070000 683.570007 Now time to define strategy. To do so, all we need to do is to create binary Series with signals, and, optionally, trade price Series with float elements. The strategy based on the KF will be the following: The short position (buy) is opened, when the price crosses the 'predicted price' up closed, when the price crosses the 'predicted price' down The long position (sell) is opened, when the price crosses the 'predicted price' down closed, when the price crosses the 'predicted price' up Let's train KF and save its predictions. In [31]: # Test of Kalaman Filter Class # a transition model: We measure the state each day -> delta_t = 1. delta_t = 1. F_init = np . array ([[ 1 , delta_t ],[ 0 , 1 ]]) # an 'acting-force' model G_init = np . array ([[ delta_t ** 2 / 2 ], [ delta_t ]]) # an observation model H_init = np . array ([[ 1 , 0 ]]) # an initial state: the price and its velocity x_init = np . array ([[ ohlc . C [ 0 ]],[ 0. ]]) # a priory covariance matrix # If the initial price and velocity are not known perfectly # the covariance matrix should be initialized with a suitably large number, say L, on its diagonal. P_init = np . array ([[ 1 , 0 ],[ 0 , 100. ]]) # L=100. kf = KalmanFilter ( ohlc . C [:] . as_matrix (), F_init , G_init , H_init , P_init , x_init , acc_variance = 1. , obs_variance = 100. , delta_t = delta_t ) INFO:root:building the KalmanFilter model INFO:root:done building the KalmanFilter model In [32]: def _get_all_predictions ( kalman_model ): ''' return all predicted prices ''' time_steps = len ( kalman_model . data ) price_guesses = np . zeros (( 2 , time_steps )) price_predicted = np . zeros (( 2 , time_steps )) price_actual = np . zeros (( 2 , time_steps )) for i in range ( time_steps ): x_p , P_p = kalman_model . predict () x_g , P_g = kalman_model . update ( np . array ([[ kalman_model . data [ i ]]])) price_guesses [:, i ] = x_g [:, 0 ] price_predicted [:, i ] = x_p [:, 0 ] price_actual [:, i ] = np . array ([[ kalman_model . data [ i ]],[ 0. ]])[:, 0 ] # Here 0. --> observed velocity :-) guesses , actual , predicted = price_guesses [ 0 ,:], price_actual [ 0 ,:], price_predicted [ 0 ,:] return predicted # update the definition of the class KalmanFilter . get_all_predictions = _get_all_predictions In [33]: ohlc [ 'predicted' ] = kf . get_all_predictions () In [34]: ohlc . tail () Out[34]: O H L C V AC predicted Date 2016-02-01 750.460022 757.859985 743.270020 752.000000 4801800 752.000000 730.150664 2016-02-02 784.500000 789.869995 764.650024 764.650024 6332400 764.650024 743.689946 2016-02-03 770.219971 774.500000 720.500000 726.950012 6162300 726.950012 758.585902 2016-02-04 722.809998 727.000000 701.859985 708.010010 5145900 708.010010 752.016438 2016-02-05 703.869995 703.989990 680.150024 683.570007 5070000 683.570007 737.473066 In [35]: buy = cover = ( ohlc . C > ohlc . predicted ) & ( ohlc . C . shift () < ohlc . predicted . shift ()) # cross up sell = short = ( ohlc . C < ohlc . predicted ) & ( ohlc . C . shift () > ohlc . predicted . shift ()) # cross down print '> Price \\n %s \\n ' % ohlc . C . tail () print '> Predicted \\n %s \\n ' % ohlc . predicted . tail () print '> Buy/Long signals \\n %s \\n ' % buy . tail () print '> Short/Sell signals \\n %s \\n ' % sell . tail () > Price Date 2016-02-01 752.000000 2016-02-02 764.650024 2016-02-03 726.950012 2016-02-04 708.010010 2016-02-05 683.570007 Name: C, dtype: float64 > Predicted Date 2016-02-01 730.150664 2016-02-02 743.689946 2016-02-03 758.585902 2016-02-04 752.016438 2016-02-05 737.473066 Name: predicted, dtype: float64 > Buy/Long signals Date 2016-02-01 False 2016-02-02 False 2016-02-03 False 2016-02-04 False 2016-02-05 False dtype: bool > Short/Sell signals Date 2016-02-01 False 2016-02-02 False 2016-02-03 True 2016-02-04 False 2016-02-05 False dtype: bool Time to run backtest. Backtest will try to extract signals and prices and bars from whatever dict-like object you passed as first argument. Could be dict, could be pandas.DataFrame or anything. To make thing easier, pass local namespace (extracted py calling locals()), that contains every variable you created up to this point. In [36]: bt_kf = pybacktest . Backtest ( locals (), 'kf_cross' ) print filter ( lambda x : not x . startswith ( '_' ), dir ( bt_kf )) print ' \\n > bt.signals \\n %s ' % bt_kf . signals . tail () print ' \\n > bt.trades \\n %s ' % bt_kf . trades . tail () print ' \\n > bt.positions \\n %s ' % bt_kf . positions . tail () print ' \\n > bt.equity \\n %s ' % bt_kf . equity . tail () print ' \\n > bt.trade_price \\n %s ' % bt_kf . trade_price . tail () ['dataobj', 'default_price', 'eqplot', 'equity', 'name', 'ohlc', 'plot_equity', 'plot_trades', 'positions', 'prices', 'report', 'run_time', 'signals', 'sigplot', 'stats', 'summary', 'trade_price', 'trades', 'trdplot'] > bt.signals Buy Cover Sell Short Date 2016-02-01 False False False False 2016-02-02 False False False False 2016-02-03 False False True True 2016-02-04 False False False False 2016-02-05 False False False False > bt.trades pos price vol Date 2016-01-19 -1 703.299988 -2 2016-01-20 1 688.609985 2 2016-01-28 -1 722.219971 -2 2016-01-29 1 731.530029 2 2016-02-04 -1 722.809998 -2 > bt.positions Date 2016-01-15 -1 2016-01-19 1 2016-01-27 -1 2016-01-28 1 2016-02-03 -1 dtype: float64 > bt.equity Date 2016-01-19 11.010010 2016-01-20 14.690003 2016-01-28 33.609986 2016-01-29 -9.310058 2016-02-04 -8.720031 dtype: float64 > bt.trade_price Date 2016-02-01 750.460022 2016-02-02 784.500000 2016-02-03 770.219971 2016-02-04 722.809998 2016-02-05 703.869995 Name: O, dtype: float64 In [37]: bt_kf . summary () ---------------------------------------------- | Backtest(kf_cross, 2016-08-02 18:11 CET) | ---------------------------------------------- backtest: days: !!python/long '505' from: '2014-09-17 00:00:00' to: '2016-02-04 00:00:00' trades: 98 exposure: trades/month: 5.4444 performance: PF: 1.3023 RF: 0.7456 averages: gain: 17.228 loss: -9.1231 trade: 1.6324 payoff: 1.8884 profit: 159.9775 winrate: 0.4082 risk/return profile: MPI: 0.1122 UPI: 0.0206 WCDD (monte-carlo 0.99 quantile): 232.2036 maxdd: 214.5602 sharpe: 0.0819 sortino: 0.199 ---------------------------------------------- In [38]: figsize ( 12.5 , 7 ) bt_kf . plot_equity () In [39]: figsize ( 12.5 , 7 ) bt_kf . plot_trades () legend ( loc = 'upper left' ) Out[39]: <matplotlib.legend.Legend at 0xb57b3eec> In [40]: # the main equity is earned in the second half of the last year bt_kf . trdplot [ '2014-6-9' : '2014-12-31' ] legend ( loc = 'upper left' ) Out[40]: <matplotlib.legend.Legend at 0xd4700ec> It looks promising! At least, our strategy returns the 155.9276 of profit in comparison with the underlying strategy, i.e. the opened long (buy) position for the whole period. Also,looking at the plots, we can conclude that the strategy requires the optimization: we need to catch the transformation from non-stationary to non-stationary processes. In the latter case, we have to close all open positions. Such tool to deal with the non-stationary process is ARMA model, which will be discussed later. Now we can introduce the crossing SMA strategy: In [41]: # we need to increase the period of analysis ohlc = pybacktest . load_from_yahoo ( 'GOOG' , start = \"2011\" ) ohlc Out[41]: O H L C V AC Date 2014-03-27 568.002570 568.002570 552.922516 558.462551 13100 558.462551 2014-03-28 561.202549 566.432590 558.672477 559.992504 41200 559.992504 2014-03-31 566.892592 567.002574 556.932537 556.972503 10800 556.972503 2014-04-01 558.712565 568.452595 558.712565 567.162558 7900 567.162558 2014-04-02 599.992707 604.832763 562.192568 567.002574 147100 567.002574 2014-04-03 569.852553 587.282679 564.132581 569.742571 5099200 569.742571 2014-04-04 574.652643 577.772650 543.002488 543.142460 6369300 543.142460 2014-04-07 540.742445 548.482483 527.152440 538.152456 4401700 538.152456 2014-04-08 542.602466 555.002500 541.612446 554.902556 3151200 554.902556 2014-04-09 559.622532 565.372554 552.952506 564.142557 3330800 564.142557 2014-04-10 565.002582 565.002582 539.902495 540.952433 4036900 540.952433 2014-04-11 532.552381 540.002440 526.532392 530.602392 3924800 530.602392 2014-04-14 538.252462 544.102429 529.562370 532.522453 2575100 532.522453 2014-04-15 536.822454 538.452473 518.462348 536.442444 3855100 536.442444 2014-04-16 543.002488 557.002492 540.002440 556.542490 4893300 556.542490 2014-04-17 548.812490 549.502492 531.152424 536.102400 6809500 536.102400 2014-04-21 536.102400 536.702435 525.602352 528.622414 2566700 528.622414 2014-04-22 528.642427 537.232391 527.512375 534.812425 2365400 534.812425 2014-04-23 533.792415 533.872408 526.252389 526.942391 2052300 526.942391 2014-04-24 530.072374 531.652452 522.122349 525.162363 1883200 525.162363 2014-04-25 522.512395 524.702361 515.422333 516.182352 2100400 516.182352 2014-04-28 517.182348 518.602319 502.802274 517.152359 3335500 517.152359 2014-04-29 516.902344 529.462425 516.322324 527.702410 2699100 527.702410 2014-04-30 527.602343 528.002366 522.522372 526.662388 1751200 526.662388 2014-05-01 527.112352 532.932391 523.882364 531.352374 1905500 531.352374 2014-05-02 533.762426 534.002403 525.612389 527.932411 1688500 527.932411 2014-05-05 524.822381 528.902418 521.322364 527.812392 1024100 527.812392 2014-05-06 525.232379 526.812396 515.062337 515.142330 1689000 515.142330 2014-05-07 515.792305 516.682320 503.302272 509.962291 3224300 509.962291 2014-05-08 508.462297 517.232290 506.452298 511.002313 2021300 511.002313 ... ... ... ... ... ... ... 2015-12-23 753.469971 754.210022 744.000000 750.309998 1565900 750.309998 2015-12-24 749.549988 751.349976 746.619995 748.400024 527200 748.400024 2015-12-28 752.919983 762.989990 749.520020 762.510010 1515300 762.510010 2015-12-29 766.690002 779.979980 766.429993 776.599976 1763700 776.599976 2015-12-30 776.599976 777.599976 766.900024 771.000000 1293300 771.000000 2015-12-31 769.500000 769.500000 758.340027 758.880005 1489600 758.880005 2016-01-04 743.000000 744.059998 731.257996 741.840027 3258200 741.840027 2016-01-05 746.450012 752.000000 738.640015 742.580017 1947700 742.580017 2016-01-06 730.000000 747.179993 728.919983 743.619995 1938600 743.619995 2016-01-07 730.309998 738.500000 719.059998 726.390015 2944300 726.390015 2016-01-08 731.450012 733.229980 713.000000 714.469971 2442600 714.469971 2016-01-11 716.609985 718.854980 703.539978 716.030029 2089300 716.030029 2016-01-12 721.679993 728.750000 717.317017 726.070007 2000500 726.070007 2016-01-13 730.849976 734.739990 698.609985 700.559998 2468300 700.559998 2016-01-14 705.380005 721.924988 689.099976 714.719971 2211900 714.719971 2016-01-15 692.289978 706.739990 685.369995 694.450012 3592400 694.450012 2016-01-19 703.299988 709.979980 693.409973 701.789978 2258500 701.789978 2016-01-20 688.609985 706.849976 673.260010 698.450012 3439400 698.450012 2016-01-21 702.179993 719.190002 694.460022 706.590027 2410300 706.590027 2016-01-22 723.599976 728.130005 720.120972 725.250000 2006500 725.250000 2016-01-25 723.580017 729.679993 710.010010 711.669983 1704600 711.669983 2016-01-26 713.849976 718.280029 706.479980 713.039978 1324300 713.039978 2016-01-27 713.669983 718.234985 694.390015 699.989990 2140000 699.989990 2016-01-28 722.219971 733.690002 712.349976 730.960022 2658000 730.960022 2016-01-29 731.530029 744.989990 726.799988 742.950012 3394900 742.950012 2016-02-01 750.460022 757.859985 743.270020 752.000000 4801800 752.000000 2016-02-02 784.500000 789.869995 764.650024 764.650024 6332400 764.650024 2016-02-03 770.219971 774.500000 720.500000 726.950012 6162300 726.950012 2016-02-04 722.809998 727.000000 701.859985 708.010010 5145900 708.010010 2016-02-05 703.869995 703.989990 680.150024 683.570007 5070000 683.570007 470 rows × 6 columns In [42]: short_ma = 10 # long_ma = 50 # ms = pd . rolling_mean ( ohlc . C , short_ma ) ml = pd . rolling_mean ( ohlc . C , long_ma ) buy = cover = ( ms > ml ) & ( ms . shift () < ml . shift ()) # ma cross up sell = short = ( ms < ml ) & ( ms . shift () > ml . shift ()) # ma cross down print '> Short MA \\n %s \\n ' % ms . tail () print '> Long MA \\n %s \\n ' % ml . tail () print '> Buy/Cover signals \\n %s \\n ' % buy . tail () print '> Short/Sell signals \\n %s \\n ' % sell . tail () bt = pybacktest . Backtest ( locals (), 'ma_cross' ) print filter ( lambda x : not x . startswith ( '_' ), dir ( bt )) print ' \\n > bt.signals \\n %s ' % bt . signals . tail () print ' \\n > bt.trades \\n %s ' % bt . trades . tail () print ' \\n > bt.positions \\n %s ' % bt . positions . tail () print ' \\n > bt.equity \\n %s ' % bt . equity . tail () print ' \\n > bt.trade_price \\n %s ' % bt . trade_price . tail () > Short MA Date 2016-02-01 718.269000 2016-02-02 724.555005 2016-02-03 727.405005 2016-02-04 727.547003 2016-02-05 723.379004 dtype: float64 > Long MA Date 2016-02-01 740.030001 2016-02-02 740.523002 2016-02-03 740.293803 2016-02-04 739.322003 2016-02-05 737.873804 dtype: float64 > Buy/Cover signals Date 2016-02-01 False 2016-02-02 False 2016-02-03 False 2016-02-04 False 2016-02-05 False dtype: bool > Short/Sell signals Date 2016-02-01 False 2016-02-02 False 2016-02-03 False 2016-02-04 False 2016-02-05 False dtype: bool ['dataobj', 'default_price', 'eqplot', 'equity', 'name', 'ohlc', 'plot_equity', 'plot_trades', 'positions', 'prices', 'report', 'run_time', 'signals', 'sigplot', 'stats', 'summary', 'trade_price', 'trades', 'trdplot'] > bt.signals Buy Cover Sell Short Date 2016-02-01 False False False False 2016-02-02 False False False False 2016-02-03 False False False False 2016-02-04 False False False False 2016-02-05 False False False False > bt.trades pos price vol Date 2015-04-09 -1 541.032486 -2 2015-07-17 1 649.000000 2 2015-09-14 -1 625.700012 -2 2015-10-13 1 643.150024 2 2016-01-13 -1 730.849976 -2 > bt.positions Date 2015-04-08 -1 2015-07-16 1 2015-09-11 -1 2015-10-12 1 2016-01-12 -1 dtype: float64 > bt.equity Date 2015-04-09 13.030120 2015-07-17 -107.967514 2015-09-14 -23.299988 2015-10-13 -17.450012 2016-01-13 87.699952 dtype: float64 > bt.trade_price Date 2016-02-01 750.460022 2016-02-02 784.500000 2016-02-03 770.219971 2016-02-04 722.809998 2016-02-05 703.869995 Name: O, dtype: float64 In [43]: bt . summary () ---------------------------------------------- | Backtest(ma_cross, 2016-08-02 18:12 CET) | ---------------------------------------------- backtest: days: !!python/long '509' from: '2014-08-22 00:00:00' to: '2016-01-13 00:00:00' trades: 10 exposure: trades/month: 1.1111 performance: PF: 0.8821 RF: -0.1362 averages: gain: 50.5101 loss: -24.5411 trade: -2.0257 payoff: 2.0582 profit: -20.2575 winrate: 0.3 risk/return profile: MPI: -0.0304 UPI: -0.0274 WCDD (monte-carlo 0.99 quantile): 171.7877 maxdd: 148.7175 sharpe: -0.0399 sortino: -0.0536 ---------------------------------------------- In [44]: figsize ( 12.5 , 7 ) bt . plot_equity () legend ( loc = 'upper left' ) Out[44]: <matplotlib.legend.Legend at 0xa9c29bcc> In [45]: # the main equity is earned in the last few months bt . trdplot [ '2014-2-1' : '2015-6-9' ] pd . rolling_mean ( ohlc . C [ '2014-2-1' : '2015-6-9' ], short_ma ) . plot ( c = 'green' ) pd . rolling_mean ( ohlc . C [ '2014-2-1' : '2015-6-9' ], long_ma ) . plot ( c = 'blue' ) legend ( loc = 'upper left' ) Out[45]: <matplotlib.legend.Legend at 0xa9c6edac> The strategy based on crossed SMA doesn't work at all. The reason of this is the too short period used in this analysis. As a result, too small short SMA and long SMA (reduce in 2.5 and 4 times accordingly). References Hoffman & Gelman. (2011). The No-U-Turn Sampler: Adaptively Setting Path Lengths in Hamiltonian Monte Carlo. In [ ]: if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML'; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: 'center',\" + \" displayIndent: '0em',\" + \" showMathMenu: true,\" + \" tex2jax: { \" + \" inlineMath: [ ['$','$'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }\" + \" } \" + \"}); \"; (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Statistics","title":"Stochastic Kalman Filter in the Finance"},{"url":"http://igormarfin.github.io/blog/2016/02/07/a-post-with-an-image/","text":"Sometimes you would like to post articles with images. What you can do is the following make post_with_image # to test the html output make html make serve # to publish to github.com make publish make post_with_image command. Basicaly, it performs make add_entry_rst A post with an Image,blog \\\\\\, post \\\\\\, image,BuildingThisBlog cat how-to-post-with-image.rst >> $( BLOG_SOURCE ) /content/*a-post-with-an-image.rst mkdir -p $( BLOG_SOURCE ) /content/images cp pelican-plugin-post-stats-medium-example.png $( BLOG_SOURCE ) /content/images/ echo \" Here is my image .. figure:: /images/pelican-plugin-post-stats-medium-example.png :align: right This is the caption of the figure. \" >> $( BLOG_SOURCE ) /content/*a-post-with-an-image.rst Also it is important that your pelicanconf.py has images in the static path: STATIC_PATHS =[ 'images' ] Here is my image This is the caption of the figure. That's it.","tags":"BuildingThisBlog","title":"A post with an Image"},{"url":"http://igormarfin.github.io/blog/2016/02/07/a-hierarchical-bayesian-model-of-the-bundes-league/","text":"/*! * * IPython notebook * */.ansibold{font-weight:bold}.ansiblack{color:black}.ansired{color:darkred}.ansigreen{color:darkgreen}.ansiyellow{color:#c4a000}.ansiblue{color:darkblue}.ansipurple{color:darkviolet}.ansicyan{color:steelblue}.ansigray{color:gray}.ansibgblack{background-color:black}.ansibgred{background-color:red}.ansibggreen{background-color:green}.ansibgyellow{background-color:yellow}.ansibgblue{background-color:blue}.ansibgpurple{background-color:magenta}.ansibgcyan{background-color:cyan}.ansibggray{background-color:gray}div.cell{border:1px solid transparent;display:-webkit-box;-webkit-box-orient:vertical;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:vertical;-moz-box-align:stretch;display:box;box-orient:vertical;box-align:stretch;display:flex;flex-direction:column;align-items:stretch;border-radius:2px;box-sizing:border-box;-moz-box-sizing:border-box;-webkit-box-sizing:border-box;border-width:thin;border-style:solid;width:100%;padding:5px;margin:0;outline:none}div.cell.selected{border-color:#ababab}@media print{div.cell.selected{border-color:transparent}}.edit_mode div.cell.selected{border-color:green}@media print{.edit_mode div.cell.selected{border-color:transparent}}.prompt{min-width:14ex;padding:.4em;margin:0;font-family:monospace;text-align:right;line-height:1.21429em}@media (max-width:540px){.prompt{text-align:left}}div.inner_cell{display:-webkit-box;-webkit-box-orient:vertical;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:vertical;-moz-box-align:stretch;display:box;box-orient:vertical;box-align:stretch;display:flex;flex-direction:column;align-items:stretch;-webkit-box-flex:1;-moz-box-flex:1;box-flex:1;flex:1}@-moz-document url-prefix(){div.inner_cell{overflow-x:hidden}}div.input_area{border:1px solid #cfcfcf;border-radius:2px;background:#f7f7f7;line-height:1.21429em}div.prompt:empty{padding-top:0;padding-bottom:0}div.unrecognized_cell{padding:5px 5px 5px 0;display:-webkit-box;-webkit-box-orient:horizontal;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:horizontal;-moz-box-align:stretch;display:box;box-orient:horizontal;box-align:stretch;display:flex;flex-direction:row;align-items:stretch}div.unrecognized_cell .inner_cell{border-radius:2px;padding:5px;font-weight:bold;color:red;border:1px solid #cfcfcf;background:#eaeaea}div.unrecognized_cell .inner_cell a{color:inherit;text-decoration:none}div.unrecognized_cell .inner_cell a:hover{color:inherit;text-decoration:none}@media (max-width:540px){div.unrecognized_cell>div.prompt{display:none}}@media print{div.code_cell{page-break-inside:avoid}}div.input{page-break-inside:avoid;display:-webkit-box;-webkit-box-orient:horizontal;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:horizontal;-moz-box-align:stretch;display:box;box-orient:horizontal;box-align:stretch;display:flex;flex-direction:row;align-items:stretch}@media (max-width:540px){div.input{display:-webkit-box;-webkit-box-orient:vertical;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:vertical;-moz-box-align:stretch;display:box;box-orient:vertical;box-align:stretch;display:flex;flex-direction:column;align-items:stretch}}div.input_prompt{color:navy;border-top:1px solid transparent}div.input_area>div.highlight{margin:.4em;border:none;padding:0;background-color:transparent}div.input_area>div.highlight>pre{margin:0;border:none;padding:0;background-color:transparent}.CodeMirror{line-height:1.21429em;font-size:14px;height:auto;background:none}.CodeMirror-scroll{overflow-y:hidden;overflow-x:auto}.CodeMirror-lines{padding:.4em}.CodeMirror-linenumber{padding:0 8px 0 4px}.CodeMirror-gutters{border-bottom-left-radius:2px;border-top-left-radius:2px}.CodeMirror pre{padding:0;border:0;border-radius:0}.highlight-base{}.highlight-variable{}.highlight-variable-2{color:#1a1a1a}.highlight-variable-3{color:#333}.highlight-string{color:#ba2121}.highlight-comment{color:#408080;font-style:italic}.highlight-number{80}.highlight-atom{color:#88f}.highlight-keyword{8000;font-weight:bold}.highlight-builtin{8000}.highlight-error{color:#f00}.highlight-operator{color:#a2f;font-weight:bold}.highlight-meta{color:#a2f}.highlight-def{f}.highlight-string-2{color:#f50}.highlight-qualifier{color:#555}.highlight-bracket{color:#997}.highlight-tag{color:#170}.highlight-attribute{c}.highlight-header{color:blue}.highlight-quote{90}.highlight-link{c}.cm-s-ipython span.cm-keyword{8000;font-weight:bold}.cm-s-ipython span.cm-atom{color:#88f}.cm-s-ipython span.cm-number{80}.cm-s-ipython span.cm-def{f}.cm-s-ipython span.cm-variable{}.cm-s-ipython span.cm-operator{color:#a2f;font-weight:bold}.cm-s-ipython span.cm-variable-2{color:#1a1a1a}.cm-s-ipython span.cm-variable-3{color:#333}.cm-s-ipython span.cm-comment{color:#408080;font-style:italic}.cm-s-ipython span.cm-string{color:#ba2121}.cm-s-ipython span.cm-string-2{color:#f50}.cm-s-ipython span.cm-meta{color:#a2f}.cm-s-ipython span.cm-qualifier{color:#555}.cm-s-ipython span.cm-builtin{8000}.cm-s-ipython span.cm-bracket{color:#997}.cm-s-ipython span.cm-tag{color:#170}.cm-s-ipython span.cm-attribute{c}.cm-s-ipython span.cm-header{color:blue}.cm-s-ipython span.cm-quote{90}.cm-s-ipython span.cm-link{c}.cm-s-ipython span.cm-error{color:#f00}.cm-s-ipython span.cm-tab{background:url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADAAAAAMCAYAAAAkuj5RAAAAAXNSR0IArs4c6QAAAGFJREFUSMft1LsRQFAQheHPowAKoACx3IgEKtaEHujDjORSgWTH/ZOdnZOcM/sgk/kFFWY0qV8foQwS4MKBCS3qR6ixBJvElOobYAtivseIE120FaowJPN75GMu8j/LfMwNjh4HUpwg4LUAAAAASUVORK5CYII=);background-position:right;background-repeat:no-repeat}div.output_wrapper{position:relative;display:-webkit-box;-webkit-box-orient:vertical;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:vertical;-moz-box-align:stretch;display:box;box-orient:vertical;box-align:stretch;display:flex;flex-direction:column;align-items:stretch}div.output_scroll{height:24em;width:100%;overflow:auto;border-radius:2px;-webkit-box-shadow:inset 0 2px 8px rgba(0,0,0,0.8);box-shadow:inset 0 2px 8px rgba(0,0,0,0.8);display:block}div.output_collapsed{margin:0;padding:0;display:-webkit-box;-webkit-box-orient:vertical;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:vertical;-moz-box-align:stretch;display:box;box-orient:vertical;box-align:stretch;display:flex;flex-direction:column;align-items:stretch}div.out_prompt_overlay{height:100%;padding:0 .4em;position:absolute;border-radius:2px}div.out_prompt_overlay:hover{-webkit-box-shadow:inset 0 0 1px #000;box-shadow:inset 0 0 1px #000;background:rgba(240,240,240,0.5)}div.output_prompt{color:darkred}div.output_area{padding:0;page-break-inside:avoid;display:-webkit-box;-webkit-box-orient:horizontal;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:horizontal;-moz-box-align:stretch;display:box;box-orient:horizontal;box-align:stretch;display:flex;flex-direction:row;align-items:stretch}div.output_area .MathJax_Display{text-align:left !important}div.output_area div.output_area .output{display:-webkit-box;-webkit-box-orient:vertical;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:vertical;-moz-box-align:stretch;display:box;box-orient:vertical;box-align:stretch;display:flex;flex-direction:column;align-items:stretch}@media (max-width:540px){div.output_area{display:-webkit-box;-webkit-box-orient:vertical;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:vertical;-moz-box-align:stretch;display:box;box-orient:vertical;box-align:stretch;display:flex;flex-direction:column;align-items:stretch}}div.output_area pre{margin:0;padding:0;border:0;vertical-align:baseline;color:black;background-color:transparent;border-radius:0}div.output_subarea{padding:.4em;-webkit-box-flex:1;-moz-box-flex:1;box-flex:1;flex:1}div.output_text{text-align:left;line-height:1.21429em}div.output_stderr{background:#fdd}div.output_latex{text-align:left}div.output_javascript:empty{padding:0}.js-error{color:darkred}div.raw_input_container{font-family:monospace;padding-top:5px}input.raw_input{font-family:inherit;font-size:inherit;color:inherit;width:auto;vertical-align:baseline;padding:0 .25em;margin:0 .25em}input.raw_input:focus{box-shadow:none}p.p-space{margin-bottom:10px}div.output_unrecognized{padding:5px;font-weight:bold;color:red}div.output_unrecognized a{color:inherit;text-decoration:none}div.output_unrecognized a:hover{color:inherit;text-decoration:none}.rendered_html{}.rendered_html :link{text-decoration:underline}.rendered_html :visited{text-decoration:underline}.rendered_html h1:first-child{margin-top:.538em}.rendered_html h2:first-child{margin-top:.636em}.rendered_html h3:first-child{margin-top:.777em}.rendered_html h4:first-child{margin-top:1em}.rendered_html h5:first-child{margin-top:1em}.rendered_html h6:first-child{margin-top:1em}.rendered_html *+ul{margin-top:1em}.rendered_html *+ol{margin-top:1em}.rendered_html *+table{margin-top:1em}.rendered_html *+p{margin-top:1em}.rendered_html *+img{margin-top:1em}div.text_cell{display:-webkit-box;-webkit-box-orient:horizontal;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:horizontal;-moz-box-align:stretch;display:box;box-orient:horizontal;box-align:stretch;display:flex;flex-direction:row;align-items:stretch}@media (max-width:540px){div.text_cell>div.prompt{display:none}}div.text_cell_render{outline:none;resize:none;width:inherit;border-style:none;padding:.5em .5em .5em .4em;box-sizing:border-box;-moz-box-sizing:border-box;-webkit-box-sizing:border-box}a.anchor-link:link{text-decoration:none;padding:0 20px;visibility:hidden}h1:hover .anchor-link,h2:hover .anchor-link,h3:hover .anchor-link,h4:hover .anchor-link,h5:hover .anchor-link,h6:hover .anchor-link{visibility:visible}.text_cell.rendered .input_area{display:none}.text_cell.unrendered .text_cell_render{display:none}.cm-header-1,.cm-header-2,.cm-header-3,.cm-header-4,.cm-header-5,.cm-header-6{font-weight:bold;font-family:\"Helvetica Neue\",Helvetica,Arial,sans-serif}.cm-header-1{font-size:185.7%}.cm-header-2{font-size:157.1%}.cm-header-3{font-size:128.6%}.cm-header-4{font-size:110%}.cm-header-5{font-size:100%;font-style:italic}.cm-header-6{font-size:100%;font-style:italic}.widget-interact>div,.widget-interact>input{padding:2.5px}.widget-area{page-break-inside:avoid;display:-webkit-box;-webkit-box-orient:horizontal;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:horizontal;-moz-box-align:stretch;display:box;box-orient:horizontal;box-align:stretch;display:flex;flex-direction:row;align-items:stretch}.widget-area .widget-subarea{padding:.44em .4em .4em 1px;margin-left:6px;box-sizing:border-box;-moz-box-sizing:border-box;-webkit-box-sizing:border-box;display:-webkit-box;-webkit-box-orient:vertical;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:vertical;-moz-box-align:stretch;display:box;box-orient:vertical;box-align:stretch;display:flex;flex-direction:column;align-items:stretch;-webkit-box-flex:2;-moz-box-flex:2;box-flex:2;flex:2;-webkit-box-align:start;-moz-box-align:start;box-align:start;align-items:flex-start}.widget-area.connection-problems .prompt:after{content:\"\\f127\";font-family:'FontAwesome';color:#d9534f;font-size:14px;top:3px;padding:3px}.slide-track{border:1px solid #ccc;background:#fff;border-radius:2px}.widget-hslider{padding-left:8px;padding-right:2px;overflow:visible;width:350px;height:5px;max-height:5px;margin-top:13px;margin-bottom:10px;border:1px solid #ccc;background:#fff;border-radius:2px;display:-webkit-box;-webkit-box-orient:horizontal;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:horizontal;-moz-box-align:stretch;display:box;box-orient:horizontal;box-align:stretch;display:flex;flex-direction:row;align-items:stretch}.widget-hslider .ui-slider{border:0;background:none;display:-webkit-box;-webkit-box-orient:horizontal;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:horizontal;-moz-box-align:stretch;display:box;box-orient:horizontal;box-align:stretch;display:flex;flex-direction:row;align-items:stretch;-webkit-box-flex:1;-moz-box-flex:1;box-flex:1;flex:1}.widget-hslider .ui-slider .ui-slider-handle{width:12px;height:28px;margin-top:-8px;border-radius:2px}.widget-hslider .ui-slider .ui-slider-range{height:12px;margin-top:-4px;background:#eee}.widget-vslider{padding-bottom:5px;overflow:visible;width:5px;max-width:5px;height:250px;margin-left:12px;border:1px solid #ccc;background:#fff;border-radius:2px;display:-webkit-box;-webkit-box-orient:vertical;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:vertical;-moz-box-align:stretch;display:box;box-orient:vertical;box-align:stretch;display:flex;flex-direction:column;align-items:stretch}.widget-vslider .ui-slider{border:0;background:none;margin-left:-4px;margin-top:5px;display:-webkit-box;-webkit-box-orient:vertical;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:vertical;-moz-box-align:stretch;display:box;box-orient:vertical;box-align:stretch;display:flex;flex-direction:column;align-items:stretch;-webkit-box-flex:1;-moz-box-flex:1;box-flex:1;flex:1}.widget-vslider .ui-slider .ui-slider-handle{width:28px;height:12px;margin-left:-9px;border-radius:2px}.widget-vslider .ui-slider .ui-slider-range{width:12px;margin-left:-1px;background:#eee}.widget-text{width:350px;margin:0}.widget-listbox{width:350px;margin-bottom:0}.widget-numeric-text{width:150px;margin:0}.widget-progress{margin-top:6px;min-width:350px}.widget-progress .progress-bar{-webkit-transition:none;-moz-transition:none;-ms-transition:none;-o-transition:none;transition:none}.widget-combo-btn{min-width:125px}.widget_item .dropdown-menu li a{color:inherit}.widget-hbox{display:-webkit-box;-webkit-box-orient:horizontal;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:horizontal;-moz-box-align:stretch;display:box;box-orient:horizontal;box-align:stretch;display:flex;flex-direction:row;align-items:stretch}.widget-hbox input[type=\"checkbox\"]{margin-top:9px;margin-bottom:10px}.widget-hbox .widget-label{min-width:10ex;padding-right:8px;padding-top:5px;text-align:right;vertical-align:text-top}.widget-hbox .widget-readout{padding-left:8px;padding-top:5px;text-align:left;vertical-align:text-top}.widget-vbox{display:-webkit-box;-webkit-box-orient:vertical;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:vertical;-moz-box-align:stretch;display:box;box-orient:vertical;box-align:stretch;display:flex;flex-direction:column;align-items:stretch}.widget-vbox .widget-label{padding-bottom:5px;text-align:center;vertical-align:text-bottom}.widget-vbox .widget-readout{padding-top:5px;text-align:center;vertical-align:text-top}.widget-box{box-sizing:border-box;-moz-box-sizing:border-box;-webkit-box-sizing:border-box;-webkit-box-align:start;-moz-box-align:start;box-align:start;align-items:flex-start}.widget-radio-box{display:-webkit-box;-webkit-box-orient:vertical;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:vertical;-moz-box-align:stretch;display:box;box-orient:vertical;box-align:stretch;display:flex;flex-direction:column;align-items:stretch;box-sizing:border-box;-moz-box-sizing:border-box;-webkit-box-sizing:border-box;padding-top:4px}.widget-radio-box label{margin-top:0}.widget-radio{margin-left:20px} .highlight .hll { background-color: #ffffcc } .highlight { background: #f8f8f8; } .highlight .c { color: #408080; font-style: italic } /* Comment */ .highlight .err { border: 1px solid #FF0000 } /* Error */ .highlight .k { color: #008000; font-weight: bold } /* Keyword */ .highlight .o { color: #666666 } /* Operator */ .highlight .cm { color: #408080; font-style: italic } /* Comment.Multiline */ .highlight .cp { color: #BC7A00 } /* Comment.Preproc */ .highlight .c1 { color: #408080; font-style: italic } /* Comment.Single */ .highlight .cs { color: #408080; font-style: italic } /* Comment.Special */ .highlight .gd { color: #A00000 } /* Generic.Deleted */ .highlight .ge { font-style: italic } /* Generic.Emph */ .highlight .gr { color: #FF0000 } /* Generic.Error */ .highlight .gh { color: #000080; font-weight: bold } /* Generic.Heading */ .highlight .gi { color: #00A000 } /* Generic.Inserted */ .highlight .go { color: #888888 } /* Generic.Output */ .highlight .gp { color: #000080; font-weight: bold } /* Generic.Prompt */ .highlight .gs { font-weight: bold } /* Generic.Strong */ .highlight .gu { color: #800080; font-weight: bold } /* Generic.Subheading */ .highlight .gt { color: #0044DD } /* Generic.Traceback */ .highlight .kc { color: #008000; font-weight: bold } /* Keyword.Constant */ .highlight .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */ .highlight .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */ .highlight .kp { color: #008000 } /* Keyword.Pseudo */ .highlight .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */ .highlight .kt { color: #B00040 } /* Keyword.Type */ .highlight .m { color: #666666 } /* Literal.Number */ .highlight .s { color: #BA2121 } /* Literal.String */ .highlight .na { color: #7D9029 } /* Name.Attribute */ .highlight .nb { color: #008000 } /* Name.Builtin */ .highlight .nc { color: #0000FF; font-weight: bold } /* Name.Class */ .highlight .no { color: #880000 } /* Name.Constant */ .highlight .nd { color: #AA22FF } /* Name.Decorator */ .highlight .ni { color: #999999; font-weight: bold } /* Name.Entity */ .highlight .ne { color: #D2413A; font-weight: bold } /* Name.Exception */ .highlight .nf { color: #0000FF } /* Name.Function */ .highlight .nl { color: #A0A000 } /* Name.Label */ .highlight .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */ .highlight .nt { color: #008000; font-weight: bold } /* Name.Tag */ .highlight .nv { color: #19177C } /* Name.Variable */ .highlight .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */ .highlight .w { color: #bbbbbb } /* Text.Whitespace */ .highlight .mb { color: #666666 } /* Literal.Number.Bin */ .highlight .mf { color: #666666 } /* Literal.Number.Float */ .highlight .mh { color: #666666 } /* Literal.Number.Hex */ .highlight .mi { color: #666666 } /* Literal.Number.Integer */ .highlight .mo { color: #666666 } /* Literal.Number.Oct */ .highlight .sb { color: #BA2121 } /* Literal.String.Backtick */ .highlight .sc { color: #BA2121 } /* Literal.String.Char */ .highlight .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */ .highlight .s2 { color: #BA2121 } /* Literal.String.Double */ .highlight .se { color: #BB6622; font-weight: bold } /* Literal.String.Escape */ .highlight .sh { color: #BA2121 } /* Literal.String.Heredoc */ .highlight .si { color: #BB6688; font-weight: bold } /* Literal.String.Interpol */ .highlight .sx { color: #008000 } /* Literal.String.Other */ .highlight .sr { color: #BB6688 } /* Literal.String.Regex */ .highlight .s1 { color: #BA2121 } /* Literal.String.Single */ .highlight .ss { color: #19177C } /* Literal.String.Symbol */ .highlight .bp { color: #008000 } /* Name.Builtin.Pseudo */ .highlight .vc { color: #19177C } /* Name.Variable.Class */ .highlight .vg { color: #19177C } /* Name.Variable.Global */ .highlight .vi { color: #19177C } /* Name.Variable.Instance */ .highlight .il { color: #666666 } /* Literal.Number.Integer.Long */ A Hierarchical Bayesian Model of the Bundes League $$\\\\[2pt]$$ Igor Marfin [Unister Gmb@2014] < igor.marfin@unister.de > $$\\\\[40pt]$$ Abstract I came across the following blog post on [1] : I quote from it, about the realization for Premier League Football: It occurred to me that this problem is perfect for a Bayesian model. We want to infer the latent paremeters (every team's strength) that are generating the data we observe (the scorelines). Moreover, we know that the scorelines are a noisy measurement of team strength, so ideally, we want a model that makes it easy to quantify our uncertainty about the underlying strengths. In this tutorial, I'm going to reproduce the model described in this post [1] using pymc. While they used the results of the 2013-2014 Premier League, I'm going to utilize the results of the 2013-2014 Bundes League. More details can be found at my repository https://bitbucket.org/iggy_floyd/bayesian-pymc-bundesliga . $$\\\\[5pt]$$ Outline Abstract Initialization of the notebook Getting and cleaning the data Scraping data from the Wikipedia Final preparation of the dataframe The model Visualization Plot of the Mean of Posteriors Plot of the HPDI Simulations Prediction of 2014/2015 BundesLiga final results (total scored goals) Prediction of the matches scoring in 2013/2014 season Prediction of the matches scoring for 2015/2016 season $$\\\\[5pt]$$ Initialization To set up the python environment for the data analysis and make a nicer style of the notebook, one can run the following commands in the beginning of our modeling: In [1]: import sys sys . path = [ '/usr/local/lib/python2.7/dist-packages' ] + sys . path # to fix the problem with numpy: this replaces 1.6 version by 1.9 % matplotlib inline % pylab inline ion () import os import matplotlib import numpy as np import matplotlib.pyplot as pl import matplotlib as mpl import logging import pymc as pm # a plotter and dataframe modules import seaborn as sns # seaborn to make a nice plots of the data import pandas as pd import scipy.stats as stats # Set up logging. logger = logging . getLogger () logger . setLevel ( logging . INFO ) from book_format import load_style , figsize , set_figsize load_style () Populating the interactive namespace from numpy and matplotlib Out[1]: @import url('http://fonts.googleapis.com/css?family=Source+Code+Pro'); @import url('http://fonts.googleapis.com/css?family=Vollkorn'); @import url('http://fonts.googleapis.com/css?family=Arimo'); div.cell{ width: 1200px; margin-left: 0% !important; margin-right: auto; } div.text_cell code { background: transparent; color: #000000; font-weight: 600; font-size: 11pt; font-style: bold; font-family: 'Source Code Pro', Consolas, monocco, monospace; } h1 { font-family: 'Open sans',verdana,arial,sans-serif; } div.input_area { background: #F6F6F9; border: 1px solid #586e75; } .text_cell_render h1 { font-weight: 200; font-size: 30pt; line-height: 100%; color:#c76c0c; margin-bottom: 0.5em; margin-top: 1em; display: block; white-space: wrap; } h2 { font-family: 'Open sans',verdana,arial,sans-serif; } .text_cell_render h2 { font-weight: 200; font-size: 16pt; font-style: italic; line-height: 100%; color:#c76c0c; margin-bottom: 0.5em; margin-top: 1.5em; display: inline; white-space: wrap; } h3 { font-family: 'Open sans',verdana,arial,sans-serif; } .text_cell_render h3 { font-weight: 200; font-size: 14pt; line-height: 100%; color:#d77c0c; margin-bottom: 0.5em; margin-top: 2em; display: block; white-space: nowrap; } h4 { font-family: 'Open sans',verdana,arial,sans-serif; } .text_cell_render h4 { font-weight: 100; font-size: 14pt; color:#d77c0c; margin-bottom: 0.5em; margin-top: 0.5em; display: block; white-space: nowrap; } h5 { font-family: 'Open sans',verdana,arial,sans-serif; } .text_cell_render h5 { font-weight: 200; font-style: normal; color: #1d3b84; font-size: 16pt; margin-bottom: 0em; margin-top: 0.5em; display: block; white-space: nowrap; } div.text_cell_render{ font-family: 'Arimo',verdana,arial,sans-serif; line-height: 125%; font-size: 120%; text-align:justify; text-justify:inter-word; } div.output_subarea.output_text.output_pyout { overflow-x: auto; overflow-y: scroll; max-height: 50000px; } div.output_subarea.output_stream.output_stdout.output_text { overflow-x: auto; overflow-y: scroll; max-height: 50000px; } div.output_wrapper{ margin-top:0.2em; margin-bottom:0.2em; } code{ font-size: 70%; } .rendered_html code{ background-color: transparent; } ul{ margin: 2em; } ul li{ padding-left: 0.5em; margin-bottom: 0.5em; margin-top: 0.5em; } ul li li{ padding-left: 0.2em; margin-bottom: 0.2em; margin-top: 0.2em; } ol{ margin: 2em; } ol li{ padding-left: 0.5em; margin-bottom: 0.5em; margin-top: 0.5em; } ul li{ padding-left: 0.5em; margin-bottom: 0.5em; margin-top: 0.2em; } a:link{ font-weight: bold; color:#447adb; } a:visited{ font-weight: bold; color: #1d3b84; } a:hover{ font-weight: bold; color: #1d3b84; } a:focus{ font-weight: bold; color:#447adb; } a:active{ font-weight: bold; color:#447adb; } .rendered_html :link { text-decoration: underline; } .rendered_html :hover { text-decoration: none; } .rendered_html :visited { text-decoration: none; } .rendered_html :focus { text-decoration: none; } .rendered_html :active { text-decoration: none; } .warning{ color: rgb( 240, 20, 20 ) } hr { color: #f3f3f3; background-color: #f3f3f3; height: 1px; } blockquote{ display:block; background: #fcfcfc; border-left: 5px solid #c76c0c; font-family: 'Open sans',verdana,arial,sans-serif; width:1000px; padding: 10px 10px 10px 10px; text-align:justify; text-justify:inter-word; } blockquote p { margin-bottom: 0; line-height: 125%; font-size: 100%; } MathJax.Hub.Config({ TeX: { extensions: [\"AMSmath.js\"] }, tex2jax: { inlineMath: [ ['$','$'], [\"\\\\(\",\"\\\\)\"] ], displayMath: [ ['$$','$$'], [\"\\\\[\",\"\\\\]\"] ] }, displayAlign: 'center', // Change this to 'center' to center equations. \"HTML-CSS\": { scale:100, availableFonts: [\"Neo-Euler\"], preferredFont: \"Neo-Euler\", webFont: \"Neo-Euler\", styles: {'.MathJax_Display': {\"margin\": 4}} } }); $$\\\\[5pt]$$ Getting and cleaning the data Well, the most obvious way to find data is to address ourselves to pages of the Wikipedia. After googling, we have come to the page [2] with results of the 2013/14 Bundesliga. They contain scores of the 51st season of the Bundesliga, Germany's premier football league. The season began on 9 August 2013 and the final matchday was on 10 May 2014. $$\\\\[5pt]$$ Scraping data from the Wikipedia I have found myself a challenge to work with data presented in the Wikipedia. Basicaly, we need some universal tool to scrape the data from a table placed on a Wikipedia page. Below, one can find the useful code to scrapping data from the wiki. I have extended the approach given in the post [3] . In [2]: ''' Igor Marfin <Unister Gmb, 2014> igor.marfin@unister.de A tool to scrape data from wiki-pages ''' from bs4 import BeautifulSoup import urllib2 import warnings from StringIO import StringIO import pandas as pd warnings . filterwarnings ( 'ignore' ) class MyWikiParser ( BeautifulSoup ): ''' BeautifulSoup parser: parses <table> </table> tags''' def __init__ ( self , page , table_class_properties , table_header ): BeautifulSoup . __init__ ( self , page , fromEncoding = \"UTF-8\" ) self . recording = 0 self . data = [] self . table_class_properties = table_class_properties self . table_header = table_header def handle_data ( self , column_tag = \"td\" ): tables = self . findAll ( \"table\" , self . table_class_properties ) for table in tables : first_row = table . find ( \"tr\" ) first_cell_in_first_row = first_row . find ( column_tag ) rows = [] if ( first_cell_in_first_row is not None ) and ( self . table_header in str ( first_cell_in_first_row )): for row in table . findAll ( \"tr\" ): vals = row . find_all ( 'td' ) vals = map ( lambda x : x . text . encode ( 'utf-8' ) . split ( '!' )[ 1 ] if len ( x . text . encode ( 'utf-8' ) . split ( '!' )) > 1 else x . text . encode ( 'utf-8' ) . split ( '!' )[ 0 ], vals ) # fix some text data # rows.append([val.text.encode('utf-8').replace('\\xe2\\x95\\xb2','/').replace('\\xe2\\x80\\x93','-') for val in vals]) rows . append ([ val . replace ( ' \\xe2\\x95\\xb2 ' , '/' ) . replace ( ' \\xe2\\x80\\x93 ' , '-' ) for val in vals ]) self . data = rows class Wiki_Table ( object ): ''' exctracts values for the table from Wiki-Page files ''' def __init__ ( self , url , table_class_properties , table_header , column_tag = \"td\" ): self . table = [] header = { 'User-Agent' : 'Mozilla/5.0' } #Needed to prevent 403 error on Wikipedia req = urllib2 . Request ( url , headers = header ) page = urllib2 . urlopen ( req ) parser = MyWikiParser ( page , table_class_properties , table_header ) parser . handle_data ( column_tag ) self . table = parser . data # a page containing the data wikipage = 'https://en.wikipedia.org/wiki/2013 %E 2 %80% 9314_Bundesliga#Results' # a pattern on the header to find the table among others title_pattern = 'Home ╲ Away' # a pattern on the class properties of the table used to find it table_props = { \"class\" : \"wikitable\" } # parse the table from the wiki page wt = Wiki_Table ( wikipage , table_props , title_pattern ) # create a dataframe on the data table_csv = map ( lambda x : ',' . join ( x ), wt . table ) table_csv = ' \\n ' . join ( table_csv ) data = StringIO ( table_csv ) #df = pd.read_csv(data,delimiter=',') df = pd . read_csv ( data , delimiter = ',' , index_col = 0 ) print df AUG LEV FCB DOR MGL BRS FRA FRE HAM H96 \\ Home / Away FC Augsburg NaN 1-3 1-0 0-4 2-2 4-1 2-1 2-1 3-1 1-1 Bayer Leverkusen 2-1 NaN 1-1 2-2 4-2 1-1 0-1 3-1 5-3 2-0 Bayern Munich 3-0 2-1 NaN 0-3 3-1 2-0 5-0 4-0 3-1 2-0 Borussia Dortmund 2-2 0-1 0-3 NaN 1-2 2-1 4-0 5-0 6-2 1-0 Borussia Mönchengladbach 1-2 0-1 0-2 2-0 NaN 4-1 4-1 1-0 3-1 3-0 Eintracht Braunschweig 0-1 1-0 0-2 1-2 1-1 NaN 0-2 0-1 4-2 3-0 Eintracht Frankfurt 1-1 0-2 0-1 1-2 1-0 3-0 NaN 1-4 2-2 2-3 SC Freiburg 2-4 3-2 1-1 0-1 4-2 2-0 1-1 NaN 0-3 2-1 Hamburger SV 0-1 2-1 1-4 3-0 0-2 4-0 1-1 1-1 NaN 3-1 Hannover 96 2-1 1-1 0-4 0-3 3-1 0-0 2-0 3-2 2-1 NaN Hertha BSC 0-0 0-1 1-3 0-4 1-0 2-0 6-1 0-0 1-0 0-3 1899 Hoffenheim 2-0 1-2 1-2 2-2 2-1 3-1 0-0 3-3 3-0 3-1 Mainz 05 3-0 1-4 0-2 1-3 0-0 2-0 1-0 2-0 3-2 2-0 1. FC Nürnberg 0-1 1-4 0-2 1-1 0-2 2-1 2-5 0-3 0-5 0-2 Schalke 04 4-1 2-0 0-4 1-3 0-1 3-1 2-0 2-0 3-3 2-0 VfB Stuttgart 1-4 0-1 1-2 2-3 0-2 2-2 1-1 2-0 1-0 4-2 Werder Bremen 1-0 1-0 0-7 1-5 1-1 0-0 0-3 0-0 1-0 3-2 VfL Wolfsburg 1-1 3-1 1-6 2-1 3-1 0-2 2-1 2-2 1-1 1-3 BSC HOF MAI NUR S04 STU BRE WOL Home / Away FC Augsburg 0-0 2-0 2-1 0-1 1-2 2-1 3-1 1-2 Bayer Leverkusen 2-1 2-3 0-1 3-0 1-2 2-1 2-1 3-1 Bayern Munich 3-2 3-3 4-1 2-0 5-1 1-0 5-2 1-0 Borussia Dortmund 1-2 3-2 4-2 3-0 0-0 6-1 1-0 2-1 Borussia Mönchengladbach 3-0 2-2 3-1 3-1 2-1 1-1 4-1 2-2 Eintracht Braunschweig 0-2 1-0 3-1 1-1 2-3 0-4 0-1 1-1 Eintracht Frankfurt 1-0 1-2 2-0 1-1 3-3 2-1 0-0 1-2 SC Freiburg 1-1 1-1 1-2 3-2 0-2 1-3 3-1 0-3 Hamburger SV 0-3 1-5 2-3 2-1 0-3 3-3 0-2 1-3 Hannover 96 1-1 1-4 4-1 3-3 2-1 0-0 1-2 2-0 Hertha BSC NaN 1-1 3-1 1-3 0-2 0-1 3-2 1-2 1899 Hoffenheim 2-3 NaN 2-4 2-2 3-3 4-1 4-4 6-2 Mainz 05 1-1 2-2 NaN 2-0 0-1 3-2 3-0 2-0 1. FC Nürnberg 2-2 4-0 1-1 NaN 0-0 2-0 0-2 1-1 Schalke 04 2-0 4-0 0-0 4-1 NaN 3-0 3-1 2-1 VfB Stuttgart 1-2 6-2 1-2 1-1 3-1 NaN 1-1 1-2 Werder Bremen 2-0 3-1 2-3 3-3 1-1 1-1 NaN 1-3 VfL Wolfsburg 2-0 2-1 3-0 4-1 4-0 3-1 3-0 NaN In [3]: # store the code to the file % history -f Wiki_Table.py -l 1 File u'Wiki_Table.py' exists. Overwrite? y Overwriting file. $$\\\\[5pt]$$ Final preparation of the dataframe Clearly, we need do some cleaning here. Let's split the score into two numeric columns. In [4]: df . index = df . columns rows = [] for i in df . index : for c in df . columns : if i == c : continue score = df . ix [ i , c ] score = [ int ( row ) for row in score . split ( '-' )] rows . append ([ i , c , score [ 0 ], score [ 1 ]]) df_update = pd . DataFrame ( rows , columns = [ 'home' , 'away' , 'home_score' , 'away_score' ]) df_update . head () Out[4]: home away home_score away_score 0 AUG LEV 1 3 1 AUG FCB 1 0 2 AUG DOR 0 4 3 AUG MGL 2 2 4 AUG BRS 4 1 Much better, but still not done. We need an easy way to refer to the teams. Let's create a lookup table, which maps team name to a unique integer i. In [5]: teams = df_update . home . unique () teams = pd . DataFrame ( teams , columns = [ 'team' ]) teams [ 'i' ] = teams . index print \"Number of teams: {0}\" . format ( len ( teams )) teams . head () Number of teams: 18 Out[5]: team i 0 AUG 0 1 LEV 1 2 FCB 2 3 DOR 3 4 MGL 4 Now, we can merge this into our main dataframe to create the columns i_home and i_away. In [6]: df = pd . merge ( df_update , teams , left_on = 'home' , right_on = 'team' , how = 'left' ) df = df . rename ( columns = { 'i' : 'i_home' }) . drop ( 'team' , 1 ) df = pd . merge ( df , teams , left_on = 'away' , right_on = 'team' , how = 'left' ) df = df . rename ( columns = { 'i' : 'i_away' }) . drop ( 'team' , 1 ) df . head () Out[6]: home away home_score away_score i_home i_away 0 AUG LEV 1 3 0 1 1 AUG FCB 1 0 0 2 2 AUG DOR 0 4 0 3 3 AUG MGL 2 2 0 4 4 AUG BRS 4 1 0 5 Now, let's extract the data into arrays, so that pymc can work with it. Note that each of the arrays (observed_home_goals, observed_away_goals, home_team, away_team) are the same length, and that the ith entry of each refers to the same game. In [7]: observed_home_goals = df . home_score . values observed_away_goals = df . away_score . values home_team = df . i_home . values away_team = df . i_away . values num_teams = len ( df . i_home . unique ()) num_games = len ( home_team ) print \"Number of games: {0}\" . format ( num_games ) Number of games: 306 This next step is unnecessary, but it would be nice to do it - come up with some decent starting values for the att and def parameters. The log transformation will make sense shortly. In [8]: g = df . groupby ([ 'i_away' ]) att_starting_points = np . log ( g . away_score . mean ()) g = df . groupby ( 'i_home' ) def_starting_points = - np . log ( g . away_score . mean ()) # negative because this property plays # a negative role $$\\\\[5pt]$$ The model In the post [1] , they model the observed goals-scored counts as follows. The Bundes league is made up by a total of T= 18 teams, playing each other once in a season. We indicate the number of points scored by the home and the away team in the g-th game of the season (306 games) as $y_{g1}$ and $y_{g2}$ respectively. The vector of observed counts $\\mathbb{y} = (y_{g1}, y_{g2})$ is modelled as independent Poisson: $$y_{gi}| \\theta_{gj} \\sim\\;\\; Poisson(\\theta_{gj}),$$ where the $\\theta_{gj}$ parameters represent the scoring intensity in the g-th game for the team playing at home (j=1) and away (j=2), respectively. We model these parameters according to a formulation that has been used widely in the statistical literature, assuming a log-linear random effect model: $$log \\theta_{g1} = home + att_{h(g)} + def_{a(g)} $$$$log \\theta_{g2} = att_{a(g)} + def_{h(g)}$$ the parameter home represents the advantage for the team hosting the game and we assume that this effect is constant for all the teams and throughout the season. Here h(g) and a(g) denote the home and away teams in the g-th game. Note that we are breaking out team strength into attacking and defending strength. A large negative defense parameter will sap the mojo from the opposing team's attacking parameter. The variable $home$ is modelled as a fixed effect, assuming a standard flat prior distribution. We use the notation of describing the Normal distribution in terms of mean and the precision. $$home \\sim\\; Normal(0,0.0001).$$ Conversely, for each t = 1, ..., 18, the team-specific effects are modelled as exchangeable from a common distribution: $$att_t \\sim\\; Normal(\\mu_{att}, \\tau_{att})$$ and $$def_t \\sim\\; Normal(\\mu_{def}, \\tau_{def}).$$ To ensure identifiability, we also impose a sum-to-zero constraint on the attack and defense parameters: $$\\sum_t&#94;T att_t =0$$$$\\sum_t&#94;T def_t =0$$ We could simplify our model. It didn't make sense to me to have a $\\mu_{att}$ when we're enforcing the sum-to-zero constraint by subtracting the mean anyway. So I eliminated $\\mu_{att}$ and $\\mu_{def}$: $$att_t \\sim\\; Normal(0, \\tau_{att})$$ and $$def_t \\sim\\; Normal(0, \\tau_{def}).$$ Also because of the sum-to-zero constraint, it seemed to me that we needed an intercept term in the log-linear model, capturing the average goals scored per game by the away team. This we model with the intercept : $$log \\theta_{g1} = intercept+home + att_{h(g)} + def_{a(g)} $$$$log \\theta_{g2} = intercept+att_{a(g)} + def_{h(g)},$$ where intercept has a flat distribution $$intercept \\sim\\; Normal(0,0.0001).$$ Below is what the model looks like in the PyMC. In [9]: #defines the class for the Modeling class Model ( object ): \"\"\" A Model class :param np.ndarray data: The data to use for learning the model. \"\"\" def predict_match_map ( self , data ): raise NotImplementedError ( 'prediction not implemented for the class' ) def split_title ( self , title ): \"\"\"Change \"BaselineMethod\" to \"Baseline Method\".\"\"\" words = [] tmp = [ title [ 0 ]] for c in title [ 1 :]: if c . isupper (): words . append ( '' . join ( tmp )) tmp = [ c ] else : tmp . append ( c ) words . append ( '' . join ( tmp )) return ' ' . join ( words ) def find_map ( self ): ''' calculates the MAP ''' raise NotImplementedError ( 'find_map not implemented yet' ) def load_map ( self ): ''' loads the calculated MAP ''' raise NotImplementedError ( 'load_map not implemented yet' ) def draw_samples ( self ): ''' makes simulations ''' raise NotImplementedError ( 'draw_samples not implemented yet' ) def __str__ ( self ): ''' string presentation of the class ''' return self . split_title ( self . __class__ . __name__ ) def __init__ ( self , data ): logging . info ( 'building the model....' ) #observed points in each game self . observed_home_goals = data [ 'observed_home_goals' ] self . observed_away_goals = data [ 'observed_away_goals' ] #hyperpriors in the model # parameter common to all teams # the 'home' parameter self . home = pm . Normal ( 'home' , 0 , . 0001 , value = 0 ) self . num_teams = data [ 'num_teams' ] self . home_team = data [ 'home_team' ] self . away_team = data [ 'away_team' ] # 'precision' parameters in the team-specific 'att' or 'def' srength # it doesn't matter if we use Uniform or Gamma priors self . tau_att = pm . Gamma ( 'tau_att' , . 1 , . 1 , value = 10 ) self . tau_def = pm . Gamma ( 'tau_def' , . 1 , . 1 , value = 10 ) # the 'intercept' parameter to get the average of goals scored per game self . intercept = pm . Normal ( 'intercept' , 0 , . 0001 , value = 0 ) #team-specific parameters self . team_atts = pm . Normal ( \"team_atts\" , mu = 0 , tau = self . tau_att , size = self . num_teams , value = att_starting_points . values ) self . team_defs = pm . Normal ( \"team_defs\" , mu = 0 , tau = self . tau_def , size = self . num_teams , value = def_starting_points . values ) # trick to code the sum to zero contraint @pm.deterministic def atts ( team_atts = self . team_atts ): atts = team_atts . copy () atts = atts - np . mean ( team_atts ) return atts @pm.deterministic def defs ( team_defs = self . team_defs ): defs = team_defs . copy () defs = defs - np . mean ( team_defs ) return defs # the hiearchical parameters: parameter 'theta_home' @pm.deterministic def home_theta ( home_team = self . home_team , away_team = self . away_team , home = self . home , atts = atts , defs = defs , intercept = self . intercept ): return np . exp ( intercept + home + atts [ home_team ] + defs [ away_team ]) # parameter 'theta_away' @pm.deterministic def away_theta ( home_team = self . home_team , away_team = self . away_team , home = self . home , atts = atts , defs = defs , intercept = self . intercept ): return np . exp ( intercept + atts [ away_team ] + defs [ home_team ]) # what we want to predict in the model is points self . home_points = pm . Poisson ( 'home_points' , mu = home_theta , value = self . observed_home_goals , observed = True ) self . away_points = pm . Poisson ( 'away_points' , mu = away_theta , value = self . observed_away_goals , observed = True ) # our model collects all definitions in self.model self . model = pm . Model ([ self . home , self . intercept , self . tau_att , self . tau_def , home_theta , away_theta , self . team_atts , self . team_defs , atts , defs , self . home_points , self . away_points ]) logging . info ( 'done building the model' ) # create the model object used in the further analysis observed_data_model = { 'observed_home_goals' : observed_home_goals , 'observed_away_goals' : observed_away_goals , 'num_teams' : num_teams , 'home_team' : home_team , 'away_team' : away_team , } bundesliga_model = Model ( observed_data_model ) ? bundesliga_model INFO:root:building the model.... INFO:root:done building the model Here we like to add all functionality of the basic Model: MAP calculation samples drawing simulation of the season in the bundesliga. We start with the MAP stuff. In [11]: ''' a MAP support in the Model Class ''' import os import scipy as sp import time try : import ujson as json except ImportError : import json # First, we define functions to save/load our parameters to/from the file. def save_np_vars ( vars , savedir ): \"\"\" Save a dictionary of numpy variables to `savedir`. We assume the directory does not exist; an OSError will be raised if it does. \"\"\" logging . info ( 'writing numpy vars to directory: %s ' % savedir ) os . mkdir ( savedir ) shapes = {} for varname in vars : data = vars [ varname ] var_file = os . path . join ( savedir , varname + '.txt' ) np . savetxt ( var_file , data . reshape ( - 1 , data . size )) shapes [ varname ] = data . shape ## Store shape information for reloading. shape_file = os . path . join ( savedir , 'shapes.json' ) with open ( shape_file , 'w' ) as sfh : json . dump ( shapes , sfh ) return # Define the function to upload our parameters from the file def load_np_vars ( savedir ): \"\"\"Load numpy variables saved with `save_np_vars`.\"\"\" shape_file = os . path . join ( savedir , 'shapes.json' ) with open ( shape_file , 'r' ) as sfh : shapes = json . load ( sfh ) vars = {} for varname , shape in shapes . items (): var_file = os . path . join ( savedir , varname + '.txt' ) vars [ varname ] = np . loadtxt ( var_file ) . reshape ( shape ) return vars # property 'map_dir' def _map_dir ( self ): ''' return the dir name where we store the MAP''' self . basename_map = 'map' return os . path . join ( 'data' , self . basename_map ) # calculatiton the MAP def _find_map ( self ): \"\"\"Find mode of posterior using Powell optimization.\"\"\" tstart = time . time () logging . info ( 'finding PMF MAP using Powell optimization...' ) self . _map = pm . MAP ( self . model ) self . _map . fit () elapsed = int ( time . time () - tstart ) logging . info ( 'found PMF MAP in %d seconds' % elapsed ) #pymc2 # This is going to take a good deal of time to find, so let's save it. savedir = os . path . join ( 'data' , self . basename_map ) map_to_save = {} for var in list ( self . _map . variables ): print \"var: \" , var map_to_save . update ({ str ( var ): var . value }) self . _map = map_to_save save_np_vars ( self . _map , self . map_dir ) logging . info ( 'MAP was stored in %s folder' % savedir ) return # load a map def _load_map ( self ): \"\"\"load map from the directory\"\"\" self . _map = load_np_vars ( self . map_dir ) return def _map ( self ): \"\"\" a MAP property\"\"\" try : return self . _map except : if os . path . isdir ( self . map_dir ): self . load_map () else : self . find_map () return self . _map # Update our class with the new MAP infrastructure. Model . find_map = _find_map Model . load_map = _load_map Model . map_dir = property ( _map_dir ) Model . map = property ( _map ) # test: print a MAP for some variable: 'tau_att' logging . info ( 'MAP of %s is %2.4f ' % ( 'tau_att' , bundesliga_model . map [ 'tau_att' ])) #print 'saving...' #save_np_vars(bundesliga_model.map,bundesliga_model.map_dir) INFO:root:MAP of tau_att is 14.6564 saving... Now we would like to introduce the MCMC sampling. In [12]: ''' a MCMC support in the Model Class ''' # Draw MCMC samples. def _trace_dir ( self ): ''' trace dir property''' self . basename_mcmc = 'mcmc' return os . path . join ( 'data' , self . basename_mcmc ) # Sampler def _draw_samples ( self , nsamples = 1000 ): ''' draws out samples ''' # First make sure the trace_dir does not already exist. if os . path . isdir ( self . trace_dir ): raise OSError ( 'trace directory %s already exists. Please move or delete.' % self . trace_dir ) # pymc2 logging . info ( 'drawing %d samples' % ( nsamples )) self . mcmc = pm . MCMC ( self . model , db = 'txt' , dbname = self . trace_dir ) logging . info ( 'backing up trace to directory: %s ' % self . trace_dir ) self . mcmc . sample ( nsamples ) self . traces = {} for var in list ( self . mcmc . stochastics ): self . traces . update ( { str ( var ): self . mcmc . trace ( str ( var ))[:] } ) for var in list ( self . mcmc . deterministics ): self . traces . update ( { str ( var ): self . mcmc . trace ( str ( var ))[:] } ) self . mcmc . db . close () def _load_trace ( self ): ''' loads traces of MCMC to the model ''' self . mcmc = pm . MCMC ( self . model , db = pm . database . txt . load ( self . trace_dir )) self . traces = {} for var in list ( self . mcmc . stochastics ): self . traces . update ( { str ( var ): self . mcmc . trace ( str ( var ))[:] } ) for var in list ( self . mcmc . deterministics ): self . traces . update ( { str ( var ): self . mcmc . trace ( str ( var ))[:] } ) # Update our class with the sampling infrastructure. Model . trace_dir = property ( _trace_dir ) Model . draw_samples = _draw_samples Model . load_trace = _load_trace # let's make samples bundesliga_model . draw_samples ( 100000 ) INFO:root:drawing 100000 samples INFO:root:backing up trace to directory: data/mcmc [-----------------100%-----------------] 100000 of 100000 complete in 98.7 sec $$\\\\[5pt]$$ Diagnostics Let's see if/how the model converged. The home parameter looks good, and indicates that home field advantage amounts to goals per game at the intercept. We can see that it converges just like the model for the Premier League in the other tutorial. I wonder and this is left as a question if all field sports have models of this form that converge. In [13]: # 'home parameter' pm . Matplot . plot ( bundesliga_model . home ) # 'tau_att parameter for all commands' pm . Matplot . plot ( bundesliga_model . tau_att ) Plotting home Plotting tau_att $$\\\\[5pt]$$ Visualization We want to add additional functionality to our model: the plotter which will make plottings of teams' strength. The strategy is the following: We will plot the additional information about team like Qualification or relegation taken from [4] together with distribution of the paramters defs and atts . First, we scrappe data from the wiki. In [14]: # a page containing the data wikipage = 'https://en.wikipedia.org/wiki/2013 %E 2 %80% 9314_Bundesliga#League_table' # a pattern on the header to find the table among others title_pattern = 'Pos' # a pattern on the class properties of the table used to find it table_props = { \"class\" : \"wikitable\" } # parse the table from the wiki page wt = Wiki_Table ( wikipage , table_props , title_pattern , \"th\" ) #print wt.table # create a dataframe on the data table_csv = map ( lambda x : ',' . join ( x ), wt . table ) table_csv = ' \\n ' . join ( table_csv ) data = StringIO ( table_csv ) df_final_table = pd . read_csv ( data , delimiter = ',' , header = None ) df_final_table . columns = [ 'Pos' , 'Team' , 'Pld' , 'W' , 'D' , 'L' , 'GF' , 'GA' , 'GD' , 'Pts' , 'Qualification' ] # fix some 'Qualification' values df_final_table . loc [ 0 : 2 , 'Qualification' ] = 'Champions_League_Group' df_final_table . loc [ 3 , 'Qualification' ] = 'Champions_PlayOff_Round' df_final_table . loc [ 4 , 'Qualification' ] = 'Europe_League_Group' df_final_table . loc [ 5 , 'Qualification' ] = 'Europe_League_PlayOff_Round' df_final_table . loc [ 6 , 'Qualification' ] = 'Europe_League_PlayOff_3rdRound' df_final_table . loc [ len ( df_final_table ) - 3 , 'Qualification' ] = 'Relegation_PlayOff' df_final_table . loc [ len ( df_final_table ) - 2 , 'Qualification' ] = 'Relegation_2ndBundesLiga' df_final_table . loc [ len ( df_final_table ) - 1 , 'Qualification' ] = 'Relegation_2ndBundesLiga' print df_final_table . head ( 8 ) Pos Team Pld W D L GF GA GD Pts \\ 0 1 Bayern Munich (C) 34 29 3 2 94 23 +71 90 1 2 Borussia Dortmund 34 22 5 7 80 38 +42 71 2 3 Schalke 04 34 19 7 8 63 43 +20 64 3 4 Bayer Leverkusen 34 19 4 11 60 41 +19 61 4 5 VfL Wolfsburg 34 18 6 10 63 50 +13 60 5 6 Borussia Mönchengladbach 34 16 7 11 59 43 +16 55 6 7 Mainz 05 34 16 5 13 52 54 −2 53 7 8 FC Augsburg 34 15 7 12 47 47 0 52 Qualification 0 Champions_League_Group 1 Champions_League_Group 2 Champions_League_Group 3 Champions_PlayOff_Round 4 Europe_League_Group 5 Europe_League_PlayOff_Round 6 Europe_League_PlayOff_3rdRound 7 NaN In [15]: print df_final_table . tail ( 3 ) Pos Team Pld W D L GF GA GD Pts \\ 15 16 Hamburger SV (O) 34 7 6 21 51 75 −24 27 16 17 1. FC Nürnberg (R) 34 5 11 18 37 70 −33 26 17 18 Eintracht Braunschweig (R) 34 6 7 21 29 60 −31 25 Qualification 15 Relegation_PlayOff 16 Relegation_2ndBundesLiga 17 Relegation_2ndBundesLiga In order to plot the add. info, we need to add abbreviations of the teams. In [16]: # add abbvreviations df_final_table [ 'Abbvr' ] = pd . Series ( np . random . randn (), index = df_final_table . index ) def find_team ( name ): return np . array ([ name in val for val in df_final_table [ 'Team' ]]) # the whole list of teams in the 2013-2014 season of the BundesLiga df_final_table . loc [ find_team ( \"Bayern Munich\" ), 'Abbvr' ] = \"FCB\" #1 df_final_table . loc [ find_team ( \"Borussia Dortmund\" ), 'Abbvr' ] = \"DOR\" #2 df_final_table . loc [ find_team ( \"Schalke 04\" ), 'Abbvr' ] = \"S04\" #3 df_final_table . loc [ find_team ( \"Bayer Leverkusen\" ), 'Abbvr' ] = \"LEV\" #4 df_final_table . loc [ find_team ( \"VfL Wolfsburg\" ), 'Abbvr' ] = \"WOL\" #5 df_final_table . loc [ find_team ( \"nchengladbach\" ), 'Abbvr' ] = \"MGL\" #6 df_final_table . loc [ find_team ( \"Mainz 05\" ), 'Abbvr' ] = \"MAI\" #7 df_final_table . loc [ find_team ( \"FC Augsburg\" ), 'Abbvr' ] = \"AUG\" #8 df_final_table . loc [ find_team ( \"1899 Hoffenheim\" ), 'Abbvr' ] = \"HOF\" #9 df_final_table . loc [ find_team ( \"Hannover 96\" ), 'Abbvr' ] = \"H96\" #10 df_final_table . loc [ find_team ( \"Hertha BSC\" ), 'Abbvr' ] = \"BSC\" #11 df_final_table . loc [ find_team ( \"Werder Bremen\" ), 'Abbvr' ] = \"BRE\" #12 df_final_table . loc [ find_team ( \"Eintracht Frankfurt\" ), 'Abbvr' ] = \"FRA\" #13 df_final_table . loc [ find_team ( \"SC Freiburg\" ), 'Abbvr' ] = \"FRE\" #14 df_final_table . loc [ find_team ( \"VfB Stuttgart\" ), 'Abbvr' ] = \"STU\" #15 df_final_table . loc [ find_team ( \"Hamburger SV\" ), 'Abbvr' ] = \"HAM\" #16 df_final_table . loc [ find_team ( \"rnberg\" ), 'Abbvr' ] = \"NUR\" #17 df_final_table . loc [ find_team ( \"Eintracht Braunschweig\" ), 'Abbvr' ] = \"BRS\" #18 print df_final_table . head ( 10 ) print df_final_table . tail ( 3 ) Pos Team Pld W D L GF GA GD Pts \\ 0 1 Bayern Munich (C) 34 29 3 2 94 23 +71 90 1 2 Borussia Dortmund 34 22 5 7 80 38 +42 71 2 3 Schalke 04 34 19 7 8 63 43 +20 64 3 4 Bayer Leverkusen 34 19 4 11 60 41 +19 61 4 5 VfL Wolfsburg 34 18 6 10 63 50 +13 60 5 6 Borussia Mönchengladbach 34 16 7 11 59 43 +16 55 6 7 Mainz 05 34 16 5 13 52 54 −2 53 7 8 FC Augsburg 34 15 7 12 47 47 0 52 8 9 1899 Hoffenheim 34 11 11 12 72 70 +2 44 9 10 Hannover 96 34 12 6 16 46 59 −13 42 Qualification Abbvr 0 Champions_League_Group FCB 1 Champions_League_Group DOR 2 Champions_League_Group S04 3 Champions_PlayOff_Round LEV 4 Europe_League_Group WOL 5 Europe_League_PlayOff_Round MGL 6 Europe_League_PlayOff_3rdRound MAI 7 NaN AUG 8 NaN HOF 9 NaN H96 Pos Team Pld W D L GF GA GD Pts \\ 15 16 Hamburger SV (O) 34 7 6 21 51 75 −24 27 16 17 1. FC Nürnberg (R) 34 5 11 18 37 70 −33 26 17 18 Eintracht Braunschweig (R) 34 6 7 21 29 60 −31 25 Qualification Abbvr 15 Relegation_PlayOff HAM 16 Relegation_2ndBundesLiga NUR 17 Relegation_2ndBundesLiga BRS Here is our plotter class. In [17]: #defines the class for the Modeling class Plotter ( object ): \"\"\" A Plotter class \"\"\" def __init__ ( self , model , table ): self . model = model self . table = table self . deterministic_stats = {} for var in list ( self . model . model . deterministics ): self . deterministic_stats . update ({ str ( var ): var . stats ()}) def plot_final_table ( self , prop1 , prop2 , selection , type_plot ): ''' a scatter plot of the properties prop1 and prop2 calculated by the model''' raise NotImplementedError ( 'baseline prediction not implemented for base class' ) $$\\\\[5pt]$$ Plot of the Mean of Posteriors In [18]: ''' Plot posterior parameters of the teams selected from the final table. ''' def _plot_final_table ( self , prop1 , prop2 , selection , type_plot ): df_avg = pd . DataFrame ({ prop1 : self . deterministic_stats [ prop1 ][ 'mean' ], prop2 : self . deterministic_stats [ prop2 ][ 'mean' ], 'teams' : teams . team . values }, index = teams . team . values ) df_avg = pd . merge ( df_avg , self . table , left_index = True , right_on = 'Abbvr' , how = 'left' ) def select_team ( selections_crit ): return np . array ([ val in selections_crit for val in df_final_table [ 'Qualification' ]]) sns . set_context ( \"poster\" ) plt . figure ( figsize = ( 15 , 8 )) g = sns . FacetGrid ( df_avg . loc [ select_team ( selection )], hue = type_plot , size = 10 ) g . map ( plt . scatter , prop1 , prop2 , s = 50 , alpha =. 7 , linewidth =. 5 , edgecolor = \"white\" ) g . add_legend (); # Update our Plotter Plotter . plot_final_table = _plot_final_table selection_criteria = [ 'Champions_League_Group' , 'Champions_PlayOff_Round' , 'Europe_League_Group' , 'Europe_League_PlayOff_Round' , 'Europe_League_PlayOff_3rdRound' , 'Relegation_PlayOff' , 'Relegation_2ndBundesLiga' , 'Relegation_2ndBundesLiga' ] plotting = Plotter ( bundesliga_model , df_final_table ) plotting . plot_final_table ( 'atts' , 'defs' , selection_criteria , 'Qualification' ) <matplotlib.figure.Figure at 0xd2ab24c> In [19]: plotting . plot_final_table ( 'atts' , 'defs' , selection_criteria , 'teams' ) <matplotlib.figure.Figure at 0xc9613ac> So as you would expect, the top teams are in the lower right side of the chart, indicating a positive attack effect and a negative defense effect. Interestingly, FCB appears to have both a stronger attack and a stronger defense than two teams that finished above it in the table, DOR and LEV. But we're just looking at posterior means here, and we ought to take advantage of the fact that we can quantify our posterior uncertainty around these parameters. $$\\\\[5pt]$$ Plot of the HPDI Let's look at the Highest Posterior Density intervals (HPDI) for the attack parameters. In [20]: ''' Plot posterior parameters of the teams selected from the final table. ''' def _plot_HPDI ( self , prop1 ): #print deterministic_stats[prop1]['95% HPD interval'].T df_hpd = pd . DataFrame ( self . deterministic_stats [ prop1 ][ '95% HPD interval' ] . T , columns = [ 'hpd_low' , 'hpd_high' ], index = teams . team . values ) df_median = pd . DataFrame ( self . deterministic_stats [ prop1 ][ 'quantiles' ][ 50 ], columns = [ 'hpd_median' ], index = teams . team . values ) df_hpd = df_hpd . join ( df_median ) df_hpd [ 'relative_lower' ] = df_hpd . hpd_median - df_hpd . hpd_low df_hpd [ 'relative_upper' ] = df_hpd . hpd_high - df_hpd . hpd_median df_hpd = df_hpd . sort_index ( by = 'hpd_median' ) df_hpd = df_hpd . reset_index () df_hpd [ 'x' ] = df_hpd . index + . 5 sns . set_context ( \"poster\" ) fig , axs = plt . subplots ( figsize = ( 10 , 4 )) axs . errorbar ( df_hpd . x , df_hpd . hpd_median , yerr = ( df_hpd [[ 'relative_lower' , 'relative_upper' ]] . values ) . T , fmt = 'o' ) axs . set_title ( 'HPD of %s , by Team' % prop1 ) axs . set_xlabel ( 'Team' ) axs . set_ylabel ( 'Posterior %s ' % prop1 ) _ = axs . set_xticks ( df_hpd . index + . 5 ) _ = axs . set_xticklabels ( df_hpd [ 'index' ] . values , rotation = 45 ) # Update our Plotter Plotter . plot_HPDI = _plot_HPDI plotting . plot_HPDI ( 'atts' ) plotting . plot_HPDI ( 'defs' ) $$\\\\[5pt]$$ Simulations We can take draws from the posterior distributions of the parameters, and simulate a season or many seasons. Below is the simulation code, just so you can see how I did it. In [21]: ''' Simulation support in the Model Class ''' def _simulate_season ( self , df = None ): \"\"\" Simulate a season once, using one random draw from the mcmc chain. \"\"\" # get the number of samples num_samples = self . traces [ 'home' ] . shape [ 0 ] # randomly draw a seasson number from traces draw = np . random . randint ( 0 , num_samples ) atts_draw = pd . DataFrame ({ 'att' : self . traces [ \"atts\" ][ draw , :],}) defs_draw = pd . DataFrame ({ 'def' : self . traces [ \"defs\" ][ draw , :],}) home_draw = self . traces [ \"home\" ][ draw ] intercept_draw = self . traces [ \"intercept\" ][ draw ] # make a season dataframe with new columns: att_home, def_home,att_away,def_away season = df . copy () season = pd . merge ( season , atts_draw , left_on = 'i_home' , right_index = True ) season = pd . merge ( season , defs_draw , left_on = 'i_home' , right_index = True ) season = season . rename ( columns = { 'att' : 'att_home' , 'def' : 'def_home' }) season = pd . merge ( season , atts_draw , left_on = 'i_away' , right_index = True ) season = pd . merge ( season , defs_draw , left_on = 'i_away' , right_index = True ) season = season . rename ( columns = { 'att' : 'att_away' , 'def' : 'def_away' }) # add random model paramers values generated for this season: home, intercept, home_theta etc season [ 'home' ] = home_draw # home parameter season [ 'intercept' ] = intercept_draw # intercept parameter season [ 'home_theta' ] = season . apply ( lambda x : math . exp ( x [ 'intercept' ] + x [ 'home' ] + x [ 'att_home' ] + x [ 'def_away' ]), axis = 1 ) season [ 'away_theta' ] = season . apply ( lambda x : math . exp ( x [ 'intercept' ] + x [ 'att_away' ] + x [ 'def_home' ]), axis = 1 ) # the most interesting part: simulate the score of each match season [ 'home_goals' ] = season . apply ( lambda x : np . random . poisson ( x [ 'home_theta' ]), axis = 1 ) season [ 'away_goals' ] = season . apply ( lambda x : np . random . poisson ( x [ 'away_theta' ]), axis = 1 ) season [ 'home_outcome' ] = season . apply ( lambda x : 'win' if x [ 'home_goals' ] > x [ 'away_goals' ] else 'loss' if x [ 'home_goals' ] < x [ 'away_goals' ] else 'draw' , axis = 1 ) season [ 'away_outcome' ] = season . apply ( lambda x : 'win' if x [ 'home_goals' ] < x [ 'away_goals' ] else 'loss' if x [ 'home_goals' ] > x [ 'away_goals' ] else 'draw' , axis = 1 ) # add dummy columns for categorical variables: each dummy will take 1 or 0 season = season . join ( pd . get_dummies ( season . home_outcome , prefix = 'home' )) season = season . join ( pd . get_dummies ( season . away_outcome , prefix = 'away' )) return season def _create_season_table ( self , season ): \"\"\" Using a season dataframe output by simulate_season(), create a summary dataframe with wins, losses, goals for, etc. \"\"\" # get summary on home teams g = season . groupby ( 'i_home' ) home = pd . DataFrame ({ 'home_goals' : g . home_goals . sum (), 'home_goals_against' : g . away_goals . sum (), 'home_wins' : g . home_win . sum (), 'home_draws' : g . home_draw . sum (), 'home_losses' : g . home_loss . sum () }) # get summary on away teams g = season . groupby ( 'i_away' ) away = pd . DataFrame ({ 'away_goals' : g . away_goals . sum (), 'away_goals_against' : g . home_goals . sum (), 'away_wins' : g . away_win . sum (), 'away_draws' : g . away_draw . sum (), 'away_losses' : g . away_loss . sum () }) df = home . join ( away ) # how many 'x' were during the season for each command df [ 'wins' ] = df . home_wins + df . away_wins # x=wins df [ 'draws' ] = df . home_draws + df . away_draws # x=draws df [ 'losses' ] = df . home_losses + df . away_losses # x=losses df [ 'points' ] = df . wins * 3 + df . draws # total points collected df [ 'gf' ] = df . home_goals + df . away_goals # total goals scored df [ 'ga' ] = df . home_goals_against + df . away_goals_against # total goals missed df [ 'gd' ] = df . gf - df . ga # difference df = pd . merge ( teams , df , left_on = 'i' , right_index = True ) # add teams information df = df . sort_index ( by = 'points' , ascending = False ) # sort teams by the points collected df = df . reset_index () df [ 'position' ] = df . index + 1 df [ 'champion' ] = ( df . position == 1 ) . astype ( int ) df [ 'qualified_for_CL' ] = ( df . position < 5 ) . astype ( int ) df [ 'relegated' ] = ( df . position > 17 ) . astype ( int ) return df def _simulate_seasons ( self , n = 100 , df = None ): ''' simulates n seasons ''' dfs = [] for i in range ( n ): s = self . simulate_season ( df ) t = self . create_season_table ( s ) t [ 'iteration' ] = i dfs . append ( t ) return pd . concat ( dfs , ignore_index = True ) # Update our class with the sampling infrastructure. Model . simulate_season = _simulate_season Model . create_season_table = _create_season_table Model . simulate_seasons = _simulate_seasons seasons_simulated = bundesliga_model . simulate_seasons ( 100 , df ) We extend our plotter to plot simulated points and scored goals for each team. In [22]: ''' Plot simulated points for the team with Abbvr ''' def _plot_sim_points ( self , abbvr , seasons_simulated ): ''' plot the simulated points ''' sns . set_context ( \"poster\" ) ax = seasons_simulated . points [ seasons_simulated . team == abbvr ] . hist ( figsize = ( 10 , 5 )) median = seasons_simulated . points [ seasons_simulated . team == abbvr ] . median () team_name = str ( self . table . loc [ self . table [ 'Abbvr' ] == abbvr , 'Team' ] . values [ 0 ]) num_sims = len ( seasons_simulated ) / self . model . traces [ 'atts' ] . shape [ 1 ] ax . set_title ( ' %s : 2013-14 points, %d simulations' % ( team_name . decode ( 'utf8' ), num_sims )) ax . plot ([ median , median ], ax . get_ylim ()) plt . annotate ( 'Median: %s ' % median , xy = ( median + 1 , ax . get_ylim ()[ 1 ] - 2 )) # Update our Plotter Plotter . plot_sim_points = _plot_sim_points # comparison DOR and FCB plotting . plot_sim_points ( 'FCB' , seasons_simulated ) plotting . plot_sim_points ( 'DOR' , seasons_simulated ) And how many goals they scored: In [23]: ''' Plot simulated scored goals for the team with Abbvr ''' def _plot_sim_scored_goals ( self , abbvr , seasons_simulated ): ''' plot the simulated scored goals ''' sns . set_context ( \"poster\" ) ax = seasons_simulated . gf [ seasons_simulated . team == abbvr ] . hist ( figsize = ( 10 , 5 )) median = seasons_simulated . gf [ seasons_simulated . team == abbvr ] . median () team_name = str ( self . table . loc [ self . table [ 'Abbvr' ] == abbvr , 'Team' ] . values [ 0 ]) num_sims = len ( seasons_simulated ) / self . model . traces [ 'atts' ] . shape [ 1 ] ax . set_title ( ' %s : 2013-14 scored goals, %d simulations' % ( team_name . decode ( 'utf8' ), num_sims )) ax . plot ([ median , median ], ax . get_ylim ()) plt . annotate ( 'Median: %s ' % median , xy = ( median + 1 , ax . get_ylim ()[ 1 ] - 2 )) # Update our Plotter Plotter . plot_sim_scored_goals = _plot_sim_scored_goals # comparison DOR and FCB plotting . plot_sim_scored_goals ( 'FCB' , seasons_simulated ) plotting . plot_sim_scored_goals ( 'DOR' , seasons_simulated ) In [24]: ''' Plot simulated missed goals for the team with Abbvr ''' def _plot_sim_missed_goals ( self , abbvr , seasons_simulated ): ''' plot the simulated missed goals ''' sns . set_context ( \"poster\" ) ax = seasons_simulated . ga [ seasons_simulated . team == abbvr ] . hist ( figsize = ( 10 , 5 )) median = seasons_simulated . ga [ seasons_simulated . team == abbvr ] . median () team_name = str ( self . table . loc [ self . table [ 'Abbvr' ] == abbvr , 'Team' ] . values [ 0 ]) num_sims = len ( seasons_simulated ) / self . model . traces [ 'atts' ] . shape [ 1 ] ax . set_title ( ' %s : 2013-14 missed goals, %d simulations' % ( team_name . decode ( 'utf8' ), num_sims )) ax . plot ([ median , median ], ax . get_ylim ()) plt . annotate ( 'Median: %s ' % median , xy = ( median + 1 , ax . get_ylim ()[ 1 ] - 2 )) # Update our Plotter Plotter . plot_sim_missed_goals = _plot_sim_missed_goals # comparison DOR and FCB plotting . plot_sim_missed_goals ( 'FCB' , seasons_simulated ) plotting . plot_sim_missed_goals ( 'DOR' , seasons_simulated ) We want to compare the simulated data with a real table for 2013/2014 years from the Wikipedia [4] . In [25]: ''' Plot final goals scored by each team ''' def _plot_goals ( self , seasons_simulated ): ''' plot the simulated and real scored goals ''' g = seasons_simulated . groupby ( 'team' ) season_hdis = pd . DataFrame ({ 'points_lower' : g . points . quantile ( . 05 ), 'points_upper' : g . points . quantile ( . 95 ), 'goals_for_lower' : g . gf . quantile ( . 05 ), 'goals_for_median' : g . gf . median (), 'goals_for_upper' : g . gf . quantile ( . 95 ), 'goals_against_lower' : g . ga . quantile ( . 05 ), 'goals_against_upper' : g . ga . quantile ( . 95 ), }) season_hdis = pd . merge ( season_hdis , self . table , left_index = True , right_on = 'Abbvr' ) column_order = [ 'Abbvr' , 'points_lower' , 'Pts' , 'points_upper' , 'goals_for_lower' , 'GF' , 'goals_for_median' , 'goals_for_upper' , 'goals_against_lower' , 'GA' , 'goals_against_upper' ,] season_hdis = season_hdis [ column_order ] season_hdis [ 'relative_goals_upper' ] = season_hdis . goals_for_upper - season_hdis . goals_for_median season_hdis [ 'relative_goals_lower' ] = season_hdis . goals_for_median - season_hdis . goals_for_lower season_hdis = season_hdis . sort_index ( by = 'GF' ) season_hdis = season_hdis . reset_index () season_hdis [ 'x' ] = season_hdis . index + . 5 sns . set_context ( \"poster\" ) fig , axs = plt . subplots ( figsize = ( 10 , 6 )) axs . scatter ( season_hdis . x , season_hdis . GF , c = sns . palettes . color_palette ()[ 4 ], zorder = 10 , label = 'Actual Goals For' ) axs . errorbar ( season_hdis . x , season_hdis . goals_for_median , yerr = ( season_hdis [[ 'relative_goals_lower' , 'relative_goals_upper' ]] . values ) . T , fmt = 's' , c = sns . palettes . color_palette ()[ 5 ], label = 'Simulations' ) axs . set_title ( 'Actual Goals For, and 90% Interval from Simulations, by Team' ) axs . set_xlabel ( 'Team' ) axs . set_ylabel ( 'Goals Scored' ) axs . set_xlim ( 0 , 20 ) axs . legend () _ = axs . set_xticks ( season_hdis . index + . 5 ) _ = axs . set_xticklabels ( season_hdis [ 'Abbvr' ] . values , rotation = 45 ) # Update our Plotter Plotter . plot_goals = _plot_goals # comparison plotting . plot_goals ( seasons_simulated ) $$\\\\[5pt]$$ Predictions of 2014/2015 BundesLiga results Let's try to predict results on 2014/2015 season [5] . We compare simulated 1000 seasons and plot our predictions together with real results. First, we generate 1000 seasons: In [29]: seasons_simulated_1000 = bundesliga_model . simulate_seasons ( 1000 , df ) Then, we scrappe the final table of BundesLiga results for the 2014/2015 season. In [30]: # a page containing the data wikipage = 'https://en.wikipedia.org/wiki/2014 %E 2 %80% 9315_Bundesliga#League_table' # a pattern on the header to find the table among others title_pattern = 'Pos<' # a pattern on the class properties of the table used to find it table_props = { \"class\" : \"wikitable\" } # parse the table from the wiki page wt = Wiki_Table ( wikipage , table_props , title_pattern , \"th\" ) #print wt.table # create a dataframe on the data table_csv = map ( lambda x : ',' . join ( x ), wt . table ) table_csv = ' \\n ' . join ( table_csv ) data = StringIO ( table_csv ) df_final_table_2015 = pd . read_csv ( data , delimiter = ',' , header = None ) df_final_table_2015 . columns = [ 'Team' , 'Pld' , 'W' , 'D' , 'L' , 'GF' , 'GA' , 'GD' , 'Pts' , 'Qualification' ] # fix some 'Qualification' values df_final_table_2015 . loc [ 0 : 2 , 'Qualification' ] = 'Champions_League_Group' df_final_table_2015 . loc [ 3 , 'Qualification' ] = 'Champions_PlayOff_Round' df_final_table_2015 . loc [ 4 , 'Qualification' ] = 'Europe_League_Group' df_final_table_2015 . loc [ 5 , 'Qualification' ] = 'Europe_League_PlayOff_Round' df_final_table_2015 . loc [ 6 , 'Qualification' ] = 'Europe_League_PlayOff_3rdRound' df_final_table_2015 . loc [ len ( df_final_table_2015 ) - 3 , 'Qualification' ] = 'Relegation_PlayOff' df_final_table_2015 . loc [ len ( df_final_table_2015 ) - 2 , 'Qualification' ] = 'Relegation_2ndBundesLiga' df_final_table_2015 . loc [ len ( df_final_table_2015 ) - 1 , 'Qualification' ] = 'Relegation_2ndBundesLiga' In [32]: # add abbvreviations df_final_table_2015 [ 'Abbvr' ] = pd . Series ( np . random . randn (), index = df_final_table . index ) def find_team ( name ): return np . array ([ name in val for val in df_final_table_2015 [ 'Team' ]]) # the whole list of teams in the 2013-2014 season of the BundesLiga df_final_table_2015 . loc [ find_team ( \"Bayern Munich\" ), 'Abbvr' ] = \"FCB\" #1 df_final_table_2015 . loc [ find_team ( \"Borussia Dortmund\" ), 'Abbvr' ] = \"DOR\" #2 df_final_table_2015 . loc [ find_team ( \"Schalke 04\" ), 'Abbvr' ] = \"S04\" #3 df_final_table_2015 . loc [ find_team ( \"Bayer Leverkusen\" ), 'Abbvr' ] = \"LEV\" #4 df_final_table_2015 . loc [ find_team ( \"VfL Wolfsburg\" ), 'Abbvr' ] = \"WOL\" #5 df_final_table_2015 . loc [ find_team ( \"nchengladbach\" ), 'Abbvr' ] = \"MGL\" #6 df_final_table_2015 . loc [ find_team ( \"Mainz 05\" ), 'Abbvr' ] = \"MAI\" #7 df_final_table_2015 . loc [ find_team ( \"FC Augsburg\" ), 'Abbvr' ] = \"AUG\" #8 df_final_table_2015 . loc [ find_team ( \"1899 Hoffenheim\" ), 'Abbvr' ] = \"HOF\" #9 df_final_table_2015 . loc [ find_team ( \"Hannover 96\" ), 'Abbvr' ] = \"H96\" #10 df_final_table_2015 . loc [ find_team ( \"Hertha BSC\" ), 'Abbvr' ] = \"BSC\" #11 df_final_table_2015 . loc [ find_team ( \"Werder Bremen\" ), 'Abbvr' ] = \"BRE\" #12 df_final_table_2015 . loc [ find_team ( \"Eintracht Frankfurt\" ), 'Abbvr' ] = \"FRA\" #13 df_final_table_2015 . loc [ find_team ( \"SC Freiburg\" ), 'Abbvr' ] = \"FRE\" #14 df_final_table_2015 . loc [ find_team ( \"VfB Stuttgart\" ), 'Abbvr' ] = \"STU\" #15 df_final_table_2015 . loc [ find_team ( \"Hamburger SV\" ), 'Abbvr' ] = \"HAM\" #16 df_final_table_2015 . loc [ find_team ( \"rnberg\" ), 'Abbvr' ] = \"NUR\" #17 df_final_table_2015 . loc [ find_team ( \"Eintracht Braunschweig\" ), 'Abbvr' ] = \"BRS\" #18 In [33]: plotting . table = df_final_table_2015 plotting . plot_goals ( seasons_simulated_1000 ) Following, obtained results, we see that we have properly predicted (falled in the 90% HPDI of the simulated scored goals) the final results for such teams as FCB,WOL,LEV,FRA,MGL etc but also we have encountered some overestimations on scored goals for DOR, HOF and HAM Definitely, the main failer of the current season were \"Borussia Dortmund\" (DOR) and \"SF Hamburg\" (HAM). $$\\\\[5pt]$$ Prediction of the matches scoring in 2013/2014 season It would be perhaps interesting to predict the scoring of the paticular matches. We will add this functionality to the our model class. Here is a code. In [34]: ''' Predict of the match result support in the Model Class ''' def _predict_match_map ( self , teams , team_abbvr_home , team_abbvr_away ): \"\"\" makes predictions on the match using MAP values :param: teams -- a table with the abbrevation and numerical code for each team :param: team_abbvr_home -- the abbrevation of the team playing at home :param: team_abbvr_away -- the abbrevation of the guest team \"\"\" #get indexes of the home/away teams i_home = teams . loc [ teams [ 'team' ] == team_abbvr_home , 'i' ] . values [ 0 ] i_away = teams . loc [ teams [ 'team' ] == team_abbvr_away , 'i' ] . values [ 0 ] # there are two ways of the calculation the score using MAP # method 1 # get atts properties of all teams atts = bundesliga_model . map [ 'atts' ] # get defs properties of all teams defs = bundesliga_model . map [ 'defs' ] # get home parameter home = bundesliga_model . map [ 'home' ] # get intercept parameter intercept = bundesliga_model . map [ 'intercept' ] #calculation of the scoring home_theta = np . exp ( intercept + home + atts [ i_home ] + defs [ i_away ] ) away_theta = np . exp ( intercept + atts [ i_away ] + defs [ i_home ] ) # method 2 # get home_theta and away_theta #i_theta = i_home*(len(teams)-1) + i_away -1 #home_theta = bundesliga_model.map['home_theta'][i_theta] #away_theta = bundesliga_model.map['away_theta'][i_theta] home_goals = np . random . poisson ( home_theta ) away_goals = np . random . poisson ( away_theta ) return ( home_goals , away_goals , \"win\" if home_goals > away_goals else ( \"loss\" if away_goals != home_goals else \"draw\" )) def _predict_match_mcmc ( self , teams , team_abbvr_home , team_abbvr_away ): \"\"\" makes predictions on the match using MCMC simulations :param: teams -- a table with the abbrevation and numerical code for each team :param: team_abbvr_home -- the abbrevation of the team playing at home :param: team_abbvr_away -- the abbrevation of the guest team \"\"\" #get indexes of the home/away teams i_home = teams . loc [ teams [ 'team' ] == team_abbvr_home , 'i' ] . values [ 0 ] i_away = teams . loc [ teams [ 'team' ] == team_abbvr_away , 'i' ] . values [ 0 ] # get the number of samples num_samples = self . traces [ 'home' ] . shape [ 0 ] # randomly draw a seasson number from traces draw = np . random . randint ( 0 , num_samples ) atts_draw = self . traces [ \"atts\" ][ draw , :] defs_draw = self . traces [ \"defs\" ][ draw , :] home_draw = self . traces [ \"home\" ][ draw ] intercept_draw = self . traces [ \"intercept\" ][ draw ] #calculation of the scoring home_theta = np . exp ( intercept_draw + home_draw + atts_draw [ i_home ] + defs_draw [ i_away ] ) away_theta = np . exp ( intercept_draw + atts_draw [ i_away ] + defs_draw [ i_home ] ) home_goals = np . random . poisson ( home_theta ) away_goals = np . random . poisson ( away_theta ) return ( home_goals , away_goals , \"win\" if home_goals > away_goals else ( \"loss\" if away_goals != home_goals else \"draw\" )) # Update the model definition Model . predict_match_map = _predict_match_map Model . predict_match_mcmc = _predict_match_mcmc print bundesliga_model . predict_match_map ( teams , \"AUG\" , \"LEV\" ) print bundesliga_model . predict_match_mcmc ( teams , \"AUG\" , \"LEV\" ) (2, 0, 'win') (1, 1, 'draw') Let's extend our plotter to make plots of our score-predictions. In [35]: ''' Plot predicted scores for the team with team_abbvr_home,team_abbvr_away ''' def _plot_predicted_score ( self , model , teams , team_abbvr_home , team_abbvr_away , n , real_score_data = None , type_of_predict = 'MAP' ): ''' plot the predicted scores ''' sns . set_context ( \"poster\" ) plt . figure ( figsize = ( 15 , 8 )) func = model . predict_match_map if type_of_predict == \"MCMC\" : func = model . predict_match_mcmc #f, axes = pl.subplots(figsize=(12, 8)) x = pd . Series ( np . array ([ func ( teams , team_abbvr_home , team_abbvr_away )[ 0 ] for i in range ( n ) ]), name = team_abbvr_home ) y = pd . Series ( np . array ([ func ( teams , team_abbvr_home , team_abbvr_away )[ 1 ] for i in range ( n ) ]), name = team_abbvr_away ) # Show the joint distribution using kernel density estimation g = sns . jointplot ( x , y , kind = \"kde\" , size = 10 , space = 0 ) if real_score_data is not None : ax = plt . gcf () . axes [ 0 ] real_x = real_score_data . loc [( real_score_data [ 'home' ] == team_abbvr_home ) & ( real_score_data [ 'away' ] == team_abbvr_away ), \"home_score\" ] real_y = real_score_data . loc [( real_score_data [ 'home' ] == team_abbvr_home ) & ( real_score_data [ 'away' ] == team_abbvr_away ), \"away_score\" ] ax . scatter ( real_x . values , real_y . values , s = np . array ([ np . pi * ( 15 ) ** 2 ]), label = 'real score' ) _ = ax . legend ( loc = 'upper center' , shadow = True ) return # Update our Plotter Plotter . plot_predicted_score = _plot_predicted_score plotting . plot_predicted_score ( bundesliga_model , teams , \"AUG\" , \"LEV\" , 1000 , df , 'MAP' ) plotting . plot_predicted_score ( bundesliga_model , teams , \"AUG\" , \"LEV\" , 1000 , df , 'MCMC' ) <matplotlib.figure.Figure at 0xcaaa0ec> <matplotlib.figure.Figure at 0xc72ccac> It looks like the score of AUG/LEV as 1:3 was clearly strange. We expected a result 1:1. In [36]: plotting . plot_predicted_score ( bundesliga_model , teams , \"DOR\" , \"HAM\" , 1000 , df , 'MAP' ) <matplotlib.figure.Figure at 0xcb4566c> And again,we expected results 2:1, 3:0,3:1,4:1 but not 6:2. In [37]: plotting . plot_predicted_score ( bundesliga_model , teams , \"DOR\" , \"FCB\" , 1000 , df , 'MAP' ) <matplotlib.figure.Figure at 0xc31a3cc> Here we add some statistical calculation on mean, median and HPDI of the predicted scoring to our model. In [39]: ''' statistics on predicted scores ''' def _pediction_scrore_statistics ( self , teams , team_abbvr_home , team_abbvr_away , n , CL = 0.05 ): \"\"\" make a table on statistics \"\"\" #get indexes of the home/away teams i_home = teams . loc [ teams [ 'team' ] == team_abbvr_home , 'i' ] . values [ 0 ] i_away = teams . loc [ teams [ 'team' ] == team_abbvr_away , 'i' ] . values [ 0 ] home_scores = [] away_scores = [] for i in range ( n ): home_score , away_score , _ = self . predict_match_map ( teams , team_abbvr_home , team_abbvr_away ) home_scores += [ home_score ] away_scores += [ away_score ] stats = pd . DataFrame ({ 'home_score' : home_scores , 'away_score' : away_scores , }) . describe ( percentiles = [ . 05 , . 1 , . 25 , . 32 , . 31 , . 5 , . 68 , . 69 , . 75 , . 90 , . 95 ]) min_val = str ( int (( 1. - CL ) * 100 )) + '%' max_val = str ( int (( CL ) * 100 )) + '%' stats = pd . DataFrame . from_records ( [ ( team_abbvr_home , stats [ \"home_score\" ][ \"mean\" ], stats [ \"home_score\" ][ min_val ], stats [ \"home_score\" ][ max_val ]), ( team_abbvr_away , stats [ \"away_score\" ][ \"mean\" ], stats [ \"away_score\" ][ min_val ], stats [ \"away_score\" ][ max_val ]) ] ) stats . columns = [ 'Team' , 'Mean' , min_val , max_val ] return stats # Update the model definition Model . pediction_scrore_statistics = _pediction_scrore_statistics bundesliga_model . pediction_scrore_statistics ( teams , \"AUG\" , \"LEV\" , 1000 , CL = 0.95 ) Out[39]: Team Mean 5% 95% 0 AUG 1.280 0 3.00 1 LEV 1.329 0 3.05 For example, from the table above, we could expect at least one goal should be scored. Also the case when more than 2 goals scored by one team, is unexpected. We can introduce an ad-hoc calculation of the particular result: In [40]: ''' probability on predicted scores ''' def _pediction_scrore_probas ( self , teams , team_abbvr_home , team_abbvr_away , n ): \"\"\" calculate probas a table on statistics \"\"\" #get indexes of the home/away teams i_home = teams . loc [ teams [ 'team' ] == team_abbvr_home , 'i' ] . values [ 0 ] i_away = teams . loc [ teams [ 'team' ] == team_abbvr_away , 'i' ] . values [ 0 ] scores = {} for i in range ( n ): home_score , away_score , _ = self . predict_match_map ( teams , team_abbvr_home , team_abbvr_away ) if ( home_score , away_score ) in scores . keys (): scores [( home_score , away_score )] += 1. else : scores [( home_score , away_score )] = 1. # change values to probabilites for key in scores . keys (): scores [ key ] *= 100. / float ( n ) import operator return scores , sorted ( scores . items (), key = operator . itemgetter ( 1 ), reverse = True ) # Update the model definition Model . pediction_scrore_probas = _pediction_scrore_probas # get first five results with the highest probabilities print bundesliga_model . pediction_scrore_probas ( teams , \"AUG\" , \"LEV\" , 1000 )[ 1 ][: 5 ] # what is the probability to get result 4:1 try : proba = bundesliga_model . pediction_scrore_probas ( teams , \"AUG\" , \"LEV\" , 1000 )[ 0 ][( 4 , 1 )] except : proba = 0. print proba [((1, 1), 14.0), ((0, 1), 9.3), ((1, 0), 8.5), ((2, 0), 7.800000000000001), ((2, 1), 7.7)] 1.1 So, following our model, we see that the score 1:1 has a frequency probability about 14% while, 4:1 has 1.1% correspondingly. $$\\\\[5pt]$$ Prediction of the matches scoring for 2015/2016 season As a final task, I would like to predict the table like this [ 2 ] for the 2015/2016 season of the BundesLiga. First, we scrape data of 2013/2014 and 2014/2015, then we combine them and train our model. Finally, we will predict results of the matches between teams in each pair. The fist step is scrapping. In [41]: # 2013/2014 # a page containing the data wikipage = 'https://en.wikipedia.org/wiki/2013 %E 2 %80% 9314_Bundesliga#Results' # a pattern on the header to find the table among others title_pattern = 'Home ╲ Away' # a pattern on the class properties of the table used to find it table_props = { \"class\" : \"wikitable\" } # parse the table from the wiki page wt = Wiki_Table ( wikipage , table_props , title_pattern ) # create a dataframe on the data table_csv = map ( lambda x : ',' . join ( x ), wt . table ) table_csv = ' \\n ' . join ( table_csv ) data = StringIO ( table_csv ) df_2013 = pd . read_csv ( data , delimiter = ',' , index_col = 0 ) df_2013 . index = df_2013 . columns rows = [] for i in df_2013 . index : for c in df_2013 . columns : if i == c : continue score = df_2013 . ix [ i , c ] score = [ int ( row ) for row in score . split ( '-' )] rows . append ([ i , c , score [ 0 ], score [ 1 ]]) df_2013 = pd . DataFrame ( rows , columns = [ 'home' , 'away' , 'home_score' , 'away_score' ]) In [42]: # 2014/2015 # a page containing the data wikipage = 'https://en.wikipedia.org/wiki/2014 %E 2 %80% 9315_Bundesliga#Results' # a pattern on the header to find the table among others title_pattern = 'Home ╲ Away' # a pattern on the class properties of the table used to find it table_props = { \"class\" : \"wikitable\" } # parse the table from the wiki page wt = Wiki_Table ( wikipage , table_props , title_pattern ) # create a dataframe on the data table_csv = map ( lambda x : ',' . join ( x ), wt . table ) table_csv = ' \\n ' . join ( table_csv ) data = StringIO ( table_csv ) df_2014 = pd . read_csv ( data , delimiter = ',' , index_col = 0 ) df_2014 . index = df_2014 . columns rows = [] for i in df_2014 . index : for c in df_2014 . columns : if i == c : continue score = df_2014 . ix [ i , c ] score = [ int ( row ) for row in score . split ( '-' )] rows . append ([ i , c , score [ 0 ], score [ 1 ]]) df_2014 = pd . DataFrame ( rows , columns = [ 'home' , 'away' , 'home_score' , 'away_score' ]) In [43]: print df_2013 . describe () print df_2014 . describe () home_score away_score count 306.000000 306.000000 mean 1.748366 1.411765 std 1.383301 1.257264 min 0.000000 0.000000 25% 1.000000 0.000000 50% 2.000000 1.000000 75% 3.000000 2.000000 max 6.000000 7.000000 home_score away_score count 306.000000 306.000000 mean 1.588235 1.166667 std 1.320851 1.190812 min 0.000000 0.000000 25% 1.000000 0.000000 50% 1.000000 1.000000 75% 2.000000 2.000000 max 8.000000 6.000000 In [44]: # combine the results of 2013/2014 and 2014/2015 seasons df_2013_2014 = pd . concat ([ df_2013 , df_2014 ], ignore_index = True ) df_2013_2014 . describe () Out[44]: home_score away_score count 612.000000 612.000000 mean 1.668301 1.289216 std 1.353703 1.229619 min 0.000000 0.000000 25% 1.000000 0.000000 50% 2.000000 1.000000 75% 2.250000 2.000000 max 8.000000 7.000000 Extract the list of teams participating in both seasons: In [45]: teams_2013_2014 = df_2013_2014 . home . unique () teams_2013_2014 = pd . DataFrame ( teams_2013_2014 , columns = [ 'team' ]) teams_2013_2014 [ 'i' ] = teams_2013_2014 . index print \"Number of teams: {0}\" . format ( len ( teams_2013_2014 )) teams_2013_2014 . head () Number of teams: 20 Out[45]: team i 0 AUG 0 1 LEV 1 2 FCB 2 3 DOR 3 4 MGL 4 Merge the teams info in our dataframe: In [46]: df_2013_2014 = pd . merge ( df_2013_2014 , teams_2013_2014 , left_on = 'home' , right_on = 'team' , how = 'left' ) df_2013_2014 = df_2013_2014 . rename ( columns = { 'i' : 'i_home' }) . drop ( 'team' , 1 ) df_2013_2014 = pd . merge ( df_2013_2014 , teams_2013_2014 , left_on = 'away' , right_on = 'team' , how = 'left' ) df_2013_2014 = df_2013_2014 . rename ( columns = { 'i' : 'i_away' }) . drop ( 'team' , 1 ) print \"How many games in both seasons are? : %d \" % len (df_2013_2014) df_2013_2014 . head () How many games in both seasons are? : 612 Out[46]: home away home_score away_score i_home i_away 0 AUG LEV 1 3 0 1 1 AUG FCB 1 0 0 2 2 AUG DOR 0 4 0 3 3 AUG MGL 2 2 0 4 4 AUG BRS 4 1 0 5 In [47]: observed_home_goals_2013_2014 = df_2013_2014 . home_score . values observed_away_goals_2013_2014 = df_2013_2014 . away_score . values home_team_2013_2014 = df_2013_2014 . i_home . values away_team_2013_2014 = df_2013_2014 . i_away . values num_teams_2013_2014 = len ( df_2013_2014 . i_home . unique ()) num_games_2013_2014 = len ( home_team_2013_2014 ) print \"Number of games: {0}\" . format ( num_games_2013_2014 ) # prepare the starting point for our model g = df_2013_2014 . groupby ([ 'i_away' ]) att_starting_points = np . log ( g . away_score . mean ()) g = df_2013_2014 . groupby ( 'i_home' ) def_starting_points = - np . log ( g . away_score . mean ()) # negative because this property plays Number of games: 612 Let's build our model: In [48]: # create the model object used in the further analysis observed_data_model_2013_2014 = { 'observed_home_goals' : observed_home_goals_2013_2014 , 'observed_away_goals' : observed_away_goals_2013_2014 , 'num_teams' : num_teams_2013_2014 , 'home_team' : home_team_2013_2014 , 'away_team' : away_team_2013_2014 , } bundesliga_model_2013_2014 = Model ( observed_data_model_2013_2014 ) ? bundesliga_model_2013_2014 INFO:root:building the model.... INFO:root:done building the model Calculate MAP: In [659]: # remove data/map folder first #!rm -r \"data/map\" In [50]: logging . info ( 'MAP of %s is %2.4f ' % ( 'tau_att' , bundesliga_model_2013_2014 . map [ 'tau_att' ])) print 'saving...' #save_np_vars(bundesliga_model_2013_2014.map,bundesliga_model_2013_2014.map_dir) INFO:root:MAP of tau_att is 15.2102 saving... Draw MCMC: In [51]: # let's make samples bundesliga_model_2013_2014 . draw_samples ( 30000 ) INFO:root:drawing 30000 samples INFO:root:backing up trace to directory: data/mcmc [-----------------100%-----------------] 30000 of 30000 complete in 44.8 sec Make simulation of 100 seasons: In [52]: seasons_simulated_2013_2014 = bundesliga_model_2013_2014 . simulate_seasons ( 100 , df_2013_2014 ) Plot final goals of the teams: In [53]: plotting_2015 = Plotter ( bundesliga_model_2013_2014 , df_final_table ) # df_final_table is not correct plotting_2015 . plot_goals ( seasons_simulated_2013_2014 ) # because the df_final_table is not correct # \"Actual Goals\" are not correct Finally, we can predict the table as in at wikipage of scores for teams played in the season 2013/2014. The scores with the highest probabilities are chosen. In [54]: rows_2015 = [] for x in df_2013 . values : score = bundesliga_model_2013_2014 . pediction_scrore_probas ( teams_2013_2014 , x [ 0 ], x [ 1 ], 500 )[ 1 ][ 1 ] rows_2015 . append (( x [ 0 ], x [ 1 ], score [ 0 ][ 0 ], score [ 0 ][ 1 ])) In [55]: df_2015 = pd . DataFrame ( rows_2015 , columns = df_2013 . columns . values ) df_2015 . head ( len ( df_2015 )) Out[55]: home away home_score away_score 0 AUG LEV 0 1 1 AUG FCB 1 2 2 AUG DOR 1 2 3 AUG MGL 2 1 4 AUG BRS 2 0 5 AUG FRA 1 1 6 AUG FRE 1 0 7 AUG HAM 2 1 8 AUG H96 2 1 9 AUG BSC 1 1 10 AUG HOF 1 1 11 AUG MAI 0 1 12 AUG NUR 1 0 13 AUG S04 0 1 14 AUG STU 1 0 15 AUG BRE 1 1 16 AUG WOL 1 0 17 LEV AUG 2 0 18 LEV FCB 1 1 19 LEV DOR 1 2 20 LEV MGL 2 1 21 LEV BRS 1 0 22 LEV FRA 1 1 23 LEV FRE 1 1 24 LEV HAM 3 0 25 LEV H96 1 0 26 LEV BSC 1 1 27 LEV HOF 2 1 28 LEV MAI 2 1 29 LEV NUR 1 0 ... ... ... ... ... 276 BRE MGL 1 1 277 BRE BRS 1 1 278 BRE FRA 1 0 279 BRE FRE 2 1 280 BRE HAM 1 0 281 BRE H96 1 0 282 BRE BSC 1 0 283 BRE HOF 1 1 284 BRE MAI 2 1 285 BRE NUR 1 0 286 BRE S04 1 2 287 BRE STU 2 1 288 BRE WOL 1 0 289 WOL AUG 1 1 290 WOL LEV 2 1 291 WOL FCB 1 1 292 WOL DOR 2 1 293 WOL MGL 2 1 294 WOL BRS 2 1 295 WOL FRA 1 1 296 WOL FRE 2 1 297 WOL HAM 4 1 298 WOL H96 1 1 299 WOL BSC 2 1 300 WOL HOF 1 1 301 WOL MAI 1 1 302 WOL NUR 1 0 303 WOL S04 2 1 304 WOL STU 1 0 305 WOL BRE 3 1 306 rows × 4 columns $$\\\\[5pt]$$ References [1]http://danielweitzenfeld.github.io/passtheroc/blog/2014/10/28/bayes-premier-league/ [2] 2013/14 BundesLiga results [3] a Wiki Scrapper [4] 2013/14 BundesLiga table [5] 2014/15 BundesLiga table if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML'; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: 'center',\" + \" displayIndent: '0em',\" + \" showMathMenu: true,\" + \" tex2jax: { \" + \" inlineMath: [ ['$','$'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }\" + \" } \" + \"}); \"; (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Statistics","title":"A Hierarchical Bayesian Model of the Bundes League"},{"url":"http://igormarfin.github.io/blog/2016/02/07/shifted-beta-geometric-customer-lifetime-value-model/","text":"/*! * * IPython notebook * */.ansibold{font-weight:bold}.ansiblack{color:black}.ansired{color:darkred}.ansigreen{color:darkgreen}.ansiyellow{color:#c4a000}.ansiblue{color:darkblue}.ansipurple{color:darkviolet}.ansicyan{color:steelblue}.ansigray{color:gray}.ansibgblack{background-color:black}.ansibgred{background-color:red}.ansibggreen{background-color:green}.ansibgyellow{background-color:yellow}.ansibgblue{background-color:blue}.ansibgpurple{background-color:magenta}.ansibgcyan{background-color:cyan}.ansibggray{background-color:gray}div.cell{border:1px solid transparent;display:-webkit-box;-webkit-box-orient:vertical;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:vertical;-moz-box-align:stretch;display:box;box-orient:vertical;box-align:stretch;display:flex;flex-direction:column;align-items:stretch;border-radius:2px;box-sizing:border-box;-moz-box-sizing:border-box;-webkit-box-sizing:border-box;border-width:thin;border-style:solid;width:100%;padding:5px;margin:0;outline:none}div.cell.selected{border-color:#ababab}@media print{div.cell.selected{border-color:transparent}}.edit_mode div.cell.selected{border-color:green}@media print{.edit_mode div.cell.selected{border-color:transparent}}.prompt{min-width:14ex;padding:.4em;margin:0;font-family:monospace;text-align:right;line-height:1.21429em}@media (max-width:540px){.prompt{text-align:left}}div.inner_cell{display:-webkit-box;-webkit-box-orient:vertical;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:vertical;-moz-box-align:stretch;display:box;box-orient:vertical;box-align:stretch;display:flex;flex-direction:column;align-items:stretch;-webkit-box-flex:1;-moz-box-flex:1;box-flex:1;flex:1}@-moz-document url-prefix(){div.inner_cell{overflow-x:hidden}}div.input_area{border:1px solid #cfcfcf;border-radius:2px;background:#f7f7f7;line-height:1.21429em}div.prompt:empty{padding-top:0;padding-bottom:0}div.unrecognized_cell{padding:5px 5px 5px 0;display:-webkit-box;-webkit-box-orient:horizontal;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:horizontal;-moz-box-align:stretch;display:box;box-orient:horizontal;box-align:stretch;display:flex;flex-direction:row;align-items:stretch}div.unrecognized_cell .inner_cell{border-radius:2px;padding:5px;font-weight:bold;color:red;border:1px solid #cfcfcf;background:#eaeaea}div.unrecognized_cell .inner_cell a{color:inherit;text-decoration:none}div.unrecognized_cell .inner_cell a:hover{color:inherit;text-decoration:none}@media (max-width:540px){div.unrecognized_cell>div.prompt{display:none}}@media print{div.code_cell{page-break-inside:avoid}}div.input{page-break-inside:avoid;display:-webkit-box;-webkit-box-orient:horizontal;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:horizontal;-moz-box-align:stretch;display:box;box-orient:horizontal;box-align:stretch;display:flex;flex-direction:row;align-items:stretch}@media (max-width:540px){div.input{display:-webkit-box;-webkit-box-orient:vertical;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:vertical;-moz-box-align:stretch;display:box;box-orient:vertical;box-align:stretch;display:flex;flex-direction:column;align-items:stretch}}div.input_prompt{color:navy;border-top:1px solid transparent}div.input_area>div.highlight{margin:.4em;border:none;padding:0;background-color:transparent}div.input_area>div.highlight>pre{margin:0;border:none;padding:0;background-color:transparent}.CodeMirror{line-height:1.21429em;font-size:14px;height:auto;background:none}.CodeMirror-scroll{overflow-y:hidden;overflow-x:auto}.CodeMirror-lines{padding:.4em}.CodeMirror-linenumber{padding:0 8px 0 4px}.CodeMirror-gutters{border-bottom-left-radius:2px;border-top-left-radius:2px}.CodeMirror pre{padding:0;border:0;border-radius:0}.highlight-base{}.highlight-variable{}.highlight-variable-2{color:#1a1a1a}.highlight-variable-3{color:#333}.highlight-string{color:#ba2121}.highlight-comment{color:#408080;font-style:italic}.highlight-number{80}.highlight-atom{color:#88f}.highlight-keyword{8000;font-weight:bold}.highlight-builtin{8000}.highlight-error{color:#f00}.highlight-operator{color:#a2f;font-weight:bold}.highlight-meta{color:#a2f}.highlight-def{f}.highlight-string-2{color:#f50}.highlight-qualifier{color:#555}.highlight-bracket{color:#997}.highlight-tag{color:#170}.highlight-attribute{c}.highlight-header{color:blue}.highlight-quote{90}.highlight-link{c}.cm-s-ipython span.cm-keyword{8000;font-weight:bold}.cm-s-ipython span.cm-atom{color:#88f}.cm-s-ipython span.cm-number{80}.cm-s-ipython span.cm-def{f}.cm-s-ipython span.cm-variable{}.cm-s-ipython span.cm-operator{color:#a2f;font-weight:bold}.cm-s-ipython span.cm-variable-2{color:#1a1a1a}.cm-s-ipython span.cm-variable-3{color:#333}.cm-s-ipython span.cm-comment{color:#408080;font-style:italic}.cm-s-ipython span.cm-string{color:#ba2121}.cm-s-ipython span.cm-string-2{color:#f50}.cm-s-ipython span.cm-meta{color:#a2f}.cm-s-ipython span.cm-qualifier{color:#555}.cm-s-ipython span.cm-builtin{8000}.cm-s-ipython span.cm-bracket{color:#997}.cm-s-ipython span.cm-tag{color:#170}.cm-s-ipython span.cm-attribute{c}.cm-s-ipython span.cm-header{color:blue}.cm-s-ipython span.cm-quote{90}.cm-s-ipython span.cm-link{c}.cm-s-ipython span.cm-error{color:#f00}.cm-s-ipython span.cm-tab{background:url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADAAAAAMCAYAAAAkuj5RAAAAAXNSR0IArs4c6QAAAGFJREFUSMft1LsRQFAQheHPowAKoACx3IgEKtaEHujDjORSgWTH/ZOdnZOcM/sgk/kFFWY0qV8foQwS4MKBCS3qR6ixBJvElOobYAtivseIE120FaowJPN75GMu8j/LfMwNjh4HUpwg4LUAAAAASUVORK5CYII=);background-position:right;background-repeat:no-repeat}div.output_wrapper{position:relative;display:-webkit-box;-webkit-box-orient:vertical;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:vertical;-moz-box-align:stretch;display:box;box-orient:vertical;box-align:stretch;display:flex;flex-direction:column;align-items:stretch}div.output_scroll{height:24em;width:100%;overflow:auto;border-radius:2px;-webkit-box-shadow:inset 0 2px 8px rgba(0,0,0,0.8);box-shadow:inset 0 2px 8px rgba(0,0,0,0.8);display:block}div.output_collapsed{margin:0;padding:0;display:-webkit-box;-webkit-box-orient:vertical;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:vertical;-moz-box-align:stretch;display:box;box-orient:vertical;box-align:stretch;display:flex;flex-direction:column;align-items:stretch}div.out_prompt_overlay{height:100%;padding:0 .4em;position:absolute;border-radius:2px}div.out_prompt_overlay:hover{-webkit-box-shadow:inset 0 0 1px #000;box-shadow:inset 0 0 1px #000;background:rgba(240,240,240,0.5)}div.output_prompt{color:darkred}div.output_area{padding:0;page-break-inside:avoid;display:-webkit-box;-webkit-box-orient:horizontal;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:horizontal;-moz-box-align:stretch;display:box;box-orient:horizontal;box-align:stretch;display:flex;flex-direction:row;align-items:stretch}div.output_area .MathJax_Display{text-align:left !important}div.output_area div.output_area .output{display:-webkit-box;-webkit-box-orient:vertical;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:vertical;-moz-box-align:stretch;display:box;box-orient:vertical;box-align:stretch;display:flex;flex-direction:column;align-items:stretch}@media (max-width:540px){div.output_area{display:-webkit-box;-webkit-box-orient:vertical;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:vertical;-moz-box-align:stretch;display:box;box-orient:vertical;box-align:stretch;display:flex;flex-direction:column;align-items:stretch}}div.output_area pre{margin:0;padding:0;border:0;vertical-align:baseline;color:black;background-color:transparent;border-radius:0}div.output_subarea{padding:.4em;-webkit-box-flex:1;-moz-box-flex:1;box-flex:1;flex:1}div.output_text{text-align:left;line-height:1.21429em}div.output_stderr{background:#fdd}div.output_latex{text-align:left}div.output_javascript:empty{padding:0}.js-error{color:darkred}div.raw_input_container{font-family:monospace;padding-top:5px}input.raw_input{font-family:inherit;font-size:inherit;color:inherit;width:auto;vertical-align:baseline;padding:0 .25em;margin:0 .25em}input.raw_input:focus{box-shadow:none}p.p-space{margin-bottom:10px}div.output_unrecognized{padding:5px;font-weight:bold;color:red}div.output_unrecognized a{color:inherit;text-decoration:none}div.output_unrecognized a:hover{color:inherit;text-decoration:none}.rendered_html{}.rendered_html :link{text-decoration:underline}.rendered_html :visited{text-decoration:underline}.rendered_html h1:first-child{margin-top:.538em}.rendered_html h2:first-child{margin-top:.636em}.rendered_html h3:first-child{margin-top:.777em}.rendered_html h4:first-child{margin-top:1em}.rendered_html h5:first-child{margin-top:1em}.rendered_html h6:first-child{margin-top:1em}.rendered_html *+ul{margin-top:1em}.rendered_html *+ol{margin-top:1em}.rendered_html *+table{margin-top:1em}.rendered_html *+p{margin-top:1em}.rendered_html *+img{margin-top:1em}div.text_cell{display:-webkit-box;-webkit-box-orient:horizontal;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:horizontal;-moz-box-align:stretch;display:box;box-orient:horizontal;box-align:stretch;display:flex;flex-direction:row;align-items:stretch}@media (max-width:540px){div.text_cell>div.prompt{display:none}}div.text_cell_render{outline:none;resize:none;width:inherit;border-style:none;padding:.5em .5em .5em .4em;box-sizing:border-box;-moz-box-sizing:border-box;-webkit-box-sizing:border-box}a.anchor-link:link{text-decoration:none;padding:0 20px;visibility:hidden}h1:hover .anchor-link,h2:hover .anchor-link,h3:hover .anchor-link,h4:hover .anchor-link,h5:hover .anchor-link,h6:hover .anchor-link{visibility:visible}.text_cell.rendered .input_area{display:none}.text_cell.unrendered .text_cell_render{display:none}.cm-header-1,.cm-header-2,.cm-header-3,.cm-header-4,.cm-header-5,.cm-header-6{font-weight:bold;font-family:\"Helvetica Neue\",Helvetica,Arial,sans-serif}.cm-header-1{font-size:185.7%}.cm-header-2{font-size:157.1%}.cm-header-3{font-size:128.6%}.cm-header-4{font-size:110%}.cm-header-5{font-size:100%;font-style:italic}.cm-header-6{font-size:100%;font-style:italic}.widget-interact>div,.widget-interact>input{padding:2.5px}.widget-area{page-break-inside:avoid;display:-webkit-box;-webkit-box-orient:horizontal;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:horizontal;-moz-box-align:stretch;display:box;box-orient:horizontal;box-align:stretch;display:flex;flex-direction:row;align-items:stretch}.widget-area .widget-subarea{padding:.44em .4em .4em 1px;margin-left:6px;box-sizing:border-box;-moz-box-sizing:border-box;-webkit-box-sizing:border-box;display:-webkit-box;-webkit-box-orient:vertical;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:vertical;-moz-box-align:stretch;display:box;box-orient:vertical;box-align:stretch;display:flex;flex-direction:column;align-items:stretch;-webkit-box-flex:2;-moz-box-flex:2;box-flex:2;flex:2;-webkit-box-align:start;-moz-box-align:start;box-align:start;align-items:flex-start}.widget-area.connection-problems .prompt:after{content:\"\\f127\";font-family:'FontAwesome';color:#d9534f;font-size:14px;top:3px;padding:3px}.slide-track{border:1px solid #ccc;background:#fff;border-radius:2px}.widget-hslider{padding-left:8px;padding-right:2px;overflow:visible;width:350px;height:5px;max-height:5px;margin-top:13px;margin-bottom:10px;border:1px solid #ccc;background:#fff;border-radius:2px;display:-webkit-box;-webkit-box-orient:horizontal;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:horizontal;-moz-box-align:stretch;display:box;box-orient:horizontal;box-align:stretch;display:flex;flex-direction:row;align-items:stretch}.widget-hslider .ui-slider{border:0;background:none;display:-webkit-box;-webkit-box-orient:horizontal;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:horizontal;-moz-box-align:stretch;display:box;box-orient:horizontal;box-align:stretch;display:flex;flex-direction:row;align-items:stretch;-webkit-box-flex:1;-moz-box-flex:1;box-flex:1;flex:1}.widget-hslider .ui-slider .ui-slider-handle{width:12px;height:28px;margin-top:-8px;border-radius:2px}.widget-hslider .ui-slider .ui-slider-range{height:12px;margin-top:-4px;background:#eee}.widget-vslider{padding-bottom:5px;overflow:visible;width:5px;max-width:5px;height:250px;margin-left:12px;border:1px solid #ccc;background:#fff;border-radius:2px;display:-webkit-box;-webkit-box-orient:vertical;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:vertical;-moz-box-align:stretch;display:box;box-orient:vertical;box-align:stretch;display:flex;flex-direction:column;align-items:stretch}.widget-vslider .ui-slider{border:0;background:none;margin-left:-4px;margin-top:5px;display:-webkit-box;-webkit-box-orient:vertical;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:vertical;-moz-box-align:stretch;display:box;box-orient:vertical;box-align:stretch;display:flex;flex-direction:column;align-items:stretch;-webkit-box-flex:1;-moz-box-flex:1;box-flex:1;flex:1}.widget-vslider .ui-slider .ui-slider-handle{width:28px;height:12px;margin-left:-9px;border-radius:2px}.widget-vslider .ui-slider .ui-slider-range{width:12px;margin-left:-1px;background:#eee}.widget-text{width:350px;margin:0}.widget-listbox{width:350px;margin-bottom:0}.widget-numeric-text{width:150px;margin:0}.widget-progress{margin-top:6px;min-width:350px}.widget-progress .progress-bar{-webkit-transition:none;-moz-transition:none;-ms-transition:none;-o-transition:none;transition:none}.widget-combo-btn{min-width:125px}.widget_item .dropdown-menu li a{color:inherit}.widget-hbox{display:-webkit-box;-webkit-box-orient:horizontal;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:horizontal;-moz-box-align:stretch;display:box;box-orient:horizontal;box-align:stretch;display:flex;flex-direction:row;align-items:stretch}.widget-hbox input[type=\"checkbox\"]{margin-top:9px;margin-bottom:10px}.widget-hbox .widget-label{min-width:10ex;padding-right:8px;padding-top:5px;text-align:right;vertical-align:text-top}.widget-hbox .widget-readout{padding-left:8px;padding-top:5px;text-align:left;vertical-align:text-top}.widget-vbox{display:-webkit-box;-webkit-box-orient:vertical;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:vertical;-moz-box-align:stretch;display:box;box-orient:vertical;box-align:stretch;display:flex;flex-direction:column;align-items:stretch}.widget-vbox .widget-label{padding-bottom:5px;text-align:center;vertical-align:text-bottom}.widget-vbox .widget-readout{padding-top:5px;text-align:center;vertical-align:text-top}.widget-box{box-sizing:border-box;-moz-box-sizing:border-box;-webkit-box-sizing:border-box;-webkit-box-align:start;-moz-box-align:start;box-align:start;align-items:flex-start}.widget-radio-box{display:-webkit-box;-webkit-box-orient:vertical;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:vertical;-moz-box-align:stretch;display:box;box-orient:vertical;box-align:stretch;display:flex;flex-direction:column;align-items:stretch;box-sizing:border-box;-moz-box-sizing:border-box;-webkit-box-sizing:border-box;padding-top:4px}.widget-radio-box label{margin-top:0}.widget-radio{margin-left:20px} .highlight .hll { background-color: #ffffcc } .highlight { background: #f8f8f8; } .highlight .c { color: #408080; font-style: italic } /* Comment */ .highlight .err { border: 1px solid #FF0000 } /* Error */ .highlight .k { color: #008000; font-weight: bold } /* Keyword */ .highlight .o { color: #666666 } /* Operator */ .highlight .cm { color: #408080; font-style: italic } /* Comment.Multiline */ .highlight .cp { color: #BC7A00 } /* Comment.Preproc */ .highlight .c1 { color: #408080; font-style: italic } /* Comment.Single */ .highlight .cs { color: #408080; font-style: italic } /* Comment.Special */ .highlight .gd { color: #A00000 } /* Generic.Deleted */ .highlight .ge { font-style: italic } /* Generic.Emph */ .highlight .gr { color: #FF0000 } /* Generic.Error */ .highlight .gh { color: #000080; font-weight: bold } /* Generic.Heading */ .highlight .gi { color: #00A000 } /* Generic.Inserted */ .highlight .go { color: #888888 } /* Generic.Output */ .highlight .gp { color: #000080; font-weight: bold } /* Generic.Prompt */ .highlight .gs { font-weight: bold } /* Generic.Strong */ .highlight .gu { color: #800080; font-weight: bold } /* Generic.Subheading */ .highlight .gt { color: #0044DD } /* Generic.Traceback */ .highlight .kc { color: #008000; font-weight: bold } /* Keyword.Constant */ .highlight .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */ .highlight .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */ .highlight .kp { color: #008000 } /* Keyword.Pseudo */ .highlight .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */ .highlight .kt { color: #B00040 } /* Keyword.Type */ .highlight .m { color: #666666 } /* Literal.Number */ .highlight .s { color: #BA2121 } /* Literal.String */ .highlight .na { color: #7D9029 } /* Name.Attribute */ .highlight .nb { color: #008000 } /* Name.Builtin */ .highlight .nc { color: #0000FF; font-weight: bold } /* Name.Class */ .highlight .no { color: #880000 } /* Name.Constant */ .highlight .nd { color: #AA22FF } /* Name.Decorator */ .highlight .ni { color: #999999; font-weight: bold } /* Name.Entity */ .highlight .ne { color: #D2413A; font-weight: bold } /* Name.Exception */ .highlight .nf { color: #0000FF } /* Name.Function */ .highlight .nl { color: #A0A000 } /* Name.Label */ .highlight .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */ .highlight .nt { color: #008000; font-weight: bold } /* Name.Tag */ .highlight .nv { color: #19177C } /* Name.Variable */ .highlight .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */ .highlight .w { color: #bbbbbb } /* Text.Whitespace */ .highlight .mb { color: #666666 } /* Literal.Number.Bin */ .highlight .mf { color: #666666 } /* Literal.Number.Float */ .highlight .mh { color: #666666 } /* Literal.Number.Hex */ .highlight .mi { color: #666666 } /* Literal.Number.Integer */ .highlight .mo { color: #666666 } /* Literal.Number.Oct */ .highlight .sb { color: #BA2121 } /* Literal.String.Backtick */ .highlight .sc { color: #BA2121 } /* Literal.String.Char */ .highlight .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */ .highlight .s2 { color: #BA2121 } /* Literal.String.Double */ .highlight .se { color: #BB6622; font-weight: bold } /* Literal.String.Escape */ .highlight .sh { color: #BA2121 } /* Literal.String.Heredoc */ .highlight .si { color: #BB6688; font-weight: bold } /* Literal.String.Interpol */ .highlight .sx { color: #008000 } /* Literal.String.Other */ .highlight .sr { color: #BB6688 } /* Literal.String.Regex */ .highlight .s1 { color: #BA2121 } /* Literal.String.Single */ .highlight .ss { color: #19177C } /* Literal.String.Symbol */ .highlight .bp { color: #008000 } /* Name.Builtin.Pseudo */ .highlight .vc { color: #19177C } /* Name.Variable.Class */ .highlight .vg { color: #19177C } /* Name.Variable.Global */ .highlight .vi { color: #19177C } /* Name.Variable.Instance */ .highlight .il { color: #666666 } /* Literal.Number.Integer.Long */ Shifted-Beta-Geometric (sBG) Customer Lifetime Value Model $$\\\\[2pt]$$ Igor Marfin [Unister Gmb@2014] < igor.marfin@unister.de > $$\\\\[40pt]$$ Abstract Idea of the analysis was taken from the blog [1] : The academic literature on the Lifetime Value (LTV) Model breaks down the problem into four scenarios, depending on whether the customer-business relationship is contractual, and on whether customers have discrete opportunities for transactions. we are going to implement a model from the contractual-discrete (lower-right) quadrant: the shifted-beta-geometric (sBG) model. $$\\\\[5pt]$$ Contractual-Discrete: The sBG Model The shifted-beta-geometric (sBG) model makes two assumptions: Each customer has a constant churn probability $\\theta$. You can think of this as the customer flipping a weighted coin (with probability of tails = $\\theta$) at the end of every subscription period, and they cancel their membership if the coin lands tails. Customer churn probabilities are drawn from a beta distribution with parameters $\\alpha$ and $\\beta$. We are going to make sBG Model using PyMC. More details can be found at https://bitbucket.org/iggy_floyd/bayesian-pymc-sbg-livetime-customer-model . $$\\\\[5pt]$$ Outline Abstract Contractual-Discrete: The sBG Model Initialization of the notebook Getting and cleaning the data The Model Diagnostics Discounted Expected Residual Lifetime (DERL) References $$\\\\[5pt]$$ Initialization To set up the python environment for the data analysis and make a nicer style of the notebook, one can run the following commands in the beginning of our modeling: In [1]: import sys sys . path = [ '/usr/local/lib/python2.7/dist-packages' ] + sys . path # to fix the problem with numpy: this replaces 1.6 version by 1.9 % matplotlib inline % pylab inline ion () import os import matplotlib import numpy as np import matplotlib.pyplot as pl import matplotlib as mpl import logging import pymc as pm # a plotter and dataframe modules import seaborn as sns # seaborn to make a nice plots of the data import pandas as pd import scipy.stats as stats # Set up logging. logger = logging . getLogger () logger . setLevel ( logging . INFO ) from book_format import load_style , figsize , set_figsize load_style () Populating the interactive namespace from numpy and matplotlib Out[1]: @import url('http://fonts.googleapis.com/css?family=Source+Code+Pro'); @import url('http://fonts.googleapis.com/css?family=Vollkorn'); @import url('http://fonts.googleapis.com/css?family=Arimo'); div.cell{ width: 1200px; margin-left: 0% !important; margin-right: auto; } div.text_cell code { background: transparent; color: #000000; font-weight: 600; font-size: 11pt; font-style: bold; font-family: 'Source Code Pro', Consolas, monocco, monospace; } h1 { font-family: 'Open sans',verdana,arial,sans-serif; } div.input_area { background: #F6F6F9; border: 1px solid #586e75; } .text_cell_render h1 { font-weight: 200; font-size: 30pt; line-height: 100%; color:#c76c0c; margin-bottom: 0.5em; margin-top: 1em; display: block; white-space: wrap; } h2 { font-family: 'Open sans',verdana,arial,sans-serif; } .text_cell_render h2 { font-weight: 200; font-size: 16pt; font-style: italic; line-height: 100%; color:#c76c0c; margin-bottom: 0.5em; margin-top: 1.5em; display: inline; white-space: wrap; } h3 { font-family: 'Open sans',verdana,arial,sans-serif; } .text_cell_render h3 { font-weight: 200; font-size: 14pt; line-height: 100%; color:#d77c0c; margin-bottom: 0.5em; margin-top: 2em; display: block; white-space: nowrap; } h4 { font-family: 'Open sans',verdana,arial,sans-serif; } .text_cell_render h4 { font-weight: 100; font-size: 14pt; color:#d77c0c; margin-bottom: 0.5em; margin-top: 0.5em; display: block; white-space: nowrap; } h5 { font-family: 'Open sans',verdana,arial,sans-serif; } .text_cell_render h5 { font-weight: 200; font-style: normal; color: #1d3b84; font-size: 16pt; margin-bottom: 0em; margin-top: 0.5em; display: block; white-space: nowrap; } div.text_cell_render{ font-family: 'Arimo',verdana,arial,sans-serif; line-height: 125%; font-size: 120%; text-align:justify; text-justify:inter-word; } div.output_subarea.output_text.output_pyout { overflow-x: auto; overflow-y: scroll; max-height: 50000px; } div.output_subarea.output_stream.output_stdout.output_text { overflow-x: auto; overflow-y: scroll; max-height: 50000px; } div.output_wrapper{ margin-top:0.2em; margin-bottom:0.2em; } code{ font-size: 70%; } .rendered_html code{ background-color: transparent; } ul{ margin: 2em; } ul li{ padding-left: 0.5em; margin-bottom: 0.5em; margin-top: 0.5em; } ul li li{ padding-left: 0.2em; margin-bottom: 0.2em; margin-top: 0.2em; } ol{ margin: 2em; } ol li{ padding-left: 0.5em; margin-bottom: 0.5em; margin-top: 0.5em; } ul li{ padding-left: 0.5em; margin-bottom: 0.5em; margin-top: 0.2em; } a:link{ font-weight: bold; color:#447adb; } a:visited{ font-weight: bold; color: #1d3b84; } a:hover{ font-weight: bold; color: #1d3b84; } a:focus{ font-weight: bold; color:#447adb; } a:active{ font-weight: bold; color:#447adb; } .rendered_html :link { text-decoration: underline; } .rendered_html :hover { text-decoration: none; } .rendered_html :visited { text-decoration: none; } .rendered_html :focus { text-decoration: none; } .rendered_html :active { text-decoration: none; } .warning{ color: rgb( 240, 20, 20 ) } hr { color: #f3f3f3; background-color: #f3f3f3; height: 1px; } blockquote{ display:block; background: #fcfcfc; border-left: 5px solid #c76c0c; font-family: 'Open sans',verdana,arial,sans-serif; width:1000px; padding: 10px 10px 10px 10px; text-align:justify; text-justify:inter-word; } blockquote p { margin-bottom: 0; line-height: 125%; font-size: 100%; } MathJax.Hub.Config({ TeX: { extensions: [\"AMSmath.js\"] }, tex2jax: { inlineMath: [ ['$','$'], [\"\\\\(\",\"\\\\)\"] ], displayMath: [ ['$$','$$'], [\"\\\\[\",\"\\\\]\"] ] }, displayAlign: 'center', // Change this to 'center' to center equations. \"HTML-CSS\": { scale:100, availableFonts: [\"Neo-Euler\"], preferredFont: \"Neo-Euler\", webFont: \"Neo-Euler\", styles: {'.MathJax_Display': {\"margin\": 4}} } }); $$\\\\[5pt]$$ Getting and cleaning the data We imagine that there is a company \"Unister Gmbh\" which provide one services/subscription: it books flight tickets and a membership for the customers in the company costs 10€ per month. Also the company provides 10% discount for each customers after 2nd month of their membership. The bussiness results have shown not nice numbers, and after some period of the working, the management of the company has decided to estimate and predict the \"churn\" (a decision on cancelation of the memberships) behavior of their customers. Another crusial task of the managers was a try to estimate the amount of money which can be brought by customers. Let's assume the management of the company has aggregated data on the number of customers signed to the service for the last 8 months. Then they have randomly chosen 1000 customers and decided to track their behaviour for this 8-month period: if the customer was signed to the service or had canceled the contract in the end of each month. In [2]: # numbers from CRM department of the Unister GmbH ... :-) num_customers = np . array ([ 1000 , 869 , 743 , 653 , 593 , 551 , 517 , 491 ]) num_periods = len ( num_customers ) Then we find out how many customers left, and aggregate all data in one dataframe: In [3]: previous_data = np . roll ( num_customers , - 1 ) lost_customers = num_customers - previous_data lost_customers = np . r_ [[ None ], lost_customers [: - 1 ]] customer_data = pd . DataFrame ({ \"active\" : num_customers , # customers which are still active \"lost\" : lost_customers , # customers left after each control sub-period (month) }) $$\\\\[5pt]$$ The Model Our model is defined as follows: In [4]: #defines the class for the Modeling class Model ( object ): \"\"\" A Model class :param np.ndarray data: The data to use for learning the model. \"\"\" def __init__ ( self , data ): logging . info ( 'building the model....' ) # how many periods of the measurement we have self . num_periods = len ( data ) # parameters of our model self . alpha = pm . Uniform ( 'alpha' , 0.00001 , 1000 , value = 1 ) # parameters of the probability model self . beta = pm . Uniform ( 'beta' , 0.00001 , 1000 , value = 1 ) # # determines the probability that randomly chosen customer leaves the company after some each t period @pm.deterministic def P_T_is_t ( alpha = self . alpha , beta = self . beta , num_periods = self . num_periods ): p = [ None , alpha / ( alpha + beta )] #why alpha? maybe beta? #p = [None, beta / (alpha + beta)] for t in range ( 2 , num_periods ): pt = ( beta + t - 2 ) / ( alpha + beta + t - 1 ) * p [ t - 1 ] p . append ( pt ) return p # this is probability of a randomly chosen individual survives to period t given alpha and beta @pm.deterministic def survival_function ( P_T_is_t = P_T_is_t , num_periods = num_periods ): s = [ None , 1 - P_T_is_t [ 1 ]] for t in range ( 2 , num_periods ): s . append ( s [ t - 1 ] - P_T_is_t [ t ]) # iteration: survival[k] = survival[k-1] - proba_to_leave[k-1] return s # likelihood function: P_T_is_t[:]*survival_function[-1] # here P_T_is_t[:] is a product of type: (p_1)&#94;N_1 x (p_2)&#94;N_2 x... where (p_i,N_i) are a probability and number of customers # to leave (die) the company # # survival_function[-1] --> (s_-1)&#94;N(active)_-1 # s_-1 -- survived function (or survived proba) and N(active)_-1 -- active customers to the # end of the last period # # As a summary: this is a Bernoulli model: (p_1)&#94;N_1 x (p_2)&#94;N_2..x survival_function[-1] @pm.observed def retention_rates ( P_T_is_t = P_T_is_t , survival_function = survival_function , value = data ): def logp ( value , P_T_is_t , survival_function ): active , lost = value . T [ 0 ], value . T [ 1 ] # Those who've churned along the way... # i.e, died: # their Log(L_died) is Log(Proba_to_leave_i), where i is an index over all 'lost' customers died = np . log ( P_T_is_t [ 1 :]) * lost [ 1 :] #is a numpy array # and those still active in last period still_active = np . log ( survival_function [ - 1 ]) * active [ - 1 ] # LogLikelihood is sum of the died for each month and still_active in the last period return sum ( died ) + still_active # build a model self . model = pm . MCMC ([ self . alpha , self . beta , P_T_is_t , survival_function , retention_rates ]) What doest P_T_is_t function mean? It corresponds to the formula $$ (p_1)&#94;{N_1} \\times (p_2)&#94;{N_2} \\times...,$$ where $(p_i,N_i)$ are a probability and number of customers left (died) the company. The interesting thing here is that $p_{i+1}=p_i\\times \\frac{\\beta+t_{i-1}}{\\alpha+\\beta+t_{i}}.$ survival_function converserly defines the probability for the customer to continue using the service of the company in each next month. The model is a simple a Bernoulli model: (p_1)&#94;N_1 x (p_2)&#94;N_2..x survival_function[-1] In [5]: chunk_customers_model = Model ( customer_data ) chunk_customers_model . model . sample ( 20000 , 5000 , 20 ) INFO:root:building the model.... [-----------------100%-----------------] 20000 of 20000 complete in 3.7 sec $$\\\\[5pt]$$ Diagnostics Let's see if/how the model converged. In [6]: sns . set ( style = \"darkgrid\" ) # alpha parameter pm . Matplot . plot ( chunk_customers_model . alpha ) # beta parameter pm . Matplot . plot ( chunk_customers_model . beta ) Plotting alpha Plotting beta $$\\\\[5pt]$$ Discounted Expected Residual Lifetime (DERL) The motivation for the DERL is that once you've fit an sBG model to a customer base, an obvious follow up question is, \"how much money can I expect to take in from this customer base in the future?\" The DERL for a customer who pays $x$ Euro per period and who is life (not left company) at the end of the $n&#94;{th}$ period is the number such that $DERL\\times x$ is the expected present value of future payments from that customer. The DERL is a function of $\\alpha, \\beta,$ a discount rate $d$, and the current period $n$ of the subscriber. The DERL can also give us the expected discounted CLV of a new customer, if we set $n=1$ and add an undiscounted initial payment. And because we have posterior distributions for $\\alpha$ and $\\beta$, we can easily leverage our uncertainty in $\\alpha$ and $\\beta$ to understand our uncertainty in the statistic we really care about, the CLV, i.e. the money which the customer brings to our company in the future. The calculation of $DERL\\times x$ will answer on the second qestion formulated by the management of the company. The DERL function is defined as follows: In [7]: from scipy.special import hyp2f1 def _derl ( self , alpha , beta , d , n ): \"\"\" Discounted Expected Residual Lifetime, as derived in Fader and Hardie (2010). See equation (6). :param alpha: sBG alpha param :param beta: sBG beta param :param d: discount rate :param n: customer's contract period (customer has made n-1 renewals) :return: float \"\"\" return ( beta + n - 1 ) / ( alpha + beta + n - 1 ) * hyp2f1 ( 1 , beta + n , alpha + beta + n , 1 / ( 1 + d )) def _prepare_trace ( self ): ''' prepare a dataframe with traces of the alpha and beta parameters of our model ''' self . traces = pd . DataFrame ({ 'alpha' : self . alpha . trace (), 'beta' : self . beta . trace ()}) def _derl_mcmc ( self , n = 1 , d =. 1 , cost = 10 ): ''' builds MCMC traces for DERL and DERL x Cost as functions of the sub-period n and the discount d. ''' derl_label = \"DERL\" + \"_\" + str ( d ) + \"_\" + str ( n ) self . traces [ derl_label ] = self . traces . apply ( lambda x : 1 + self . derl ( x [ 'alpha' ], x [ 'beta' ], d , n ), axis = 1 ) ltv_label = \"LTV\" + \"_\" + str ( d ) + \"_\" + str ( n ) self . traces [ ltv_label ] = float ( cost ) * self . traces [ derl_label ] return ( derl_label , ltv_label ) # Update the definition of the Model Class Model . derl = _derl Model . prepare_trace = _prepare_trace Model . derl_mcmc = _derl_mcmc Let's calculate how much money could bring the new customer in the beginning of the period. In [8]: n = 1. # a customer is alive to the end of the 1st month cost = 10. # the price of the membership d = 0.1 # discount 10% chunk_customers_model . prepare_trace () derl_label , ltv_label = chunk_customers_model . derl_mcmc ( n , d , cost ) To demonstrate DERL Value,we make the function which will plot posterior distribution of the DERL value. In [9]: def plot_derl_value ( df_trace , ltv_label ): ''' plots DERL value posterior distribution for the customers data given by df_trace ''' # plot 95% CL HDI median_clv = df_trace [ ltv_label ] . median () cred_interval = df_trace [ ltv_label ] . quantile ( . 025 ), df_trace [ ltv_label ] . quantile ( . 975 ) ax = df_trace [ ltv_label ] . hist () ax . set_title ( 'Customer Lifetime Value: %s ' % ltv_label ) ax . set_xlabel ( 'Discounted Expected Residual Lifetime Value' ) ax . plot ([ median_clv , median_clv ], ax . get_ylim ()) plt . annotate ( 'Median: %.1f ' % median_clv , xy = ( median_clv + . 02 , ax . get_ylim ()[ 1 ] - 10 )) ax . plot ([ cred_interval [ 0 ], cred_interval [ 0 ]], ax . get_ylim (), c = sns . color_palette ()[ 2 ], lw = 1 ) _ = ax . plot ([ cred_interval [ 1 ], cred_interval [ 1 ]], ax . get_ylim (), c = sns . color_palette ()[ 2 ], lw = 1 ) plot_derl_value ( chunk_customers_model . traces , ltv_label ) Wow! The newcoming customer would bring about 64 Euro to the company before he or she will cancel the membership. Ok. This is a nice result if the Unister Gmbh has millions of the customers. This 64 Euro also tells us about median of life-time for the customer, it is about 6.4 months. If the customer has decided after the 1st month of the membership to prolongate the contract, how much money the company will earn from this membership then? In [10]: n = 2. # a customer is alive to the end of the 2nd month cost = 10. # the price of the membership d = 0.1 # discount 10% derl_label , ltv_label = chunk_customers_model . derl_mcmc ( n , d , cost ) plot_derl_value ( chunk_customers_model . traces , ltv_label ) Now we expect that the customer likes the service and he/she will bring us about 69 Euro. Imagine, that the top management of \"Unister Gmbh\" wants to understand what the income of the company would be (predicted) from all customers registered in the system at that moment. In [11]: def plot_predicted_income ( model , customer_data , d = 0.1 , cost = 10. ): ''' makes plots of the predicted income ''' sns . set_context ( \"poster\" ) plt . figure ( figsize = ( 15 , 8 )) active = customer_data [ \"active\" ] . values lost = customer_data [ \"lost\" ] . values num_periods = len ( active ) ltvs = np . array ([ model . traces [ model . derl_mcmc ( i + 1 , d , cost )[ 1 ] ] . median () for i in range ( num_periods ) ]) cred_interval_low = np . array ([ model . traces [ model . derl_mcmc ( i + 1 , d , cost )[ 1 ] ] . quantile ( . 025 ) for i in range ( num_periods ) ]) cred_interval_low *= active cred_interval_high = np . array ([ model . traces [ model . derl_mcmc ( i + 1 , d , cost )[ 1 ] ] . quantile ( . 975 ) for i in range ( num_periods ) ]) cred_interval_high *= active ltvs *= active real_income = cost * active ; real_income = np . cumsum ( real_income ) pl . plot ( np . array ( range ( num_periods )), ltvs , label = 'Predicted income from the tracked customers' ) pl . fill_between ( np . array ( range ( num_periods )), cred_interval_low , cred_interval_high , facecolor = 'yellow' , alpha = 0.2 , label = '68% CL' ) pl . plot ( np . array ( range ( num_periods )), real_income , label = 'Real income from the tracked customers' ) pl . legend () plot_predicted_income ( chunk_customers_model , customer_data ) So, looking at the plot above, one of the manager could say (in April, it is the sub-period \"3\"): Ok, guys, our sample group of 1000 customers has brought us about 32K Euro up-to now and we expect to get about [45,52]K more from them. There is about 7K uncertainty of the predicted income hidden in the words above. The last points of this study is investigation of the role of the sample size. Somebody in the management of the company has decided to increase the control group used to model the chunk behaviour of the customers. He has decided to use 10K customers in the beginning the period. In [12]: # numbers from CRM department of the Unister GmbH ... :-) num_customers_10K = np . array ([ 1000 , 869 , 743 , 653 , 593 , 551 , 517 , 491 ]) num_customers_10K = 10 * num_customers_10K # the top manager has decided.... previous_data_10K = np . roll ( num_customers_10K , - 1 ) lost_customers_10K = num_customers_10K - previous_data_10K lost_customers_10K = np . r_ [[ None ], lost_customers_10K [: - 1 ]] customer_data_10K = pd . DataFrame ({ \"active\" : num_customers_10K , # customers which are still active \"lost\" : lost_customers_10K , # customers left after each control sub-period (month) }) chunk_customers_model_10K = Model ( customer_data_10K ) chunk_customers_model_10K . model . sample ( 20000 , 5000 , 20 ) INFO:root:building the model.... [-----------------100%-----------------] 20000 of 20000 complete in 3.9 sec Then using this model, he has plotted again the predicted income as a function of the month in the control period: In [13]: chunk_customers_model_10K . prepare_trace () plot_predicted_income ( chunk_customers_model_10K , customer_data ) ... and the uncertainty on the predicted income was significantly decreased. $$\\\\[5pt]$$ References [1]http://danielweitzenfeld.github.io/passtheroc/blog/2015/01/19/s-b-g/ In [ ]: if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML'; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: 'center',\" + \" displayIndent: '0em',\" + \" showMathMenu: true,\" + \" tex2jax: { \" + \" inlineMath: [ ['$','$'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }\" + \" } \" + \"}); \"; (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Statistics","title":"Shifted-Beta-Geometric Customer Lifetime Value Model"},{"url":"http://igormarfin.github.io/blog/2016/02/07/the-online-monitor-pandas-dataframe-transformations/","text":"/*! * * IPython notebook * */.ansibold{font-weight:bold}.ansiblack{color:black}.ansired{color:darkred}.ansigreen{color:darkgreen}.ansiyellow{color:#c4a000}.ansiblue{color:darkblue}.ansipurple{color:darkviolet}.ansicyan{color:steelblue}.ansigray{color:gray}.ansibgblack{background-color:black}.ansibgred{background-color:red}.ansibggreen{background-color:green}.ansibgyellow{background-color:yellow}.ansibgblue{background-color:blue}.ansibgpurple{background-color:magenta}.ansibgcyan{background-color:cyan}.ansibggray{background-color:gray}div.cell{border:1px solid transparent;display:-webkit-box;-webkit-box-orient:vertical;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:vertical;-moz-box-align:stretch;display:box;box-orient:vertical;box-align:stretch;display:flex;flex-direction:column;align-items:stretch;border-radius:2px;box-sizing:border-box;-moz-box-sizing:border-box;-webkit-box-sizing:border-box;border-width:thin;border-style:solid;width:100%;padding:5px;margin:0;outline:none}div.cell.selected{border-color:#ababab}@media print{div.cell.selected{border-color:transparent}}.edit_mode div.cell.selected{border-color:green}@media print{.edit_mode div.cell.selected{border-color:transparent}}.prompt{min-width:14ex;padding:.4em;margin:0;font-family:monospace;text-align:right;line-height:1.21429em}@media (max-width:540px){.prompt{text-align:left}}div.inner_cell{display:-webkit-box;-webkit-box-orient:vertical;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:vertical;-moz-box-align:stretch;display:box;box-orient:vertical;box-align:stretch;display:flex;flex-direction:column;align-items:stretch;-webkit-box-flex:1;-moz-box-flex:1;box-flex:1;flex:1}@-moz-document url-prefix(){div.inner_cell{overflow-x:hidden}}div.input_area{border:1px solid #cfcfcf;border-radius:2px;background:#f7f7f7;line-height:1.21429em}div.prompt:empty{padding-top:0;padding-bottom:0}div.unrecognized_cell{padding:5px 5px 5px 0;display:-webkit-box;-webkit-box-orient:horizontal;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:horizontal;-moz-box-align:stretch;display:box;box-orient:horizontal;box-align:stretch;display:flex;flex-direction:row;align-items:stretch}div.unrecognized_cell .inner_cell{border-radius:2px;padding:5px;font-weight:bold;color:red;border:1px solid #cfcfcf;background:#eaeaea}div.unrecognized_cell .inner_cell a{color:inherit;text-decoration:none}div.unrecognized_cell .inner_cell a:hover{color:inherit;text-decoration:none}@media (max-width:540px){div.unrecognized_cell>div.prompt{display:none}}@media print{div.code_cell{page-break-inside:avoid}}div.input{page-break-inside:avoid;display:-webkit-box;-webkit-box-orient:horizontal;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:horizontal;-moz-box-align:stretch;display:box;box-orient:horizontal;box-align:stretch;display:flex;flex-direction:row;align-items:stretch}@media (max-width:540px){div.input{display:-webkit-box;-webkit-box-orient:vertical;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:vertical;-moz-box-align:stretch;display:box;box-orient:vertical;box-align:stretch;display:flex;flex-direction:column;align-items:stretch}}div.input_prompt{color:navy;border-top:1px solid transparent}div.input_area>div.highlight{margin:.4em;border:none;padding:0;background-color:transparent}div.input_area>div.highlight>pre{margin:0;border:none;padding:0;background-color:transparent}.CodeMirror{line-height:1.21429em;font-size:14px;height:auto;background:none}.CodeMirror-scroll{overflow-y:hidden;overflow-x:auto}.CodeMirror-lines{padding:.4em}.CodeMirror-linenumber{padding:0 8px 0 4px}.CodeMirror-gutters{border-bottom-left-radius:2px;border-top-left-radius:2px}.CodeMirror pre{padding:0;border:0;border-radius:0}.highlight-base{}.highlight-variable{}.highlight-variable-2{color:#1a1a1a}.highlight-variable-3{color:#333}.highlight-string{color:#ba2121}.highlight-comment{color:#408080;font-style:italic}.highlight-number{80}.highlight-atom{color:#88f}.highlight-keyword{8000;font-weight:bold}.highlight-builtin{8000}.highlight-error{color:#f00}.highlight-operator{color:#a2f;font-weight:bold}.highlight-meta{color:#a2f}.highlight-def{f}.highlight-string-2{color:#f50}.highlight-qualifier{color:#555}.highlight-bracket{color:#997}.highlight-tag{color:#170}.highlight-attribute{c}.highlight-header{color:blue}.highlight-quote{90}.highlight-link{c}.cm-s-ipython span.cm-keyword{8000;font-weight:bold}.cm-s-ipython span.cm-atom{color:#88f}.cm-s-ipython span.cm-number{80}.cm-s-ipython span.cm-def{f}.cm-s-ipython span.cm-variable{}.cm-s-ipython span.cm-operator{color:#a2f;font-weight:bold}.cm-s-ipython span.cm-variable-2{color:#1a1a1a}.cm-s-ipython span.cm-variable-3{color:#333}.cm-s-ipython span.cm-comment{color:#408080;font-style:italic}.cm-s-ipython span.cm-string{color:#ba2121}.cm-s-ipython span.cm-string-2{color:#f50}.cm-s-ipython span.cm-meta{color:#a2f}.cm-s-ipython span.cm-qualifier{color:#555}.cm-s-ipython span.cm-builtin{8000}.cm-s-ipython span.cm-bracket{color:#997}.cm-s-ipython span.cm-tag{color:#170}.cm-s-ipython span.cm-attribute{c}.cm-s-ipython span.cm-header{color:blue}.cm-s-ipython span.cm-quote{90}.cm-s-ipython span.cm-link{c}.cm-s-ipython span.cm-error{color:#f00}.cm-s-ipython span.cm-tab{background:url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADAAAAAMCAYAAAAkuj5RAAAAAXNSR0IArs4c6QAAAGFJREFUSMft1LsRQFAQheHPowAKoACx3IgEKtaEHujDjORSgWTH/ZOdnZOcM/sgk/kFFWY0qV8foQwS4MKBCS3qR6ixBJvElOobYAtivseIE120FaowJPN75GMu8j/LfMwNjh4HUpwg4LUAAAAASUVORK5CYII=);background-position:right;background-repeat:no-repeat}div.output_wrapper{position:relative;display:-webkit-box;-webkit-box-orient:vertical;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:vertical;-moz-box-align:stretch;display:box;box-orient:vertical;box-align:stretch;display:flex;flex-direction:column;align-items:stretch}div.output_scroll{height:24em;width:100%;overflow:auto;border-radius:2px;-webkit-box-shadow:inset 0 2px 8px rgba(0,0,0,0.8);box-shadow:inset 0 2px 8px rgba(0,0,0,0.8);display:block}div.output_collapsed{margin:0;padding:0;display:-webkit-box;-webkit-box-orient:vertical;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:vertical;-moz-box-align:stretch;display:box;box-orient:vertical;box-align:stretch;display:flex;flex-direction:column;align-items:stretch}div.out_prompt_overlay{height:100%;padding:0 .4em;position:absolute;border-radius:2px}div.out_prompt_overlay:hover{-webkit-box-shadow:inset 0 0 1px #000;box-shadow:inset 0 0 1px #000;background:rgba(240,240,240,0.5)}div.output_prompt{color:darkred}div.output_area{padding:0;page-break-inside:avoid;display:-webkit-box;-webkit-box-orient:horizontal;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:horizontal;-moz-box-align:stretch;display:box;box-orient:horizontal;box-align:stretch;display:flex;flex-direction:row;align-items:stretch}div.output_area .MathJax_Display{text-align:left !important}div.output_area div.output_area .output{display:-webkit-box;-webkit-box-orient:vertical;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:vertical;-moz-box-align:stretch;display:box;box-orient:vertical;box-align:stretch;display:flex;flex-direction:column;align-items:stretch}@media (max-width:540px){div.output_area{display:-webkit-box;-webkit-box-orient:vertical;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:vertical;-moz-box-align:stretch;display:box;box-orient:vertical;box-align:stretch;display:flex;flex-direction:column;align-items:stretch}}div.output_area pre{margin:0;padding:0;border:0;vertical-align:baseline;color:black;background-color:transparent;border-radius:0}div.output_subarea{padding:.4em;-webkit-box-flex:1;-moz-box-flex:1;box-flex:1;flex:1}div.output_text{text-align:left;line-height:1.21429em}div.output_stderr{background:#fdd}div.output_latex{text-align:left}div.output_javascript:empty{padding:0}.js-error{color:darkred}div.raw_input_container{font-family:monospace;padding-top:5px}input.raw_input{font-family:inherit;font-size:inherit;color:inherit;width:auto;vertical-align:baseline;padding:0 .25em;margin:0 .25em}input.raw_input:focus{box-shadow:none}p.p-space{margin-bottom:10px}div.output_unrecognized{padding:5px;font-weight:bold;color:red}div.output_unrecognized a{color:inherit;text-decoration:none}div.output_unrecognized a:hover{color:inherit;text-decoration:none}.rendered_html{}.rendered_html :link{text-decoration:underline}.rendered_html :visited{text-decoration:underline}.rendered_html h1:first-child{margin-top:.538em}.rendered_html h2:first-child{margin-top:.636em}.rendered_html h3:first-child{margin-top:.777em}.rendered_html h4:first-child{margin-top:1em}.rendered_html h5:first-child{margin-top:1em}.rendered_html h6:first-child{margin-top:1em}.rendered_html *+ul{margin-top:1em}.rendered_html *+ol{margin-top:1em}.rendered_html *+table{margin-top:1em}.rendered_html *+p{margin-top:1em}.rendered_html *+img{margin-top:1em}div.text_cell{display:-webkit-box;-webkit-box-orient:horizontal;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:horizontal;-moz-box-align:stretch;display:box;box-orient:horizontal;box-align:stretch;display:flex;flex-direction:row;align-items:stretch}@media (max-width:540px){div.text_cell>div.prompt{display:none}}div.text_cell_render{outline:none;resize:none;width:inherit;border-style:none;padding:.5em .5em .5em .4em;box-sizing:border-box;-moz-box-sizing:border-box;-webkit-box-sizing:border-box}a.anchor-link:link{text-decoration:none;padding:0 20px;visibility:hidden}h1:hover .anchor-link,h2:hover .anchor-link,h3:hover .anchor-link,h4:hover .anchor-link,h5:hover .anchor-link,h6:hover .anchor-link{visibility:visible}.text_cell.rendered .input_area{display:none}.text_cell.unrendered .text_cell_render{display:none}.cm-header-1,.cm-header-2,.cm-header-3,.cm-header-4,.cm-header-5,.cm-header-6{font-weight:bold;font-family:\"Helvetica Neue\",Helvetica,Arial,sans-serif}.cm-header-1{font-size:185.7%}.cm-header-2{font-size:157.1%}.cm-header-3{font-size:128.6%}.cm-header-4{font-size:110%}.cm-header-5{font-size:100%;font-style:italic}.cm-header-6{font-size:100%;font-style:italic}.widget-interact>div,.widget-interact>input{padding:2.5px}.widget-area{page-break-inside:avoid;display:-webkit-box;-webkit-box-orient:horizontal;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:horizontal;-moz-box-align:stretch;display:box;box-orient:horizontal;box-align:stretch;display:flex;flex-direction:row;align-items:stretch}.widget-area .widget-subarea{padding:.44em .4em .4em 1px;margin-left:6px;box-sizing:border-box;-moz-box-sizing:border-box;-webkit-box-sizing:border-box;display:-webkit-box;-webkit-box-orient:vertical;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:vertical;-moz-box-align:stretch;display:box;box-orient:vertical;box-align:stretch;display:flex;flex-direction:column;align-items:stretch;-webkit-box-flex:2;-moz-box-flex:2;box-flex:2;flex:2;-webkit-box-align:start;-moz-box-align:start;box-align:start;align-items:flex-start}.widget-area.connection-problems .prompt:after{content:\"\\f127\";font-family:'FontAwesome';color:#d9534f;font-size:14px;top:3px;padding:3px}.slide-track{border:1px solid #ccc;background:#fff;border-radius:2px}.widget-hslider{padding-left:8px;padding-right:2px;overflow:visible;width:350px;height:5px;max-height:5px;margin-top:13px;margin-bottom:10px;border:1px solid #ccc;background:#fff;border-radius:2px;display:-webkit-box;-webkit-box-orient:horizontal;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:horizontal;-moz-box-align:stretch;display:box;box-orient:horizontal;box-align:stretch;display:flex;flex-direction:row;align-items:stretch}.widget-hslider .ui-slider{border:0;background:none;display:-webkit-box;-webkit-box-orient:horizontal;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:horizontal;-moz-box-align:stretch;display:box;box-orient:horizontal;box-align:stretch;display:flex;flex-direction:row;align-items:stretch;-webkit-box-flex:1;-moz-box-flex:1;box-flex:1;flex:1}.widget-hslider .ui-slider .ui-slider-handle{width:12px;height:28px;margin-top:-8px;border-radius:2px}.widget-hslider .ui-slider .ui-slider-range{height:12px;margin-top:-4px;background:#eee}.widget-vslider{padding-bottom:5px;overflow:visible;width:5px;max-width:5px;height:250px;margin-left:12px;border:1px solid #ccc;background:#fff;border-radius:2px;display:-webkit-box;-webkit-box-orient:vertical;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:vertical;-moz-box-align:stretch;display:box;box-orient:vertical;box-align:stretch;display:flex;flex-direction:column;align-items:stretch}.widget-vslider .ui-slider{border:0;background:none;margin-left:-4px;margin-top:5px;display:-webkit-box;-webkit-box-orient:vertical;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:vertical;-moz-box-align:stretch;display:box;box-orient:vertical;box-align:stretch;display:flex;flex-direction:column;align-items:stretch;-webkit-box-flex:1;-moz-box-flex:1;box-flex:1;flex:1}.widget-vslider .ui-slider .ui-slider-handle{width:28px;height:12px;margin-left:-9px;border-radius:2px}.widget-vslider .ui-slider .ui-slider-range{width:12px;margin-left:-1px;background:#eee}.widget-text{width:350px;margin:0}.widget-listbox{width:350px;margin-bottom:0}.widget-numeric-text{width:150px;margin:0}.widget-progress{margin-top:6px;min-width:350px}.widget-progress .progress-bar{-webkit-transition:none;-moz-transition:none;-ms-transition:none;-o-transition:none;transition:none}.widget-combo-btn{min-width:125px}.widget_item .dropdown-menu li a{color:inherit}.widget-hbox{display:-webkit-box;-webkit-box-orient:horizontal;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:horizontal;-moz-box-align:stretch;display:box;box-orient:horizontal;box-align:stretch;display:flex;flex-direction:row;align-items:stretch}.widget-hbox input[type=\"checkbox\"]{margin-top:9px;margin-bottom:10px}.widget-hbox .widget-label{min-width:10ex;padding-right:8px;padding-top:5px;text-align:right;vertical-align:text-top}.widget-hbox .widget-readout{padding-left:8px;padding-top:5px;text-align:left;vertical-align:text-top}.widget-vbox{display:-webkit-box;-webkit-box-orient:vertical;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:vertical;-moz-box-align:stretch;display:box;box-orient:vertical;box-align:stretch;display:flex;flex-direction:column;align-items:stretch}.widget-vbox .widget-label{padding-bottom:5px;text-align:center;vertical-align:text-bottom}.widget-vbox .widget-readout{padding-top:5px;text-align:center;vertical-align:text-top}.widget-box{box-sizing:border-box;-moz-box-sizing:border-box;-webkit-box-sizing:border-box;-webkit-box-align:start;-moz-box-align:start;box-align:start;align-items:flex-start}.widget-radio-box{display:-webkit-box;-webkit-box-orient:vertical;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:vertical;-moz-box-align:stretch;display:box;box-orient:vertical;box-align:stretch;display:flex;flex-direction:column;align-items:stretch;box-sizing:border-box;-moz-box-sizing:border-box;-webkit-box-sizing:border-box;padding-top:4px}.widget-radio-box label{margin-top:0}.widget-radio{margin-left:20px} .highlight .hll { background-color: #ffffcc } .highlight { background: #f8f8f8; } .highlight .c { color: #408080; font-style: italic } /* Comment */ .highlight .err { border: 1px solid #FF0000 } /* Error */ .highlight .k { color: #008000; font-weight: bold } /* Keyword */ .highlight .o { color: #666666 } /* Operator */ .highlight .cm { color: #408080; font-style: italic } /* Comment.Multiline */ .highlight .cp { color: #BC7A00 } /* Comment.Preproc */ .highlight .c1 { color: #408080; font-style: italic } /* Comment.Single */ .highlight .cs { color: #408080; font-style: italic } /* Comment.Special */ .highlight .gd { color: #A00000 } /* Generic.Deleted */ .highlight .ge { font-style: italic } /* Generic.Emph */ .highlight .gr { color: #FF0000 } /* Generic.Error */ .highlight .gh { color: #000080; font-weight: bold } /* Generic.Heading */ .highlight .gi { color: #00A000 } /* Generic.Inserted */ .highlight .go { color: #888888 } /* Generic.Output */ .highlight .gp { color: #000080; font-weight: bold } /* Generic.Prompt */ .highlight .gs { font-weight: bold } /* Generic.Strong */ .highlight .gu { color: #800080; font-weight: bold } /* Generic.Subheading */ .highlight .gt { color: #0044DD } /* Generic.Traceback */ .highlight .kc { color: #008000; font-weight: bold } /* Keyword.Constant */ .highlight .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */ .highlight .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */ .highlight .kp { color: #008000 } /* Keyword.Pseudo */ .highlight .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */ .highlight .kt { color: #B00040 } /* Keyword.Type */ .highlight .m { color: #666666 } /* Literal.Number */ .highlight .s { color: #BA2121 } /* Literal.String */ .highlight .na { color: #7D9029 } /* Name.Attribute */ .highlight .nb { color: #008000 } /* Name.Builtin */ .highlight .nc { color: #0000FF; font-weight: bold } /* Name.Class */ .highlight .no { color: #880000 } /* Name.Constant */ .highlight .nd { color: #AA22FF } /* Name.Decorator */ .highlight .ni { color: #999999; font-weight: bold } /* Name.Entity */ .highlight .ne { color: #D2413A; font-weight: bold } /* Name.Exception */ .highlight .nf { color: #0000FF } /* Name.Function */ .highlight .nl { color: #A0A000 } /* Name.Label */ .highlight .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */ .highlight .nt { color: #008000; font-weight: bold } /* Name.Tag */ .highlight .nv { color: #19177C } /* Name.Variable */ .highlight .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */ .highlight .w { color: #bbbbbb } /* Text.Whitespace */ .highlight .mb { color: #666666 } /* Literal.Number.Bin */ .highlight .mf { color: #666666 } /* Literal.Number.Float */ .highlight .mh { color: #666666 } /* Literal.Number.Hex */ .highlight .mi { color: #666666 } /* Literal.Number.Integer */ .highlight .mo { color: #666666 } /* Literal.Number.Oct */ .highlight .sb { color: #BA2121 } /* Literal.String.Backtick */ .highlight .sc { color: #BA2121 } /* Literal.String.Char */ .highlight .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */ .highlight .s2 { color: #BA2121 } /* Literal.String.Double */ .highlight .se { color: #BB6622; font-weight: bold } /* Literal.String.Escape */ .highlight .sh { color: #BA2121 } /* Literal.String.Heredoc */ .highlight .si { color: #BB6688; font-weight: bold } /* Literal.String.Interpol */ .highlight .sx { color: #008000 } /* Literal.String.Other */ .highlight .sr { color: #BB6688 } /* Literal.String.Regex */ .highlight .s1 { color: #BA2121 } /* Literal.String.Single */ .highlight .ss { color: #19177C } /* Literal.String.Symbol */ .highlight .bp { color: #008000 } /* Name.Builtin.Pseudo */ .highlight .vc { color: #19177C } /* Name.Variable.Class */ .highlight .vg { color: #19177C } /* Name.Variable.Global */ .highlight .vi { color: #19177C } /* Name.Variable.Instance */ .highlight .il { color: #666666 } /* Literal.Number.Integer.Long */ Online Monitor for the Pandas Dataframe Transformations $$\\\\[2pt]$$ Igor Marfin [Unister Gmb@2014] < igor.marfin@unister.de > $$\\\\[40pt]$$ Table of Contents 0.1 Abstract 0.2 Initialization 0.3 Introduction to the topic and motivation 0.3.1 Utility functions $$\\\\[10pt]$$ 0.1 Abstract Today I want to introduce you to my project called Online Monitor of the pandas dataframe transformations . The source code and documentation can be found in the repository (I.Marfin:Repository_of_the_project, 2015) This project illustrates the practical steps of the making online monitor to trace the status of the long-time transformations of the pandas dataframe. More details can be found at https://bitbucket.org/iggy_floyd/pandas-dataframe-transformation-monitor . $$\\\\[5pt]$$ 0.2 Initialization To set up the python environment for the data analysis and make a nicer style of the notebook, one can run the following commands in the beginning of our modeling: In [2]: import sys sys . path = [ '/usr/local/lib/python2.7/dist-packages' ] + sys . path # to fix the problem with numpy: this replaces 1.6 version by 1.9 % matplotlib inline % pylab inline ion () import os import matplotlib import numpy as np import matplotlib.pyplot as pl import matplotlib as mpl import logging import pymc as pm # a plotter and dataframe modules import seaborn as sns # seaborn to make a nice plots of the data import pandas as pd import scipy.stats as stats # Set up logging. logger = logging . getLogger () logger . setLevel ( logging . INFO ) from book_format import load_style , figsize , set_figsize load_style () Populating the interactive namespace from numpy and matplotlib Out[2]: @import url('http://fonts.googleapis.com/css?family=Source+Code+Pro'); @import url('http://fonts.googleapis.com/css?family=Vollkorn'); @import url('http://fonts.googleapis.com/css?family=Arimo'); div.cell{ width: 1200px; margin-left: 0% !important; margin-right: auto; } div.text_cell code { background: transparent; color: #000000; font-weight: 600; font-size: 11pt; font-style: bold; font-family: 'Source Code Pro', Consolas, monocco, monospace; } h1 { font-family: 'Open sans',verdana,arial,sans-serif; } div.input_area { background: #F6F6F9; border: 1px solid #586e75; } .text_cell_render h1 { font-weight: 200; font-size: 30pt; line-height: 100%; color:#c76c0c; margin-bottom: 0.5em; margin-top: 1em; display: block; white-space: wrap; } h2 { font-family: 'Open sans',verdana,arial,sans-serif; } .text_cell_render h2 { font-weight: 200; font-size: 16pt; font-style: italic; line-height: 100%; color:#c76c0c; margin-bottom: 0.5em; margin-top: 1.5em; display: inline; white-space: wrap; } h3 { font-family: 'Open sans',verdana,arial,sans-serif; } .text_cell_render h3 { font-weight: 200; font-size: 14pt; line-height: 100%; color:#d77c0c; margin-bottom: 0.5em; margin-top: 2em; display: block; white-space: nowrap; } h4 { font-family: 'Open sans',verdana,arial,sans-serif; } .text_cell_render h4 { font-weight: 100; font-size: 14pt; color:#d77c0c; margin-bottom: 0.5em; margin-top: 0.5em; display: block; white-space: nowrap; } h5 { font-family: 'Open sans',verdana,arial,sans-serif; } .text_cell_render h5 { font-weight: 200; font-style: normal; color: #1d3b84; font-size: 16pt; margin-bottom: 0em; margin-top: 0.5em; display: block; white-space: nowrap; } div.text_cell_render{ font-family: 'Arimo',verdana,arial,sans-serif; line-height: 125%; font-size: 120%; text-align:justify; text-justify:inter-word; } div.output_subarea.output_text.output_pyout { overflow-x: auto; overflow-y: scroll; max-height: 50000px; } div.output_subarea.output_stream.output_stdout.output_text { overflow-x: auto; overflow-y: scroll; max-height: 50000px; } div.output_wrapper{ margin-top:0.2em; margin-bottom:0.2em; } code{ font-size: 70%; } .rendered_html code{ background-color: transparent; } ul{ margin: 2em; } ul li{ padding-left: 0.5em; margin-bottom: 0.5em; margin-top: 0.5em; } ul li li{ padding-left: 0.2em; margin-bottom: 0.2em; margin-top: 0.2em; } ol{ margin: 2em; } ol li{ padding-left: 0.5em; margin-bottom: 0.5em; margin-top: 0.5em; } ul li{ padding-left: 0.5em; margin-bottom: 0.5em; margin-top: 0.2em; } a:link{ font-weight: bold; color:#447adb; } a:visited{ font-weight: bold; color: #1d3b84; } a:hover{ font-weight: bold; color: #1d3b84; } a:focus{ font-weight: bold; color:#447adb; } a:active{ font-weight: bold; color:#447adb; } .rendered_html :link { text-decoration: underline; } .rendered_html :hover { text-decoration: none; } .rendered_html :visited { text-decoration: none; } .rendered_html :focus { text-decoration: none; } .rendered_html :active { text-decoration: none; } .warning{ color: rgb( 240, 20, 20 ) } hr { color: #f3f3f3; background-color: #f3f3f3; height: 1px; } blockquote{ display:block; background: #fcfcfc; border-left: 5px solid #c76c0c; font-family: 'Open sans',verdana,arial,sans-serif; width:1000px; padding: 10px 10px 10px 10px; text-align:justify; text-justify:inter-word; } blockquote p { margin-bottom: 0; line-height: 125%; font-size: 100%; } MathJax.Hub.Config({ TeX: { extensions: [\"AMSmath.js\"] }, tex2jax: { inlineMath: [ ['$','$'], [\"\\\\(\",\"\\\\)\"] ], displayMath: [ ['$$','$$'], [\"\\\\[\",\"\\\\]\"] ] }, displayAlign: 'center', // Change this to 'center' to center equations. \"HTML-CSS\": { scale:100, availableFonts: [\"Neo-Euler\"], preferredFont: \"Neo-Euler\", webFont: \"Neo-Euler\", styles: {'.MathJax_Display': {\"margin\": 4}} } }); In [2]: %% javascript IPython . load_extensions ( \"calico-spell-check\" , \"calico-document-tools\" , \"calico-cell-tools\" ); <IPython.core.display.Javascript object> $$\\\\[5pt]$$ 0.3 Introduction to the topic and motivation My motivations, which force me to develop this project, are quite straightforward, and I will explain them now. The main part of any statistical inference is preparation of data to consumable format and generation of new data on the basis of existing information. For pythonic statisticians, this means that a pandas dataframe has to be built from the database or file new data should be inserted into the dataframe the dataframe should be transformed (normalized) All steps usually take much time if the number of entries is large. If you run your analysis on the desktop, you can logout the trace of your computations to the display via calls of sys.stdout(...) or print(...) . In the case when your code is running online in a cloud or on some remote server, such approach will be problematic. Also the logging of your calculations to files remotely, can not serve your intentions if you don't have a ssh tunnel to the remote server. So it would be interesting to develop such system which would monitor processes of the dataframe transformation in real time and be accessible in the Internet. The project (I.Marfin:Repository_of_the_project, 2015) illustrates the practical steps which one can make to build such system. $$\\\\[5pt]$$ Installation Installation looks easy. Simply, clone the project git clone https://iggy_floyd@bitbucket.org/iggy_floyd/pandas-dataframe-transformation-monitor.git and test a configuration of your system that you have all components installed: python scientific python modules In order to do this, you can just run the default rule of Makefile via the command in the shell: make In [1]: ! cd bin ; ./setup.daemons.sh ; export DAEMON_NAME = MONITORSOCKETIO_MULTI_TRANSFORM ; ./daemon.sh start ; #!cd bin; ./setup.daemons.sh; export DAEMON_NAME=NGROK; ./daemon.sh start; &#94;C In [52]: from IPython.display import IFrame IFrame ( \"templates/monitor_multi_transform_socketio.html\" , width = \"100%\" , height = 800 ) Out[52]: In [3]: # it seems that the ./daemon.sh doesn't work (:- #!cd bin; ./setup.daemons.sh; export DAEMON_NAME=MONITORSOCKETIO_MULTI_TRANSFORM; ./daemon.sh stop; ! ps aux | grep monitorserver_multi_transform | awk '{print $2}' | xargs -I {} kill -9 {} ! ps aux | grep ngrok | awk '{print $2}' | xargs -I {} kill -9 {} In [17]: % install_ext https://raw.githubusercontent.com/mozilla/kuma-lib/master/packages/ipython/IPython/Extensions/jobctrl.py Installed jobctrl.py. To use it, type: %load_ext jobctrl In [19]: %% script bash --bg --out script_out sleep 10 echo hi! Starting job # 0 in a separate thread. In [20]: %% script bash --bg --out script_out2 echo hi! Starting job # 2 in a separate thread. In [18]: % load_ext jobctrl --------------------------------------------------------------------------- ImportError Traceback (most recent call last) <ipython-input-18-2aaeaf5d2bfb> in <module> () ----> 1 get_ipython ( ) . magic ( u'load_ext jobctrl' ) /usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.pyc in magic (self, arg_s) 2302 magic_name , _ , magic_arg_s = arg_s . partition ( ' ' ) 2303 magic_name = magic_name . lstrip ( prefilter . ESC_MAGIC ) -> 2304 return self . run_line_magic ( magic_name , magic_arg_s ) 2305 2306 #------------------------------------------------------------------------- /usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.pyc in run_line_magic (self, magic_name, line) 2223 kwargs [ 'local_ns' ] = sys . _getframe ( stack_depth ) . f_locals 2224 with self . builtin_trap : -> 2225 result = fn ( * args , ** kwargs ) 2226 return result 2227 /usr/local/lib/python2.7/dist-packages/IPython/core/magics/extension.pyc in load_ext (self, module_str) /usr/local/lib/python2.7/dist-packages/IPython/core/magic.pyc in <lambda> (f, *a, **k) 191 # but it's overkill for just that one bit of state. 192 def magic_deco ( arg ) : --> 193 call = lambda f , * a , ** k : f ( * a , ** k ) 194 195 if callable ( arg ) : /usr/local/lib/python2.7/dist-packages/IPython/core/magics/extension.pyc in load_ext (self, module_str) 61 if not module_str : 62 raise UsageError ( 'Missing module name.' ) ---> 63 res = self . shell . extension_manager . load_extension ( module_str ) 64 65 if res == 'already loaded' : /usr/local/lib/python2.7/dist-packages/IPython/core/extensions.pyc in load_extension (self, module_str) 83 if module_str not in sys . modules : 84 with prepended_to_syspath ( self . ipython_extension_dir ) : ---> 85 __import__ ( module_str ) 86 mod = sys . modules [ module_str ] 87 if self . _call_load_ipython_extension ( mod ) : /home/debian/.ipython/extensions/jobctrl.py in <module> () 46 import threading , Queue 47 ---> 48 from IPython import genutils 49 50 import IPython . ipapi ImportError : cannot import name genutils In [7]: from IPython import parallel c = parallel . Client () view = c . load_balanced_view () In [10]: %% px --local class MyLongAssComputation(object): def compute(self, x): return x**2 def compute_func(x): import sys sys.path = ['/usr/local/lib/python2.7/dist-packages'] + sys.path # to fix the problem with numpy: this replaces 1.6 version by 1.9 import pandas as pd import pickle c = MyLongAssComputation() return c.compute(x) In [11]: x = np . arange ( 1 , 1000 ) squared = view . map_sync ( compute_func , x ) pickle . dump ( squared , open ( 'output.pickle' , 'w' )) [0:apply]: --------------------------------------------------------------------------- ValueError Traceback (most recent call last) <string> in <module> () <ipython-input-10-780d27adc11c> in compute_func (x) /usr/local/lib/python2.7/dist-packages/pandas/__init__.py in <module> () 5 6 try : ----> 7 from . import hashtable , tslib , lib 8 except Exception : # pragma: no cover 9 import sys pandas/src/numpy.pxd in init pandas.lib (pandas/lib.c:78660) () ValueError : numpy.dtype has the wrong size, try recompiling [2:apply]: --------------------------------------------------------------------------- ValueError Traceback (most recent call last) <string> in <module> () <ipython-input-10-780d27adc11c> in compute_func (x) /usr/local/lib/python2.7/dist-packages/pandas/__init__.py in <module> () 5 6 try : ----> 7 from . import hashtable , tslib , lib 8 except Exception : # pragma: no cover 9 import sys pandas/src/numpy.pxd in init pandas.lib (pandas/lib.c:78660) () ValueError : numpy.dtype has the wrong size, try recompiling [1:apply]: --------------------------------------------------------------------------- ValueError Traceback (most recent call last) <string> in <module> () <ipython-input-10-780d27adc11c> in compute_func (x) /usr/local/lib/python2.7/dist-packages/pandas/__init__.py in <module> () 5 6 try : ----> 7 from . import hashtable , tslib , lib 8 except Exception : # pragma: no cover 9 import sys pandas/src/numpy.pxd in init pandas.lib (pandas/lib.c:78660) () ValueError : numpy.dtype has the wrong size, try recompiling [1:apply]: --------------------------------------------------------------------------- ImportError Traceback (most recent call last) <string> in <module> () <ipython-input-10-780d27adc11c> in compute_func (x) /usr/local/lib/python2.7/dist-packages/pandas/__init__.py in <module> () 5 6 try : ----> 7 from . import hashtable , tslib , lib 8 except Exception : # pragma: no cover 9 import sys ImportError : cannot import name hashtable ... 995 more exceptions ... In [4]: ### Do not delete the following Markdown section! ### This is the BibTeX references! References &#94; &#94; I. Marfin: Repository of_the_project,. 2015. _Online Monitor for the Pandas Dataframe Transformations . URL &#94; D. WARNER NORTH,. 1968. A Tutorial Introduction to Decision Theory . URL if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML'; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: 'center',\" + \" displayIndent: '0em',\" + \" showMathMenu: true,\" + \" tex2jax: { \" + \" inlineMath: [ ['$','$'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }\" + \" } \" + \"}); \"; (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Programming","title":"The online-monitor-pandas-dataframe-transformations"},{"url":"http://igormarfin.github.io/blog/2016/02/07/bayesian-risk/","text":"/*! * * IPython notebook * */.ansibold{font-weight:bold}.ansiblack{color:black}.ansired{color:darkred}.ansigreen{color:darkgreen}.ansiyellow{color:#c4a000}.ansiblue{color:darkblue}.ansipurple{color:darkviolet}.ansicyan{color:steelblue}.ansigray{color:gray}.ansibgblack{background-color:black}.ansibgred{background-color:red}.ansibggreen{background-color:green}.ansibgyellow{background-color:yellow}.ansibgblue{background-color:blue}.ansibgpurple{background-color:magenta}.ansibgcyan{background-color:cyan}.ansibggray{background-color:gray}div.cell{border:1px solid transparent;display:-webkit-box;-webkit-box-orient:vertical;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:vertical;-moz-box-align:stretch;display:box;box-orient:vertical;box-align:stretch;display:flex;flex-direction:column;align-items:stretch;border-radius:2px;box-sizing:border-box;-moz-box-sizing:border-box;-webkit-box-sizing:border-box;border-width:thin;border-style:solid;width:100%;padding:5px;margin:0;outline:none}div.cell.selected{border-color:#ababab}@media print{div.cell.selected{border-color:transparent}}.edit_mode div.cell.selected{border-color:green}@media print{.edit_mode div.cell.selected{border-color:transparent}}.prompt{min-width:14ex;padding:.4em;margin:0;font-family:monospace;text-align:right;line-height:1.21429em}@media (max-width:540px){.prompt{text-align:left}}div.inner_cell{display:-webkit-box;-webkit-box-orient:vertical;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:vertical;-moz-box-align:stretch;display:box;box-orient:vertical;box-align:stretch;display:flex;flex-direction:column;align-items:stretch;-webkit-box-flex:1;-moz-box-flex:1;box-flex:1;flex:1}@-moz-document url-prefix(){div.inner_cell{overflow-x:hidden}}div.input_area{border:1px solid #cfcfcf;border-radius:2px;background:#f7f7f7;line-height:1.21429em}div.prompt:empty{padding-top:0;padding-bottom:0}div.unrecognized_cell{padding:5px 5px 5px 0;display:-webkit-box;-webkit-box-orient:horizontal;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:horizontal;-moz-box-align:stretch;display:box;box-orient:horizontal;box-align:stretch;display:flex;flex-direction:row;align-items:stretch}div.unrecognized_cell .inner_cell{border-radius:2px;padding:5px;font-weight:bold;color:red;border:1px solid #cfcfcf;background:#eaeaea}div.unrecognized_cell .inner_cell a{color:inherit;text-decoration:none}div.unrecognized_cell .inner_cell a:hover{color:inherit;text-decoration:none}@media (max-width:540px){div.unrecognized_cell>div.prompt{display:none}}@media print{div.code_cell{page-break-inside:avoid}}div.input{page-break-inside:avoid;display:-webkit-box;-webkit-box-orient:horizontal;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:horizontal;-moz-box-align:stretch;display:box;box-orient:horizontal;box-align:stretch;display:flex;flex-direction:row;align-items:stretch}@media (max-width:540px){div.input{display:-webkit-box;-webkit-box-orient:vertical;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:vertical;-moz-box-align:stretch;display:box;box-orient:vertical;box-align:stretch;display:flex;flex-direction:column;align-items:stretch}}div.input_prompt{color:navy;border-top:1px solid transparent}div.input_area>div.highlight{margin:.4em;border:none;padding:0;background-color:transparent}div.input_area>div.highlight>pre{margin:0;border:none;padding:0;background-color:transparent}.CodeMirror{line-height:1.21429em;font-size:14px;height:auto;background:none}.CodeMirror-scroll{overflow-y:hidden;overflow-x:auto}.CodeMirror-lines{padding:.4em}.CodeMirror-linenumber{padding:0 8px 0 4px}.CodeMirror-gutters{border-bottom-left-radius:2px;border-top-left-radius:2px}.CodeMirror pre{padding:0;border:0;border-radius:0}.highlight-base{}.highlight-variable{}.highlight-variable-2{color:#1a1a1a}.highlight-variable-3{color:#333}.highlight-string{color:#ba2121}.highlight-comment{color:#408080;font-style:italic}.highlight-number{80}.highlight-atom{color:#88f}.highlight-keyword{8000;font-weight:bold}.highlight-builtin{8000}.highlight-error{color:#f00}.highlight-operator{color:#a2f;font-weight:bold}.highlight-meta{color:#a2f}.highlight-def{f}.highlight-string-2{color:#f50}.highlight-qualifier{color:#555}.highlight-bracket{color:#997}.highlight-tag{color:#170}.highlight-attribute{c}.highlight-header{color:blue}.highlight-quote{90}.highlight-link{c}.cm-s-ipython span.cm-keyword{8000;font-weight:bold}.cm-s-ipython span.cm-atom{color:#88f}.cm-s-ipython span.cm-number{80}.cm-s-ipython span.cm-def{f}.cm-s-ipython span.cm-variable{}.cm-s-ipython span.cm-operator{color:#a2f;font-weight:bold}.cm-s-ipython span.cm-variable-2{color:#1a1a1a}.cm-s-ipython span.cm-variable-3{color:#333}.cm-s-ipython span.cm-comment{color:#408080;font-style:italic}.cm-s-ipython span.cm-string{color:#ba2121}.cm-s-ipython span.cm-string-2{color:#f50}.cm-s-ipython span.cm-meta{color:#a2f}.cm-s-ipython span.cm-qualifier{color:#555}.cm-s-ipython span.cm-builtin{8000}.cm-s-ipython span.cm-bracket{color:#997}.cm-s-ipython span.cm-tag{color:#170}.cm-s-ipython span.cm-attribute{c}.cm-s-ipython span.cm-header{color:blue}.cm-s-ipython span.cm-quote{90}.cm-s-ipython span.cm-link{c}.cm-s-ipython span.cm-error{color:#f00}.cm-s-ipython span.cm-tab{background:url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADAAAAAMCAYAAAAkuj5RAAAAAXNSR0IArs4c6QAAAGFJREFUSMft1LsRQFAQheHPowAKoACx3IgEKtaEHujDjORSgWTH/ZOdnZOcM/sgk/kFFWY0qV8foQwS4MKBCS3qR6ixBJvElOobYAtivseIE120FaowJPN75GMu8j/LfMwNjh4HUpwg4LUAAAAASUVORK5CYII=);background-position:right;background-repeat:no-repeat}div.output_wrapper{position:relative;display:-webkit-box;-webkit-box-orient:vertical;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:vertical;-moz-box-align:stretch;display:box;box-orient:vertical;box-align:stretch;display:flex;flex-direction:column;align-items:stretch}div.output_scroll{height:24em;width:100%;overflow:auto;border-radius:2px;-webkit-box-shadow:inset 0 2px 8px rgba(0,0,0,0.8);box-shadow:inset 0 2px 8px rgba(0,0,0,0.8);display:block}div.output_collapsed{margin:0;padding:0;display:-webkit-box;-webkit-box-orient:vertical;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:vertical;-moz-box-align:stretch;display:box;box-orient:vertical;box-align:stretch;display:flex;flex-direction:column;align-items:stretch}div.out_prompt_overlay{height:100%;padding:0 .4em;position:absolute;border-radius:2px}div.out_prompt_overlay:hover{-webkit-box-shadow:inset 0 0 1px #000;box-shadow:inset 0 0 1px #000;background:rgba(240,240,240,0.5)}div.output_prompt{color:darkred}div.output_area{padding:0;page-break-inside:avoid;display:-webkit-box;-webkit-box-orient:horizontal;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:horizontal;-moz-box-align:stretch;display:box;box-orient:horizontal;box-align:stretch;display:flex;flex-direction:row;align-items:stretch}div.output_area .MathJax_Display{text-align:left !important}div.output_area div.output_area .output{display:-webkit-box;-webkit-box-orient:vertical;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:vertical;-moz-box-align:stretch;display:box;box-orient:vertical;box-align:stretch;display:flex;flex-direction:column;align-items:stretch}@media (max-width:540px){div.output_area{display:-webkit-box;-webkit-box-orient:vertical;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:vertical;-moz-box-align:stretch;display:box;box-orient:vertical;box-align:stretch;display:flex;flex-direction:column;align-items:stretch}}div.output_area pre{margin:0;padding:0;border:0;vertical-align:baseline;color:black;background-color:transparent;border-radius:0}div.output_subarea{padding:.4em;-webkit-box-flex:1;-moz-box-flex:1;box-flex:1;flex:1}div.output_text{text-align:left;line-height:1.21429em}div.output_stderr{background:#fdd}div.output_latex{text-align:left}div.output_javascript:empty{padding:0}.js-error{color:darkred}div.raw_input_container{font-family:monospace;padding-top:5px}input.raw_input{font-family:inherit;font-size:inherit;color:inherit;width:auto;vertical-align:baseline;padding:0 .25em;margin:0 .25em}input.raw_input:focus{box-shadow:none}p.p-space{margin-bottom:10px}div.output_unrecognized{padding:5px;font-weight:bold;color:red}div.output_unrecognized a{color:inherit;text-decoration:none}div.output_unrecognized a:hover{color:inherit;text-decoration:none}.rendered_html{}.rendered_html :link{text-decoration:underline}.rendered_html :visited{text-decoration:underline}.rendered_html h1:first-child{margin-top:.538em}.rendered_html h2:first-child{margin-top:.636em}.rendered_html h3:first-child{margin-top:.777em}.rendered_html h4:first-child{margin-top:1em}.rendered_html h5:first-child{margin-top:1em}.rendered_html h6:first-child{margin-top:1em}.rendered_html *+ul{margin-top:1em}.rendered_html *+ol{margin-top:1em}.rendered_html *+table{margin-top:1em}.rendered_html *+p{margin-top:1em}.rendered_html *+img{margin-top:1em}div.text_cell{display:-webkit-box;-webkit-box-orient:horizontal;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:horizontal;-moz-box-align:stretch;display:box;box-orient:horizontal;box-align:stretch;display:flex;flex-direction:row;align-items:stretch}@media (max-width:540px){div.text_cell>div.prompt{display:none}}div.text_cell_render{outline:none;resize:none;width:inherit;border-style:none;padding:.5em .5em .5em .4em;box-sizing:border-box;-moz-box-sizing:border-box;-webkit-box-sizing:border-box}a.anchor-link:link{text-decoration:none;padding:0 20px;visibility:hidden}h1:hover .anchor-link,h2:hover .anchor-link,h3:hover .anchor-link,h4:hover .anchor-link,h5:hover .anchor-link,h6:hover .anchor-link{visibility:visible}.text_cell.rendered .input_area{display:none}.text_cell.unrendered .text_cell_render{display:none}.cm-header-1,.cm-header-2,.cm-header-3,.cm-header-4,.cm-header-5,.cm-header-6{font-weight:bold;font-family:\"Helvetica Neue\",Helvetica,Arial,sans-serif}.cm-header-1{font-size:185.7%}.cm-header-2{font-size:157.1%}.cm-header-3{font-size:128.6%}.cm-header-4{font-size:110%}.cm-header-5{font-size:100%;font-style:italic}.cm-header-6{font-size:100%;font-style:italic}.widget-interact>div,.widget-interact>input{padding:2.5px}.widget-area{page-break-inside:avoid;display:-webkit-box;-webkit-box-orient:horizontal;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:horizontal;-moz-box-align:stretch;display:box;box-orient:horizontal;box-align:stretch;display:flex;flex-direction:row;align-items:stretch}.widget-area .widget-subarea{padding:.44em .4em .4em 1px;margin-left:6px;box-sizing:border-box;-moz-box-sizing:border-box;-webkit-box-sizing:border-box;display:-webkit-box;-webkit-box-orient:vertical;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:vertical;-moz-box-align:stretch;display:box;box-orient:vertical;box-align:stretch;display:flex;flex-direction:column;align-items:stretch;-webkit-box-flex:2;-moz-box-flex:2;box-flex:2;flex:2;-webkit-box-align:start;-moz-box-align:start;box-align:start;align-items:flex-start}.widget-area.connection-problems .prompt:after{content:\"\\f127\";font-family:'FontAwesome';color:#d9534f;font-size:14px;top:3px;padding:3px}.slide-track{border:1px solid #ccc;background:#fff;border-radius:2px}.widget-hslider{padding-left:8px;padding-right:2px;overflow:visible;width:350px;height:5px;max-height:5px;margin-top:13px;margin-bottom:10px;border:1px solid #ccc;background:#fff;border-radius:2px;display:-webkit-box;-webkit-box-orient:horizontal;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:horizontal;-moz-box-align:stretch;display:box;box-orient:horizontal;box-align:stretch;display:flex;flex-direction:row;align-items:stretch}.widget-hslider .ui-slider{border:0;background:none;display:-webkit-box;-webkit-box-orient:horizontal;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:horizontal;-moz-box-align:stretch;display:box;box-orient:horizontal;box-align:stretch;display:flex;flex-direction:row;align-items:stretch;-webkit-box-flex:1;-moz-box-flex:1;box-flex:1;flex:1}.widget-hslider .ui-slider .ui-slider-handle{width:12px;height:28px;margin-top:-8px;border-radius:2px}.widget-hslider .ui-slider .ui-slider-range{height:12px;margin-top:-4px;background:#eee}.widget-vslider{padding-bottom:5px;overflow:visible;width:5px;max-width:5px;height:250px;margin-left:12px;border:1px solid #ccc;background:#fff;border-radius:2px;display:-webkit-box;-webkit-box-orient:vertical;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:vertical;-moz-box-align:stretch;display:box;box-orient:vertical;box-align:stretch;display:flex;flex-direction:column;align-items:stretch}.widget-vslider .ui-slider{border:0;background:none;margin-left:-4px;margin-top:5px;display:-webkit-box;-webkit-box-orient:vertical;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:vertical;-moz-box-align:stretch;display:box;box-orient:vertical;box-align:stretch;display:flex;flex-direction:column;align-items:stretch;-webkit-box-flex:1;-moz-box-flex:1;box-flex:1;flex:1}.widget-vslider .ui-slider .ui-slider-handle{width:28px;height:12px;margin-left:-9px;border-radius:2px}.widget-vslider .ui-slider .ui-slider-range{width:12px;margin-left:-1px;background:#eee}.widget-text{width:350px;margin:0}.widget-listbox{width:350px;margin-bottom:0}.widget-numeric-text{width:150px;margin:0}.widget-progress{margin-top:6px;min-width:350px}.widget-progress .progress-bar{-webkit-transition:none;-moz-transition:none;-ms-transition:none;-o-transition:none;transition:none}.widget-combo-btn{min-width:125px}.widget_item .dropdown-menu li a{color:inherit}.widget-hbox{display:-webkit-box;-webkit-box-orient:horizontal;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:horizontal;-moz-box-align:stretch;display:box;box-orient:horizontal;box-align:stretch;display:flex;flex-direction:row;align-items:stretch}.widget-hbox input[type=\"checkbox\"]{margin-top:9px;margin-bottom:10px}.widget-hbox .widget-label{min-width:10ex;padding-right:8px;padding-top:5px;text-align:right;vertical-align:text-top}.widget-hbox .widget-readout{padding-left:8px;padding-top:5px;text-align:left;vertical-align:text-top}.widget-vbox{display:-webkit-box;-webkit-box-orient:vertical;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:vertical;-moz-box-align:stretch;display:box;box-orient:vertical;box-align:stretch;display:flex;flex-direction:column;align-items:stretch}.widget-vbox .widget-label{padding-bottom:5px;text-align:center;vertical-align:text-bottom}.widget-vbox .widget-readout{padding-top:5px;text-align:center;vertical-align:text-top}.widget-box{box-sizing:border-box;-moz-box-sizing:border-box;-webkit-box-sizing:border-box;-webkit-box-align:start;-moz-box-align:start;box-align:start;align-items:flex-start}.widget-radio-box{display:-webkit-box;-webkit-box-orient:vertical;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:vertical;-moz-box-align:stretch;display:box;box-orient:vertical;box-align:stretch;display:flex;flex-direction:column;align-items:stretch;box-sizing:border-box;-moz-box-sizing:border-box;-webkit-box-sizing:border-box;padding-top:4px}.widget-radio-box label{margin-top:0}.widget-radio{margin-left:20px} .highlight .hll { background-color: #ffffcc } .highlight { background: #f8f8f8; } .highlight .c { color: #408080; font-style: italic } /* Comment */ .highlight .err { border: 1px solid #FF0000 } /* Error */ .highlight .k { color: #008000; font-weight: bold } /* Keyword */ .highlight .o { color: #666666 } /* Operator */ .highlight .cm { color: #408080; font-style: italic } /* Comment.Multiline */ .highlight .cp { color: #BC7A00 } /* Comment.Preproc */ .highlight .c1 { color: #408080; font-style: italic } /* Comment.Single */ .highlight .cs { color: #408080; font-style: italic } /* Comment.Special */ .highlight .gd { color: #A00000 } /* Generic.Deleted */ .highlight .ge { font-style: italic } /* Generic.Emph */ .highlight .gr { color: #FF0000 } /* Generic.Error */ .highlight .gh { color: #000080; font-weight: bold } /* Generic.Heading */ .highlight .gi { color: #00A000 } /* Generic.Inserted */ .highlight .go { color: #888888 } /* Generic.Output */ .highlight .gp { color: #000080; font-weight: bold } /* Generic.Prompt */ .highlight .gs { font-weight: bold } /* Generic.Strong */ .highlight .gu { color: #800080; font-weight: bold } /* Generic.Subheading */ .highlight .gt { color: #0044DD } /* Generic.Traceback */ .highlight .kc { color: #008000; font-weight: bold } /* Keyword.Constant */ .highlight .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */ .highlight .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */ .highlight .kp { color: #008000 } /* Keyword.Pseudo */ .highlight .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */ .highlight .kt { color: #B00040 } /* Keyword.Type */ .highlight .m { color: #666666 } /* Literal.Number */ .highlight .s { color: #BA2121 } /* Literal.String */ .highlight .na { color: #7D9029 } /* Name.Attribute */ .highlight .nb { color: #008000 } /* Name.Builtin */ .highlight .nc { color: #0000FF; font-weight: bold } /* Name.Class */ .highlight .no { color: #880000 } /* Name.Constant */ .highlight .nd { color: #AA22FF } /* Name.Decorator */ .highlight .ni { color: #999999; font-weight: bold } /* Name.Entity */ .highlight .ne { color: #D2413A; font-weight: bold } /* Name.Exception */ .highlight .nf { color: #0000FF } /* Name.Function */ .highlight .nl { color: #A0A000 } /* Name.Label */ .highlight .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */ .highlight .nt { color: #008000; font-weight: bold } /* Name.Tag */ .highlight .nv { color: #19177C } /* Name.Variable */ .highlight .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */ .highlight .w { color: #bbbbbb } /* Text.Whitespace */ .highlight .mb { color: #666666 } /* Literal.Number.Bin */ .highlight .mf { color: #666666 } /* Literal.Number.Float */ .highlight .mh { color: #666666 } /* Literal.Number.Hex */ .highlight .mi { color: #666666 } /* Literal.Number.Integer */ .highlight .mo { color: #666666 } /* Literal.Number.Oct */ .highlight .sb { color: #BA2121 } /* Literal.String.Backtick */ .highlight .sc { color: #BA2121 } /* Literal.String.Char */ .highlight .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */ .highlight .s2 { color: #BA2121 } /* Literal.String.Double */ .highlight .se { color: #BB6622; font-weight: bold } /* Literal.String.Escape */ .highlight .sh { color: #BA2121 } /* Literal.String.Heredoc */ .highlight .si { color: #BB6688; font-weight: bold } /* Literal.String.Interpol */ .highlight .sx { color: #008000 } /* Literal.String.Other */ .highlight .sr { color: #BB6688 } /* Literal.String.Regex */ .highlight .s1 { color: #BA2121 } /* Literal.String.Single */ .highlight .ss { color: #19177C } /* Literal.String.Symbol */ .highlight .bp { color: #008000 } /* Name.Builtin.Pseudo */ .highlight .vc { color: #19177C } /* Name.Variable.Class */ .highlight .vg { color: #19177C } /* Name.Variable.Global */ .highlight .vi { color: #19177C } /* Name.Variable.Instance */ .highlight .il { color: #666666 } /* Literal.Number.Integer.Long */ Bayesian Risk $$\\\\[2pt]$$ Igor Marfin [Unister Gmb@2014] < igor.marfin@unister.de > $$\\\\[40pt]$$ Table of Contents 0.1 Abstract 0.2 Initialization 0.3 Utility Theory 0.3.1 Utility functions 0.3.2 Example on the application of the Utility and Decision theories: \"Play Game or Die ...\" 0.3.3 Loss functions 0.4 Decision Theory in formulas 0.4.1 Frequentists risk function 0.4.1.1 Example of the frequentists risk function for the squared-error loss 0.4.2 Bayesian risk 0.5 Example: Dealing with the Bayesian (Bayes) Risk on the show The Price is Right 0.5.1 Get and Clean data 0.5.2 The Showcase Model 0.5.3 The machinery to make decisions 0.5.4 The Decisions Making machinery with the Bayes rule 0.6 Machine Learning with the Bayes Risk 0.6.1 An Example: Financial prediction 0.6.2 Get the Financial data from Google: stock prices of Apple Inc. 0.6.3 Ordinary Least Square Fit on the Apple stock prices 0.6.4 Decision Maker with the Bayes rule to predict prices of the Apple stock 0.6.5 An example: Observing Dark World $$\\\\[10pt]$$ 0.1 Abstract Cameron Davidson-Pilon, \"Probabilistic Programming and Bayesian Methods for Hackers\": Would you rather lose an arm or a leg? The chapter 5 of the Cameron's book has given me the motivation of this work. I am going to implement the Machinery of the decision making which uses the Bayes risk. Basic examples were taken from the chapter 5 of the book. Statisticians can be a sour bunch. Instead of considering their winnings, they only measure how much they have lost. In fact, they consider their wins as negative losses. But what's interesting is how they measure their losses. Our possible (or real) losses (positive or negative) force us to take the certain decisions which avoid to experience the same losses in future. We choose only one decision depending on our experience and beliefs, and these decision is advised us by the Decision theory ( DT ) (D.WARNERNORTH, 1968) . This theory provides a rational framework for choosing between alternative courses of action when the consequences resulting from this choice are imperfectly known. A decision causes something to happen, something that has different consequences depending on what the true state of nature turns out to be. For example, one has to make decisions about the design of the detector, about how much time to spend on different activities, or when to publish a result. On the other hand, estimating a parameter value is not a decision, since we cannot decide what value a parameter will have. There are two streams of thought which serve in the DT as the foundations: the utility theory, inductive use of the probability theory. We are going to consider both and make a framework based on the DT to make decisions. More details can be found at https://bitbucket.org/iggy_floyd/bayesian-pymc-risk-management . $$\\\\[5pt]$$ 0.2 Initialization To set up the python environment for the data analysis and make a nicer style of the notebook, one can run the following commands in the beginning of our modeling: In [1]: import sys sys . path = [ '/usr/local/lib/python2.7/dist-packages' ] + sys . path # to fix the problem with numpy: this replaces 1.6 version by 1.9 % matplotlib inline % pylab inline ion () import os import matplotlib import numpy as np import matplotlib.pyplot as pl import matplotlib as mpl import logging import pymc as pm # a plotter and dataframe modules import seaborn as sns # seaborn to make a nice plots of the data import pandas as pd import scipy.stats as stats # Set up logging. logger = logging . getLogger () logger . setLevel ( logging . INFO ) from book_format import load_style , figsize , set_figsize load_style () Populating the interactive namespace from numpy and matplotlib Out[1]: @import url('http://fonts.googleapis.com/css?family=Source+Code+Pro'); @import url('http://fonts.googleapis.com/css?family=Vollkorn'); @import url('http://fonts.googleapis.com/css?family=Arimo'); div.cell{ width: 1200px; margin-left: 0% !important; margin-right: auto; } div.text_cell code { background: transparent; color: #000000; font-weight: 600; font-size: 11pt; font-style: bold; font-family: 'Source Code Pro', Consolas, monocco, monospace; } h1 { font-family: 'Open sans',verdana,arial,sans-serif; } div.input_area { background: #F6F6F9; border: 1px solid #586e75; } .text_cell_render h1 { font-weight: 200; font-size: 30pt; line-height: 100%; color:#c76c0c; margin-bottom: 0.5em; margin-top: 1em; display: block; white-space: wrap; } h2 { font-family: 'Open sans',verdana,arial,sans-serif; } .text_cell_render h2 { font-weight: 200; font-size: 16pt; font-style: italic; line-height: 100%; color:#c76c0c; margin-bottom: 0.5em; margin-top: 1.5em; display: inline; white-space: wrap; } h3 { font-family: 'Open sans',verdana,arial,sans-serif; } .text_cell_render h3 { font-weight: 200; font-size: 14pt; line-height: 100%; color:#d77c0c; margin-bottom: 0.5em; margin-top: 2em; display: block; white-space: nowrap; } h4 { font-family: 'Open sans',verdana,arial,sans-serif; } .text_cell_render h4 { font-weight: 100; font-size: 14pt; color:#d77c0c; margin-bottom: 0.5em; margin-top: 0.5em; display: block; white-space: nowrap; } h5 { font-family: 'Open sans',verdana,arial,sans-serif; } .text_cell_render h5 { font-weight: 200; font-style: normal; color: #1d3b84; font-size: 16pt; margin-bottom: 0em; margin-top: 0.5em; display: block; white-space: nowrap; } div.text_cell_render{ font-family: 'Arimo',verdana,arial,sans-serif; line-height: 125%; font-size: 120%; text-align:justify; text-justify:inter-word; } div.output_subarea.output_text.output_pyout { overflow-x: auto; overflow-y: scroll; max-height: 50000px; } div.output_subarea.output_stream.output_stdout.output_text { overflow-x: auto; overflow-y: scroll; max-height: 50000px; } div.output_wrapper{ margin-top:0.2em; margin-bottom:0.2em; } code{ font-size: 70%; } .rendered_html code{ background-color: transparent; } ul{ margin: 2em; } ul li{ padding-left: 0.5em; margin-bottom: 0.5em; margin-top: 0.5em; } ul li li{ padding-left: 0.2em; margin-bottom: 0.2em; margin-top: 0.2em; } ol{ margin: 2em; } ol li{ padding-left: 0.5em; margin-bottom: 0.5em; margin-top: 0.5em; } ul li{ padding-left: 0.5em; margin-bottom: 0.5em; margin-top: 0.2em; } a:link{ font-weight: bold; color:#447adb; } a:visited{ font-weight: bold; color: #1d3b84; } a:hover{ font-weight: bold; color: #1d3b84; } a:focus{ font-weight: bold; color:#447adb; } a:active{ font-weight: bold; color:#447adb; } .rendered_html :link { text-decoration: underline; } .rendered_html :hover { text-decoration: none; } .rendered_html :visited { text-decoration: none; } .rendered_html :focus { text-decoration: none; } .rendered_html :active { text-decoration: none; } .warning{ color: rgb( 240, 20, 20 ) } hr { color: #f3f3f3; background-color: #f3f3f3; height: 1px; } blockquote{ display:block; background: #fcfcfc; border-left: 5px solid #c76c0c; font-family: 'Open sans',verdana,arial,sans-serif; width:1000px; padding: 10px 10px 10px 10px; text-align:justify; text-justify:inter-word; } blockquote p { margin-bottom: 0; line-height: 125%; font-size: 100%; } MathJax.Hub.Config({ TeX: { extensions: [\"AMSmath.js\"] }, tex2jax: { inlineMath: [ ['$','$'], [\"\\\\(\",\"\\\\)\"] ], displayMath: [ ['$$','$$'], [\"\\\\[\",\"\\\\]\"] ] }, displayAlign: 'center', // Change this to 'center' to center equations. \"HTML-CSS\": { scale:100, availableFonts: [\"Neo-Euler\"], preferredFont: \"Neo-Euler\", webFont: \"Neo-Euler\", styles: {'.MathJax_Display': {\"margin\": 4}} } }); Then we load notebook extensions which provide us useful functionality. In [2]: %% javascript IPython . load_extensions ( \"calico-spell-check\" , \"calico-document-tools\" , \"calico-cell-tools\" ); <IPython.core.display.Javascript object> $$\\\\[5pt]$$ 0.3 Utility Theory The first stage in setting up a structure for decision making is to assign numerical values to the possible outcomes. This task falls within the area covered by the modern theory of utility (D.WARNERNORTH, 1968) . Utility theory can operate with two types of the functions assigning a numerical weight to a decision: utility functions loss functions First, we consider the utility functions. $$\\\\[5pt]$$ 0.3.1 Utility functions I want to quote a few statements of D. WARNER NORTH from his article (D.WARNERNORTH, 1968) , perfectly explaining the utility functions u(A) , where A is a particular decision. Before we can introduce utility functions, we need to formulate four assumptions. Following the ideas of the D. WARNER NORTH, here are they: The first assumption D. WARNER NORTH: \"The first and perhaps the biggest assumption to be made is that any two possible outcomes resulting from a decision can be compared. Given any two possible out- comes or prizes, you can say which you prefer. In some cases you might say that they were equally desirable or undesirable, and therefore you are indifferent.\" D. WARNER NORTH: \"A reasonable extension of the existence of your preference among outcomes is that the preference be transitive; if you prefer A to B and B to C, then it follows that you prefer A to C.\" The second assumption D. WARNER NORTH: \"The second assumption, originated by von Neumann and Morgenstern, forms the core of modern utility theory: you can assign preferences in the same mariner to lotteries involving prizes as you can to the prizes them selves.\" What does it mean? D. WARNER NORTH: \"Imagine a pointer that spins in the center of a circle divided into two regions, as shown in Fig. 3. If you spin the pointer and it lands in region I, you get prize A; if it lands in region II, you get prize B. We shall assume that the pointer is spin in such a way that, when it stops, it is equally likely to be pointing in any given direction. The fraction of the circumference of the circle in region I will be denoted P, and that in region II as 1 - P. Then from the assumption that all directions are equally likely, the probability that the lottery gives you prize A is P, and the probability that you get prize B is 1 - P. We shall denote such a lottery as (P,A;1 - P,B) and represent it, by Fig. 4.\" The third assumption D. WARNER NORTH: \"Then it would seem natural for you to prefer prize A to the lottery, (P,A;1 - P,B), between prize A and prize B, and to prefer this lottery between prize A and prize B to prize B for all probabilities P between 0 and 1.\" The third assumption D. WARNER NORTH: \"Fourth, we make a continuity assumption. Consider three prizes, A, B, arid C. You prefer A to C, and C to B (and, as we have pointed out, you will therefore prefer A to B). We shall assert that there must, exist some probability P so that yrou are indifferent, to receiving prize C or the lottery (P,A;1 - P,B) between A and B. C is called the certain equivalent of the lottery (P,A;1 - P,B),and on the strength of our \"no fun in gambling\" assumption, we assume that inter-changing C and the lottery (P,A;1 - P,B) as prizes in some compound lottery does not change your evaluation of the latter lottery. We have not assumed that, given a lottery (P,A;1 - P,B), there exists a Prize C intermediate in value between A and B so that you are indifferent between C and (P,A;1 - P,B). Instead we have assumed the existence of the probability P.\" Under these assumptions, there is a concise mathematical representation possible for preferences: a utility function u( ) that assigns a number to each lottery or prize. This utility function has the following properties: u(A) > u(B), if and only if A > B (1) if C ~ (P,A;1 -P,B), then u(C) = P u(A) + (1 - P) u(B) (2) It is important to remember that Utility function is a merit of the profit from A,B,C decisions or the lottery between A and B, (P,A;1 - P,B), and the decision C. A,B and C decisions are associated with money, for example, \"you have the following preference among decisions, A>C>B. This might be that the decisions A brings you 100 Euro, but decision C gives you nothing, i.e. 0 Euro, or even worser with the decision B you loose 100 Euro, i.e -100Euro\" Imagine, that for each decision $D$, we have a list of possible outcomes $A_i$ with probabilities $P_i$, i.e. $(A_1,P_1)$,$(A_2,P_2)$,...., $(A_n,P_n)$, where $\\sum_i P_i = 1$. Then the decision the price of the decision $D$ will be $u(price_D)$ = $\\sum_i P_i \\times u(A_i)$, i.e. we are indifferent to get $price_D$ Euro and from somebody or to make our decision. If somebody propose less money, we make our decision $D$, otherwise we take money, if their amount more than $price_D$. OK, then, imagine that we have a list of decisions $\\{D&#94;j\\}$ with the corresponding set $(A_i&#94;j,P_i&#94;j)$. We construct the prices of decisions, $u(price&#94;{D&#94;j})$ = $\\sum_i P_i&#94;j \\times u(A_i&#94;j)$, and find out the decision with maximal price: $$price&#94;{D&#94;{max}} \\sim argmax(\\{u(price&#94;{D&#94;j})\\}).$$ These $D&#94;{max}$ is out preferable decision. The utility function is widely discussed in the Internet. The particular interesting functions are Risk aversion functions . A person is given the choice between two scenarios, one with a guaranteed payoff and one without. In the guaranteed scenario, the person receives 50 Euro. In the uncertain scenario, a coin is flipped to decide whether the person receives 100 Euro or nothing. The expected payoff for both scenarios is 50 Euro, meaning that an individual who was insensitive to risk would not care whether they took the guaranteed payment or the gamble. However, individuals may have different risk attitudes A person is said to be: risk-averse (or risk-avoiding) - if he or she would accept a certain payment (certainty equivalent) of less than 50 Euro (for example, 40 Euro), rather than taking the gamble and possibly receiving nothing. risk-neutral - if he or she is indifferent between the bet and a certain 50 Euro payment. risk-loving (or risk-seeking) - if he or she would accept the bet even when the guaranteed payment is more than 50 Euro (for example, 60 Euro). Following prescriptions, given in the wiki page (WikimediaFoundation, 2015) , I have created the package Utility_functions , which has codes of: constant absolute risk aversion (CARA) utility functions constant relative risk aversion (CRRA) utility functions In [3]: # let's make a few plots of the provided functions import Utility_functions as uf % cd Utility_functions % run plot.py % cd - /home/debian/debian-package/netbeans/bayesian-pymc-risk-management/Utility_functions theta 0.5 theta 0.75 theta 1.1 theta 1.5 theta 1.75 theta 2.0 theta 3.0 /home/debian/debian-package/netbeans/bayesian-pymc-risk-management $$\\\\[5pt]$$ 0.3.2 Example on the application of the Utility and Decision theories: \"Play a Game or Die ...\" How do we use the utility functions and decision theory on practice? Let's imagine the situation. You have been caught by rebels. They have given you two choices: you play the game you die You have decided not to die (I hope so), agreed to play the game. The game had the following rules: Rule-1 : Somebody from rebels falls the coin down. If the coin lands on the head side, you get 100 Euro, if the coin flips down and lands on the tail side, you pay 30 Euro Rule-2 : you can avoid playing the game, if you pay 10 Euro. You decide to build the utility function which you will use to make a decision. The utility function is quite simple: 1) make a normalization of the price of any outcome A price_of_outcome(A) = max(-100.,min(100,price_of_outcome)) 2) utility function is defined as U(A) = price_of_outcome(A)/(100.) such that the function takes values in the range [-1,1] Then after looking at the plots of the CARA utility functions, you decide to use cara_utility for the positive price of the outcome or -cara_utility for the negative price of the outcome to program the steps 1) and 2). Also you decide naively that the probabilities of the flipping on each side are the same and equal to P=0.5. Then you write your simple code which will make a decision: In [4]: ''' Igor Marfin @2014 Unister GmbH <igor.marfin@unister.de> Basics of the Decision Maker ''' import scipy.optimize as optimize # define the utility function and its parameters nothing = 0.0 # nothing is '0' Euro ''' we want the following: 1)at 90 Euro, the utility value should be close to 0.90 2)at 80 Euro, it should be close to 0.80 ... 100) at 1 Euro, it should be close to 0.01 We choose two edge points, 1) and 100), to estimate the lowest and highest values of the theta parameter. We find them solving two equations: uf.cara_utility(90.,x,nothing) -.90 == 0 uf.cara_utility(1.,x,nothing) -0.01 == 0 To do that, we use the scipy.optimize.brentq described here http://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.brentq.html ''' theta_value_high = lambda x : uf . cara_utility ( 90. , x , nothing ) -. 90 theta_value_low = lambda x : uf . cara_utility ( 1. , x , nothing ) - 0.01 # find the theta parameter values w theta_low = optimize . brentq ( theta_value_high , 0 , 1 ) theta_high = optimize . brentq ( theta_value_low , 0 , 1 ) if ( theta_low > theta_high ): theta_low , theta_high = theta_high , theta_low theta_our_choice = np . random . uniform ( theta_low , theta_high ) print \"the optimal value of theta parameter:{}\" . format ( theta_our_choice ) # probability of the flipping coins to land at some side P = 1. / 2. proba = lambda x : x # define the utility function of our choices utility_choice = lambda x : cara_utility ( x , theta_our_choice , nothing ) if x > 0. else - cara_utility ( abs ( x ), theta_our_choice , nothing ) # define outcomes outcomes = { \"win\" : 100. , \"loss\" : - 30. , \"refuse_to_play\" : - 10. } # probabilities of each outcome probas = { \"win\" : proba ( P ), \"loss\" : 1. - proba ( P ), \"refuse_to_play\" : 1.0 } def utility_decisions ( outcomes , probas ): ''' returns the utility function of each decisions ''' decisions = [] # a decision 1) decisions += [ utility_choice ( outcomes [ \"win\" ]) * probas [ \"win\" ] + utility_choice ( outcomes [ \"loss\" ]) * probas [ \"loss\" ] ] # a decision 2) decisions += [ utility_choice ( outcomes [ \"refuse_to_play\" ]) * probas [ \"refuse_to_play\" ]] return decisions decisions_utility_costs = utility_decisions ( outcomes , probas ) print \"What are the utility cost of our decisions:\" , decisions_utility_costs # transform costs to Euro and choose the decision with the highest price what_price_inEuro = lambda x , y : utility_choice ( x ) - y decisions_euro_costs = map ( lambda x : optimize . brentq ( what_price_inEuro , - 100. , 100. , args = ( x )) , decisions_utility_costs ) # a result print \"What money will cost (in Euro) the decisions to us? : \" , decisions_euro_costs print \"We decide to %s \" % 'play the game' if argmax ( decisions_euro_costs ) == 0 else 'not play the game' the optimal value of theta parameter:0.0170162318878 What are the utility cost of our decisions: [0.20890799997685033, -0.15647211501626335] What money will cost (in Euro) the decisions to us? : [13.771615891446064, -10.000000000000004] We decide to play the game $$\\\\[5pt]$$ Definitely, we have to make the first decision. $$\\\\[45pt]$$ Now, imagine, that we don't know exactly the probability for the coin flipping side. How do we react in this case? We need to update our machinery in such way that our decision making will be a function of the unknown probability: In [5]: ''' Igor Marfin @2014 Unister GmbH <igor.marfin@unister.de> Update (1) on the Decision Maker ''' # probabilities of each outcome probas = lambda x : { \"win\" : proba ( x ), \"loss\" : 1. - proba ( x ), \"refuse_to_play\" : 1.0 } proba_val = np . linspace ( 0. , 1. , 75 ) # probability of coin flipping can equaly take 75 values in the range [0,1] # cost of the first decision decision1_utility_costs = [ utility_decisions ( outcomes , probas ( x ))[ 0 ] for x in proba_val ] decision2_utility_costs = [ utility_decisions ( outcomes , probas ( x ))[ 1 ] for x in proba_val ] decision1_euro_costs = map ( lambda x : optimize . brentq ( what_price_inEuro , - 100. , 100. , args = ( x )) , decision1_utility_costs ) decision2_euro_costs = map ( lambda x : optimize . brentq ( what_price_inEuro , - 100. , 100. , args = ( x )) , decision2_utility_costs ) plt . plot ( proba_val , decision1_euro_costs , label = \"Cost of the first decisions in Euro\" , lw = 3 ) plt . hlines ( decision2_euro_costs [ 0 ], 0 , 1. , linestyles = \"--\" , label = 'the cost of the 2nd decision' ) plt . xlabel ( \"proba\" ) plt . ylabel ( \"cost of the decision\" ) plt . xlim ( - 0.04 , 1.1 ) plt . ylim ( - 100. , 100. ) plt . legend () plt . title ( \"Costs of the decisions at different probabilities of the coin flippings\" ); In [6]: # let's find the probability when we want to make the second decision utility_value_decision1 = lambda x : utility_decisions ( outcomes , probas ( x ))[ 0 ] utility_value_decision2 = lambda x : utility_decisions ( outcomes , probas ( x ))[ 1 ] # find the probability when utility_value_decision1 == utility_value_decision2 probability_value = lambda x : utility_value_decision1 ( x ) - utility_value_decision2 ( x ) # our decision maker print \"We play the game only if flipping probability to fall to a head > %3.4f \" % optimize . brentq ( probability_value , 0. , 1. ,) print \"otherwise, we don't play the game\" We play the game only if flipping probability to fall to a head > 0.1999 otherwise, we don't play the game $$\\\\[5pt]$$ Let's imagine that you have asked the rebels to flip $N$ times the coin, they have agreed and you have calculated the number of the head landings $m$. Then you update you machinery in the way: In [7]: ''' Igor Marfin @2014 Unister GmbH <igor.marfin@unister.de> Update (2) on Decision Maker Machinery ''' def decide ( limit_proba = optimize . brentq ( probability_value , 0. , 1. ,), N = 1. , m = 0. ): ''' make a decision ''' priory_proba = float ( m ) / float ( N ) if N > 0. else 0. return 'play the game' if priory_proba > limit_proba else 'not play the game' If you have got from rebels $N=21$ and $m=5$, you must have decided... In [8]: decide ( N = 21 , m = 5 ) Out[8]: 'play the game' If they have told you $N=21$ and $m=2$, you must have decided: In [9]: decide ( N = 21 , m = 2 ) Out[9]: 'not play the game' 0.3.3 Loss functions Now we introduce what statisticians and decision theorists call loss functions. Basically, there are only two features by which loss functions and utility functions differ: the meaning a number of arguments. The Meaning Utility functions u() present the profit from making the particular decision Loss functions L() present the loss from the making the particular decision. One can exchange the meaning of an utility function with the meaning of a loss function by changing the sign of returned values: $$ u(\\cdot) \\equiv -L(\\cdot) $$ A number of arguments The function u(A) takes one argument A , where A is one of the possible outcomes accepted during the decision. The function $L(\\theta,\\hat{\\theta})$ takes two arguments $\\theta,\\hat{\\theta}$: $\\theta$ is a true value of some outcome,parameter or nature phenomenon (usually unknown) $\\hat{\\theta}$ is an estimated value for this quantity. $L(\\theta,\\hat{\\theta})$ plays a role of the distance (metric) between unknown $\\theta$ and estimated $\\hat{\\theta}$. Usually, $L(\\theta,\\hat{\\theta})$ is a function of the type $L(\\theta-\\hat{\\theta})$. That means that loss function can be presented by a one-parametric function, i.e. by the utility function. Remembering the previous example of use of the utility functions $$ u(x) \\sim \\frac{x}{100.}, x=min(100.,max(-100.,x)) $$ where x was the price of outcome X, we can formulate the loss function $L(\\theta,\\hat{\\theta})$ for this case as $$L(\\theta,\\hat{\\theta})=\\frac{\\theta-\\hat{\\theta}}{100},$$ and it is obvious that $$ L(0,x) \\equiv -u(x). $$ As we said, a loss function is a function of the true parameter, and an estimate of that parameter. The important point of loss functions is that it measures how bad our current estimate is: the larger the loss, the worse the estimate is according to the loss function. A simple, and very common, example of a loss function is the squared-error loss: $$L( \\theta, \\hat{\\theta} ) = ( \\theta - \\hat{\\theta} )&#94;2$$ In [10]: ''' Igor Marfin @2014 Unister GmbH <igor.marfin@unister.de> Defintions of the Loss functions ''' def squared_error_loss ( theta_true , theta ): ''' the squared-error loss function ''' return ( theta_true - theta ) ** 2 The squared-error loss function is used in estimators like linear regression, UMVUEs and many areas of machine learning. We can also consider an asymmetric squared-error loss function, something like: $$L( \\theta, \\hat{\\theta} ) = \\begin{cases} ( \\theta - \\hat{\\theta} )&#94;2 & \\hat{\\theta} \\lt \\theta \\\\\\\\ c( \\theta - \\hat{\\theta} )&#94;2 & \\hat{\\theta} \\ge \\theta, \\;\\; 0\\lt c \\lt 1 \\end{cases}$$ In [11]: ''' Igor Marfin @2014 Unister GmbH <igor.marfin@unister.de> Defintions of the Loss functions ''' def asymmetric_squared_error_loss ( theta_true , theta , c ): ''' the asymmetric squared-error loss function ''' c = max ( 0 , min ( 1 , c )) if ( theta_true < theta ): return ( theta_true - theta ) ** 2 return c * ( theta_true - theta ) ** 2 A situation where this might be useful is in estimating web traffic for the next month, where an over-estimated outlook is preferred so as to avoid an underallocation of server resources. A negative property about the squared-error loss is that it puts a disproportionate emphasis on large outliers. This is because the loss increases quadratically, and not linearly, as the estimate moves away. A more robust loss function that increases linearly with the difference is the absolute-loss In [12]: ''' Igor Marfin @2014 Unister GmbH <igor.marfin@unister.de> Defintions of the Loss functions ''' def absolute_error_loss ( theta_true , theta ): ''' the absolute-error loss function ''' return abs ( theta_true - theta ) Other popular loss functions include: $L( \\theta, \\hat{\\theta} ) = \\mathbb{1}_{ \\hat{\\theta} \\neq \\theta }$ is the zero-one loss often used in machine learning classification algorithms. $L( \\theta, \\hat{\\theta} ) = -\\hat{\\theta}\\log( \\theta ) - (1-\\hat{ \\theta})\\log( 1 - \\theta ), \\; \\; \\hat{\\theta} \\in {0,1}, \\; \\theta \\in [0,1]$ called the log-loss, also used in machine learning. In [13]: ''' Igor Marfin @2014 Unister GmbH <igor.marfin@unister.de> Defintions of the Loss functions ''' def zero_one_loss ( theta_true , theta ): ''' the zero-one loss function ''' return 0. if theta_true == theta else 1. def log_loss ( theta_true , theta ): ''' the absolute-error loss function ''' theta_true = max ( 1e-6 , min ( 1. - 1e-6 , theta_true )) theta = max ( 1e-6 , min ( 1. - 1e-6 , theta )) return - theta * np . log ( theta_true ) - ( 1. - theta ) * np . log ( 1. - theta_true ) By shifting our focus from trying to be incredibly precise about parameter estimation to focusing on the outcomes of our parameter estimation, we can customize our estimates to be optimized for our application. This requires us to design new loss functions that reflect our goals and outcomes. Some examples of more interesting loss functions: $L( \\theta, \\hat{\\theta} ) = \\frac{ | \\theta - \\hat{\\theta} | }{ \\theta(1-\\theta) }, \\; \\; \\hat{\\theta}, \\theta \\in [0,1]$ emphasizes an estimate closer to 0 or 1 since if the true value θ is near 0 or 1, the loss will be very large unless θ&#94; is similarly close to 0 or 1. This loss function might be used by a political pundit who's job requires him or her to give confident \"Yes/No\" answers. This loss reflects that if the true parameter is close to 1 (for example, if a political outcome is very likely to occur), he or she would want to strongly agree as to not look like a skeptic. $L( \\theta, \\hat{\\theta} ) = 1 - \\exp \\left( -(\\theta - \\hat{\\theta} )&#94;2 \\right)$ is bounded between 0 and 1 and reflects that the user is indifferent to sufficiently-far-away estimates. It is similar to the zero-one loss above, but not quite as penalizing to estimates that are close to the true parameter. In [14]: ''' Igor Marfin @2014 Unister GmbH <igor.marfin@unister.de> Defintions of the Loss functions ''' def political_loss ( theta_true , theta ): ''' the political loss function ''' theta_true = max ( 1e-6 , min ( 1. - 1e-6 , theta_true )) return abs ( theta_true - theta ) / ( theta_true * ( 1. - theta_true )) def zero_one_bound_loss ( theta_true , theta ): ''' the zero-one bound loss function ''' return 1. - np . exp (( theta_true - theta ) ** 2 ) People notice one type of mistake — the failure to predict rain — more than other, false alarms. If it rains when it isn't supposed to, they curse the weatherman for ruining their picnic, whereas an unexpectedly sunny day is taken as a serendipitous bonus. [The Weather Channel's bias] is limited to slightly exaggerating the probability of rain when it is unlikely to occur — saying there is a 20 percent change when they know it is really a 5 or 10 percent chance — covering their butts in the case of an unexpected sprinkle. $$\\\\[5pt]$$ 0.4 Decision Theory in formulas Our decision-making will be based on the application of the Loss $L( \\theta, \\hat{\\theta} )$. The formulation of the DT which operates with the utilities $u(A)$ can be done in the vice-versa manner. I am going to follow notations given in the lectures of F.James at Statistics for Physis(DESY2012) (JamesF., 2012) and lectures of M. Jordan (JordanI.M., 2014) . The decision theory provides a quantification of the good selection procedure for decisions. This quantification comes from the loss function $L( \\theta, \\hat{\\theta} )$. As we see later, this good selection procedure is called the Bayes (Bayesian) rule . Decision theory deals with three different spaces: An observable space, $X$, in which all possible observations $X=(X_1,X_2,...,X_N)$ fall A parameter space contains all possible values of the parameter $\\theta$ A decision space $D$, which contains all possible decisions $d$. A decision rule $\\delta$, alternatively a decision procedure, decision function specifies what decision $d$ is to be taken given the observation $X$, that is $$ d = \\delta(X). $$ In other words, the decision rule $\\delta$ is a function taking $X$ as an argument and giving a float value $d$. Frequentists and Bayesians use the loss function differently. $$\\\\[5pt]$$ 0.4.1 Frequentists risk function In frequentist usage, the parameter $\\theta$ is fixed and thus the data are averaged over. The frequentists risk is defined as $$R(\\theta,\\delta) = E_\\theta(L(\\theta,\\delta(X))) ,$$ where $E_\\theta$ is expectation taken over the data $X$, with the parameter $\\theta$ held fixed. The expression is above can be written in the integral form: $$R(\\theta,\\delta) = E_\\theta(L(\\theta,\\delta(X))) = \\int L(\\theta,\\delta(X)) p(X|\\theta) dX, $$ where $p(X|\\theta)$ is the Likelihood (conditional probability to oberve $X$ given $\\theta$). Please, pay an attention on the fact, that $\\delta(X)$ plays a role of the estimation $\\hat{\\theta}$ for the parameter $\\theta$. The risk function is an expectation of the loss function or an average loss over all possible outcomes (observations). $$\\\\[5pt]$$ 0.4.1.1 Example of the frequentists risk function for the squared-error loss The squared-error loss function is defined as $$L( \\theta, \\hat{\\theta} ) = ( \\theta - \\hat{\\theta} )&#94;2.$$ Then the risk function is expressed as $$R(\\theta,\\delta(X)) = E_\\theta(L(\\theta,\\delta(X))) = E_\\theta( (\\theta - E_\\theta(\\delta(X)) + E_\\theta(\\delta(X)) - \\delta(X))&#94;2)=\\\\ E_\\theta((\\theta- E_\\theta(\\delta(X)))&#94;2) + E_\\theta((E_\\theta(\\delta(X)) - \\delta(X))&#94;2) $$ The risk function takes a minimum at the rule $$E_\\theta(\\delta(X)) \\equiv \\int \\delta(X)p(X|\\theta)dX \\equiv E(\\delta(X)|\\theta) = \\theta.$$$$\\\\[5pt]$$ 0.4.2 Bayesian risk In the Bayesian framework, we define the posterior risk , $$\\rho(X,\\delta(X)) = E_X(L(\\theta,\\delta(X))) = \\int L(\\theta,\\delta(X)) p(\\theta|X) d\\theta. $$ The rule $\\delta&#94;*(X)$ which minimizes the posterior risk $\\rho(X,\\delta)$ is called the Bayes action . The Bayes action for the squared-error loss function can be obtained in analogy to the minimization of the frequentists risk. In this case, the Bayes rule takes the following form: $$ d&#94;* \\equiv \\delta&#94;*(X)=E_X(\\theta) = E(\\theta|X), $$ the value of the Bayes action is an expectation of the parameter $\\theta$ given data $X$. The Bayes action is a function of $X$. Despite the tensions between frequentists and Bayesians, they occasionally steal ideas from each other. If we use the prior density $\\pi(\\theta)$ of the parameter to average the Frequentists risk, $$ r(\\delta,\\pi) = \\int R(\\theta,\\delta(X)) \\pi(\\theta)d\\theta, $$ we get the Bayes loss . The Bayes loss is an expectation of the risk function over all possible values of the parameter $\\theta$. The rule $\\delta&#94;{**}$ which minimizes the Bayes loss $r(\\delta,\\pi)$ is a called the Bayes rule . The Bayes risk is just the Bayes loss $r(\\delta&#94;{**},\\pi)$ with the Bayes rule $\\delta&#94;{**}$ plugged in. The Bayes loss can be rewritten in the way using the Bayes ratio $$ p(X|\\theta)\\pi(\\theta) = p(\\theta|X)p(X), $$$$ r(\\delta,\\pi) = \\int R(\\theta,\\delta(X)) \\pi(\\theta)d\\theta = \\int L(\\theta,\\delta(X)) p(X|\\theta)dX \\pi(\\theta)d\\theta = \\\\ \\int L(\\theta,\\delta(X)) p(\\theta|X)dX p(X)d\\theta = \\int \\rho(X,\\delta(X)) p(X) dX. $$ That means that the Bayes rule is not only the rule which minimizes the frequentists risk function, but also it minimizes the posterior risk, i.e. it is the Bayes action. The Bayes rule for the squared-error loss function is such $\\delta&#94;{**}$ was already obtained $$ \\hat{\\theta} = \\delta&#94;{**}(X) = \\delta&#94;{*}(X) = E_X(\\theta) = E(\\theta|X) $$$$\\\\[5pt]$$ In general, there is no analytical solution for the Bayes rule and Bayes risk. We have to use numerical integration, namely, the MCMC approach. We are going to develop the framework based on PyMC which will find the Bayes rule (aka the Bayes decision) (the decision when the Bayes loss is minimal) calculate the Bayes risk (the value of the Bayes loss at the Bayes decision.) for any model $M=\\{p(X|\\theta), \\pi(\\theta)\\}$ with any loss function $L(\\theta,\\hat{\\theta})$. $$\\\\[5pt]$$ 0.5 Example: Dealing with the Bayesian (Bayes) Risk on the show The Price is Right Now it is time for practical applications of the obtained theoretical knowledge. Bless you if you are ever chosen as a contestant on the Price is Right, for here we will show you how to optimize your final price on the Showcase . For those who forget the rules: Two contestants compete in The Showcase . Each contestant is shown a unique suite of prizes. After the viewing, the contestants are asked to bid on the price for their unique suite of prizes. If a bid price is over the actual price, the bid's owner is disqualified from winning. If a bid price is under the true price by less than $250, the winner is awarded both prizes. $$\\\\[5pt]$$ 0.5.1 Get and Clean data First, we want to obtain the priory knowledge about the actual prices of prize suits in this showcase. Let's try to get data from open sources. After googling for a while, I have found the retail prices of prize suites for 794 showcases in the past (www.datagrabber.org, 2015) . I have developed in my previous tutorial \"A Hierarchical Bayesian Model of the Bundes League\" a tool allowing you to scrape tables from twiki pages of the Wikipedia. I am going to use this tool to extract data from the www.datagraber.org (www.datagrabber.org, 2015) . In [15]: ''' Igor Marfin <Unister Gmb, 2014> igor.marfin@unister.de A tool to scrape data from wiki-pages ''' from bs4 import BeautifulSoup import urllib2 import warnings from StringIO import StringIO import pandas as pd warnings . filterwarnings ( 'ignore' ) class MyWikiParser ( BeautifulSoup ): ''' BeautifulSoup parser: parses <table> </table> tags''' def __init__ ( self , page , table_class_properties , table_header ): BeautifulSoup . __init__ ( self , page , fromEncoding = \"UTF-8\" ) self . recording = 0 self . data = [] self . table_class_properties = table_class_properties self . table_header = table_header def handle_data ( self , column_tag = \"td\" ): tables = self . findAll ( \"table\" , self . table_class_properties ) for table in tables : first_row = table . find ( \"tr\" ) first_cell_in_first_row = first_row . find ( column_tag ) rows = [] if ( first_cell_in_first_row is not None ) and ( self . table_header in str ( first_cell_in_first_row )): for row in table . findAll ( \"tr\" ): vals = row . find_all ( 'td' ) vals = map ( lambda x : x . text . encode ( 'utf-8' ) . split ( '!' )[ 1 ] if len ( x . text . encode ( 'utf-8' ) . split ( '!' )) > 1 else x . text . encode ( 'utf-8' ) . split ( '!' )[ 0 ], vals ) # fix some text data # rows.append([val.text.encode('utf-8').replace('\\xe2\\x95\\xb2','/').replace('\\xe2\\x80\\x93','-') for val in vals]) rows . append ([ val . replace ( ' \\xe2\\x95\\xb2 ' , '/' ) . replace ( ' \\xe2\\x80\\x93 ' , '-' ) for val in vals ]) self . data = rows class Wiki_Table ( object ): ''' exctracts values for the table from Wiki-Page files ''' def __init__ ( self , url , table_class_properties , table_header , column_tag = \"td\" ): self . table = [] header = { 'User-Agent' : 'Mozilla/5.0' } #Needed to prevent 403 error on Wikipedia req = urllib2 . Request ( url , headers = header ) page = urllib2 . urlopen ( req ) parser = MyWikiParser ( page , table_class_properties , table_header ) parser . handle_data ( column_tag ) self . table = parser . data # a page containing the data wikipage = 'http://www.datagrabber.org/welcome-to-datagrabber-org/price-is-right-showcase-cheat-prices/' # a pattern on the header to find the table among others title_pattern = '$$$' # a pattern on the class properties of the table used to find it table_props = { \"class\" : \"wp-table-reloaded wp-table-reloaded-id-3\" } # parse the table from the wiki page wt = Wiki_Table ( wikipage , table_props , title_pattern , column_tag = \"th\" ) #print wt.table # create a dataframe on the data table_csv = map ( lambda x : ';' . join ( x ), wt . table ) table_csv = ' \\n ' . join ( table_csv ) data = StringIO ( table_csv ) #print data.getvalue() df = pd . read_csv ( data , delimiter = ';' , header = None ) df . columns = [ 'Price' , 'THEME' , 'SHOWCASE 1A' , 'SHOWCASE 1B' , 'SHOWCASE 1C' ] df . head ( 3 ) df = df . convert_objects ( convert_numeric = True ) Let's plot the distribution of the retail price and get its summary, the mean and the std. deviation values: In [16]: plots_dir = \"plots/\" sns . set_context ( \"poster\" ) plt . figure ( figsize = ( 15 , 8 )) retail_price = df . loc [:, 'Price' ] retail_price . plot ( kind = 'kde' , xlim = ( 0 , 100000 ), legend = True , label = 'Retail price of the prizes in the Showcase ' ) pl . xlabel ( 'Retail' ) pl . title ( 'Retail price distribution' ) pl . legend () pl . savefig ( plots_dir + 'retail_price' + '.png' ) retail_price . describe () Out[16]: count 791.000000 mean 25662.945638 std 9323.505178 min 1100.000000 25% 19073.000000 50% 24443.000000 75% 30716.000000 max 90761.000000 Name: Price, dtype: float64 OK, from previous The Price is Right episodes, we have prior beliefs about what distribution the true price follows. For simplicity, suppose it follows a Normal: $$\\text{True Price} \\sim \\text{Normal}(\\mu_p, \\sigma_p ),$$ where $$ \\mu_p = 25663 $$ and $$ \\sigma_p = 9324 $$ For each prize in the prize suite, we have an idea of what it might cost, but this guess could differ significantly from the true price. (You can couple this with increased pressure being onstage and you can see why some bids are so wildly off). Let's suppose your beliefs about the prices of prizes also follow Normal distributions: $$\\text{Prize}_i \\sim \\text{Normal}(\\mu_i, \\sigma_i ),\\;\\; i=1,2$$ This is really why Bayesian analysis is great: we can specify what we think a fair price is through the $\\mu_i$ parameter, and express uncertainty of our guess in the $\\sigma_i$ parameter. We'll assume two prizes per suite for brevity, but this can be extended to any number. The true price $\\text{True Price}$ of the prize suite is then given by $\\text{Prize}_1 + \\text{Prize}_2 + \\epsilon$, where $\\epsilon$ is some error term. $$\\\\[45pt]$$ This is a brief model of how we should be playing the Showcase : We update $\\text{True Price}$ given we have observed both prizes and have belief distributions about them. We perform this using PyMC . The ShowcaseModel will be made. We define the machinery to make decisions based on the posterior knowledge of the $\\text{True Price}$. This machinery makes the decision It is important to understand that the machinery makes a decision about what the estimation of the total price for the prize suite we have to use in our answer. $$\\\\[45pt]$$ Lets make some values concrete. Suppose we are in the studio of the Game and there are two prizes in our prize suite: A trip to wonderful Toronto, Canada! A lovely new snowblower! We have some guesses about the true prices of these objects, but we are also pretty uncertain about them. I can express this uncertainty through the parameters of the Normals: \\begin{align} & \\text{snowblower} \\sim \\text{Normal}(3 000, 500 )\\\\\\\\ & \\text{Toronto} \\sim \\text{Normal}(12 000, 3000 )\\\\\\\\ \\end{align} For example, I believe that the true price of the trip to Toronto is 12 000 dollars, and that there is a 68.2% chance that the price falls 1 standard deviation away from this, i.e. my confidence is that, there is a 68.2% chance (the chance with 68.2% probability) when the price of the trip falls in the range [9 000, 15 000]. $$\\\\[5pt]$$ 0.5.2 The Showcase Model We can create some PyMC code to perform inference on the true price of the suite in this case. We define the Base interface for all our future bayesian models. In [17]: ''' Igor Marfin <Unister Gmb, 2014> igor.marfin@unister.de A Base Class definitions. ''' # Define a function for splitting train/test data. def split_train_test ( data , percent_test = 0.1 ): \"\"\"Split the data into train/test sets. :param int percent_test: Percentage of data to use for testing. Default 10%. \"\"\" from sklearn.cross_validation import train_test_split train , test = train_test_split ( data , test_size = percent_test ) return train , test # Define our evaluation function: RMSE - root mean squared error function def rmse ( test_data , predicted ): \"\"\" Calculate root mean squared error. Ignoring missing values in the test data. \"\"\" I = ~ np . isnan ( test_data ) # indicator for non-missing values N = I . sum () # number of non-missing values sqerror = abs ( test_data - predicted ) ** 2 # squared error array mse = sqerror [ I ] . sum () / N # mean squared error of all non-missing values return np . sqrt ( mse ) # Define the Basic class class BaseModel ( object ): \"\"\" Base Class\"\"\" def __init__ ( self , train_data ): # how many periods of the measurement we have self . num_entries = len ( train_data ) self . data = train_data self . traces = None self . predicted = None def predict ( self , train_data ): raise NotImplementedError ( 'prediction not implemented for base class' ) def prepare_trace ( self ): raise NotImplementedError ( 'preparing the traces not implemented for base class' ) def rmse ( self , test_data ): \"\"\"Calculate root mean squared error for predictions on test data.\"\"\" # rmse is a global function defined before return rmse ( test_data , self . predicted ) Our model of the Showcase True Price and price of the prize suite is the following: In [18]: ''' Igor Marfin <Unister Gmb, 2014> igor.marfin@unister.de A Model for the Showcase ''' #defines the class for the Modeling class ModelShowcase ( BaseModel ): \"\"\" A ModelShowcase class :param np.ndarray data: The data to use for learning the model. :param list price_priory: priory knowledge on the prices of the prizes in the showcase \"\"\" def __init__ ( self , data , price_priory ): logging . info ( 'building the model....' ) super ( ModelShowcase , self ) . __init__ ( data ) # a priory on 'True Price' self . mu_prior = data . describe ()[ 'Price' ][ 'mean' ] self . std_prior = data . describe ()[ 'Price' ][ 'std' ] self . true_price = pm . Normal ( \"true_price\" , self . mu_prior , 1.0 / self . std_prior ** 2 ) # a prior knowledge on the prices in the suite self . prize1 = pm . Normal ( \"prize1\" , price_priory [ 'prize1' ][ 0 ], 1.0 / price_priory [ 'prize1' ][ 1 ] ** 2 ) self . prize2 = pm . Normal ( \"prize2\" , price_priory [ 'prize2' ][ 0 ], 1.0 / price_priory [ 'prize2' ][ 1 ] ** 2 ) # determenistic total price of prizes self . price_estimate = self . prize1 + self . prize2 # Here we add the observed value via a 'Potential' Object. # This 'Potential' Object (function) plays a role of the Likelihood @pm.potential def error ( true_price = self . true_price , price_estimate = self . price_estimate ): return pm . normal_like ( true_price , price_estimate , 1 / ( 3e3 ) ** 2 ) # 3e3 is quite big std. deviation # our model collects all definitions in self.model self . model = pm . MCMC ([ self . true_price , self . prize1 , self . prize2 , error , self . price_estimate ]) logging . info ( 'done building the model' ) def _prepare_trace ( self ): ''' prepare a dataframe with traces of the parameters of our model ''' self . traces = pd . DataFrame ({ 'true_price' : self . true_price . trace (), 'prize1' : self . prize1 . trace (), 'prize2' : self . prize2 . trace (), }) return self . traces # Update the interface of our model ModelShowcase . prepare_trace = _prepare_trace # priory knowkedge about prizes: price_priory = { 'prize1' :( 3000 , 500 ), # A lovely new snowblower! 'prize2' :( 12000 , 3000 ) # A trip to wonderful Toronto, Canada! } showcase_model = ModelShowcase ( df , price_priory ) showcase_model . model . sample ( 100000 , 80000 , 20 ) #showcase_model.model.sample(100000) INFO:root:building the model.... INFO:root:done building the model [-----------------100%-----------------] 100000 of 100000 complete in 8.9 sec Let's look at the obtained posterior distribution of True Price : In [19]: ''' Igor Marfin <Unister Gmb, 2014> igor.marfin@unister.de plotting support for ModelShowcase ''' def _plot_posterior ( self ): ''' plot posterior distribution ''' figsize ( 12.5 , 4 ) import scipy.stats as stats x = np . linspace ( 5000 , 40000 ) # in the range [5000,40000] plot ( x , stats . norm . pdf ( x , self . mu_prior , self . std_prior ), c = \"k\" , lw = 2 , label = \"prior dist. of suite price (true price)\" ) traces = self . prepare_trace () _hist = plt . hist ( traces [ 'true_price' ] . values , bins = 35 , normed = True , label = \"posterior estimate of the true price\" , histtype = \"stepfilled\" ) plt . title ( \"Posterior of the true price estimate\" ) plt . vlines ( self . mu_prior , 0 , 1.1 * np . max ( _hist [ 0 ]), label = \"prior's mean\" , linestyles = \"--\" ) plt . vlines ( traces [ 'true_price' ] . mean (), 0 , 1.1 * np . max ( _hist [ 0 ]), label = \"posterior's mean\" , linestyles = \"-.\" ) plt . legend ( loc = \"upper left\" ) plots_dir = 'plots/' plt . savefig ( plots_dir + 'retail_price_posterior' + '.png' ) # Update the interface of our model ModelShowcase . plot_posterior = _plot_posterior showcase_model . plot_posterior () $$\\\\[5pt]$$ 0.5.3 The machinery to make decisions Noticing that because of our two observed prizes and subsequent guesses (including uncertainty about those guesses), we shifted our mean price estimate down about $10 000 dollars from the previous mean price. A frequentist, seeing the two prizes and having the same beliefs about their prices, would bid $\\mu_1 + \\mu_2 = 25663$, regardless of any uncertainty. Meanwhile, the naive Bayesian would simply pick the mean of the posterior distribution. Here is our machinery of the DecisionMaker for the estimation of the total price for prizes in these two cases. In [20]: ''' Igor Marfin <Unister Gmb, 2014> igor.marfin@unister.de A Base Class definition for the Decision Makers. ''' # Define the Basic class class BaseDecision ( object ): \"\"\" BaseDecision Class\"\"\" def __init__ ( self , train_data ): self . num_entries = len ( train_data ) self . data = train_data def decide ( self ): ''' returns the value of estimated total price for prizes ''' raise NotImplementedError ( 'prediction not implemented for base class' ) $$\\\\[5pt]$$ What do Frequentists decide about the price for the prize suite? In [21]: ''' Igor Marfin <Unister Gmb, 2014> igor.marfin@unister.de Decision Makers. ''' # Define the DecisionMaker for frequentists class DecisionMakerFrequentists ( BaseDecision ): \"\"\" DecisionMakerFrequentists Class\"\"\" def __init__ ( self , prior_data ): logging . info ( 'building the DecisionMakerFrequentists....' ) super ( DecisionMakerFrequentists , self ) . __init__ ( prior_data ) logging . info ( 'building the DecisionMakerFrequentists....done' ) def decide ( self ): ''' returns the value of estimated total price for prizes ''' return self . data . describe ()[ 'Price' ][ 'mean' ] def __str__ ( self ): return \" %s say: 'We think you have to bid %4.3f dollars USA'\" % ( self . __class__ . __name__ , self . decide ()) frequentists_decision_maker = DecisionMakerFrequentists ( df ) print frequentists_decision_maker INFO:root:building the DecisionMakerFrequentists.... INFO:root:building the DecisionMakerFrequentists....done DecisionMakerFrequentists say: 'We think you have to bid 25662.946 dollars USA' $$\\\\[5pt]$$ What do Naive Bayesians decide about the price for the prize suite? In [22]: ''' Igor Marfin <Unister Gmb, 2014> igor.marfin@unister.de Decision Makers. ''' # Define the DecisionMaker for Naive Bayesians class DecisionMakerNaiveBayesians ( BaseDecision ): \"\"\" DecisionMakerNaiveBayesians Class\"\"\" def __init__ ( self , prior_data , model = None ): logging . info ( 'building the DecisionMakerNaiveBayesians....' ) super ( DecisionMakerNaiveBayesians , self ) . __init__ ( prior_data ) self . model = model logging . info ( 'building the DecisionMakerNaiveBayesians....done' ) def decide ( self ): ''' returns the value of estimated total price for prizes ''' if ( self . model is None ): return 1e6 traces = self . model . prepare_trace () return traces [ 'true_price' ] . mean () def __str__ ( self ): return \" %s say: 'We think you have to bid %4.3f dollars USA'\" % ( self . __class__ . __name__ , self . decide ()) naivebayesians_decision_maker = DecisionMakerNaiveBayesians ( df , showcase_model ) print naivebayesians_decision_maker INFO:root:building the DecisionMakerNaiveBayesians.... INFO:root:building the DecisionMakerNaiveBayesians....done DecisionMakerNaiveBayesians say: 'We think you have to bid 16850.958 dollars USA' $$\\\\[5pt]$$ But we have more information about our eventual outcomes; we should incorporate this into our bid. We will use the loss function above to find the best bid ( best according to our loss). $$\\\\[5pt]$$ 0.5.4 The Decisions Making machinery with the Bayes rule First, we define the Loss function $L(\\theta,\\hat\\theta)$ as follows, recalling the conditions If a bid price is over the actual price, the bid's owner is disqualified from winning. If a bid price is under the true price by less than $250, the winner is awarded both def loss(true_price,guess,risk=80000): ''' returns the loss value ''' # Ups, we are disqualified ?! if true_price < guess: return risk # Woooooow we are awarded! elif abs(true_price - guess) <= 250: return -2*np.abs(true_price) # We missed to predict true_price, but we were close to it .... else: return np.abs(true_price - guess - 250) where risk is a parameter that defines of how bad it is if your guess is over the true price. A lower risk means that you are more comfortable with the idea of going over. If we do bid under and the difference is less than $250, we receive both prizes (modeled here as receiving twice the original prize). Otherwise, when we bid under the true_price we want to be as close as possible, hence the else loss is a increasing function of the distance between the guess and true price. In [23]: ''' Igor Marfin <Unister Gmb, 2014> igor.marfin@unister.de Decision Makers. ''' # Define the DecisionMaker for Bayesians utilizing the Bayes rule class DecisionLossBayesians ( BaseDecision ): \"\"\" DecisionLossBayesians Class\"\"\" def __init__ ( self , prior_data , model = None ): logging . info ( 'building the DecisionLossBayesians....' ) super ( DecisionLossBayesians , self ) . __init__ ( prior_data ) self . model = model self . traces = None logging . info ( 'building the DecisionLossBayesians....done' ) def __str__ ( self ): return \" %s say: 'We think you have to bid %4.3f dollars USA'\" % ( self . __class__ . __name__ , self . decide ()) def loss ( self , true_price , guess , risk = 80000 ): ''' returns the loss value ''' # Ups, we are disqualified ?! if true_price < guess : return risk # Woooooow we are awarded! elif abs ( true_price - guess ) <= 250 : return - 2 * np . abs ( true_price ) # We missed to predict true_price, but we were close to it .... else : return np . abs ( true_price - guess - 250 ) lossbayesians_decision_maker = DecisionLossBayesians ( df , showcase_model ) INFO:root:building the DecisionLossBayesians.... INFO:root:building the DecisionLossBayesians....done Let's add posterior risk to our DecisionLossBayesians : In [24]: ''' Igor Marfin <Unister Gmb, 2014> igor.marfin@unister.de Decision Makers. ''' def _posterior_risk ( self , rule , risk = 80000 ): ''' here rule is a guess of the price of the prize suite ''' if ( self . traces is None ): self . traces = self . model . prepare_trace () self . traces [ 'posterior_risk' ] = self . traces . apply ( lambda x : self . loss ( x [ 'true_price' ], rule , risk ), axis = 1 ) return self . traces [ 'posterior_risk' ] . values # Update the definition of the DecisionLossBayesians DecisionLossBayesians . posterior_risk = _posterior_risk lossbayesians_decision_maker . posterior_risk ( 1000 )[ 0 : 10 ] Out[24]: array([ 11691.16368355, 17278.25882064, 18758.79895935, 17176.76847303, 16539.20239225, 11228.87261291, 13511.96226114, 18198.09713237, 17651.353225 , 24510.8764822 ]) Let's add Bayes loss and Bayes rule to our DecisionLossBayesians class In [25]: ''' Igor Marfin <Unister Gmb, 2014> igor.marfin@unister.de Decision Makers. ''' def _bayes_loss ( self , rule , risk = 80000 ): ''' returns the bayes loss''' return np . mean ( self . posterior_risk ( rule , risk )) def _plot_bayes_loss ( self ): ''' plot the bayes risk of diffent rules (guesses) at different risk thresholds ''' figsize ( 12.5 , 7 ) guesses = np . linspace ( 5000 , 50000 , 70 ) risks = np . linspace ( 10000 , 150000 , 6 ) for _p in risks : results = [ self . bayes_loss ( _g , _p ) for _g in guesses ] plt . plot ( guesses , results , label = \" %d \" % _p ) plt . title ( \"Bayes Loss of different guesses, \\n various risk-levels of \\ overestimating\" ) plt . legend ( loc = \"upper left\" , title = \"Risk parameter\" ) plt . xlabel ( \"price bid\" ) plt . ylabel ( \"Bayes Loss\" ) plt . xlim ( 5000 , 30000 ); # Update the definition of the DecisionLossBayesians DecisionLossBayesians . bayes_loss = _bayes_loss DecisionLossBayesians . plot_bayes_loss = _plot_bayes_loss lossbayesians_decision_maker . plot_bayes_loss () In [27]: ''' Igor Marfin <Unister Gmb, 2014> igor.marfin@unister.de Decision Makers. ''' def _bayes_risk ( self , risk = 80000 ): ''' returns the bayes risk and bayes rule ''' import scipy.optimize as sop bayes_rule = sop . fmin ( self . bayes_loss , 15000 , args = ( risk ,), disp = False )[ 0 ] return self . bayes_loss ( bayes_rule , risk ), bayes_rule def _plot_bayes_risk ( self ): ''' plot the bayes risk of diffent rules (guesses) at different risk thresholds ''' figsize ( 12.5 , 7 ) guesses = np . linspace ( 5000 , 50000 , 70 ) risks = np . linspace ( 10000 , 250000 , 6 ) ax = plt . subplot ( 111 ) for _p in risks : _color = ax . _get_lines . color_cycle . next () _min_results = self . bayes_risk ( _p )[ 1 ] results = [ self . bayes_loss ( _g , _p ) for _g in guesses ] plt . plot ( guesses , results , color = _color , label = \" %d \" % _p ) plt . scatter ( _min_results , 0 , s = 60 , color = _color ) plt . vlines ( _min_results , 0 , 120000 , color = _color , linestyles = \"--\" ) print \"Bayes rule (minimum) at risk parameter %d : %.2f \" % ( _p , _min_results ) plt . title ( \"Bayes Loss & Bayes rule of different guesses, \\n various risk-levels of \\ overestimating\" ) plt . legend ( loc = \"upper left\" , title = \"Risk parameter\" ) plt . xlabel ( \"price bid\" ) plt . ylabel ( \"Bayes Loss\" ) plt . xlim ( 5000 , 30000 ); # Update the definition of the DecisionLossBayesians DecisionLossBayesians . bayes_risk = _bayes_risk DecisionLossBayesians . plot_bayes_risk = _plot_bayes_risk lossbayesians_decision_maker . plot_bayes_risk () Bayes rule (minimum) at risk parameter 10000: 14553.08 Bayes rule (minimum) at risk parameter 58000: 9261.83 Bayes rule (minimum) at risk parameter 106000: 7883.86 Bayes rule (minimum) at risk parameter 154000: 7883.86 Bayes rule (minimum) at risk parameter 202000: 7767.02 Bayes rule (minimum) at risk parameter 250000: 7583.94 As intuition suggests, as we decrease the risk threshold (care about overbidding less), we increase our bid, willing to edge closer to the true price. It is interesting how far away our optimized loss is from the posterior mean, which was about 16691. Suffice to say, in higher dimensions being able to eyeball the minimum expected loss is impossible. Hence why we require use of Scipy's fmin function. $$\\\\[5pt]$$ What do Bayesians using Bayes rule decide about the price for the prize suite? In [28]: ''' Igor Marfin <Unister Gmb, 2014> igor.marfin@unister.de Decision Makers. ''' def _decide ( self , risk = 10000 ): ''' returns the value of estimated total price for prizes ''' if ( self . model is None ): return 1e6 return self . bayes_risk ( risk )[ 1 ] # Update the definition of the DecisionLossBayesians DecisionLossBayesians . decide = _decide print lossbayesians_decision_maker DecisionLossBayesians say: 'We think you have to bid 14553.082 dollars USA' $$\\\\[10pt]$$ 0.6 Machine Learning with the Bayes Risk Whereas frequentist methods strive to achieve the best precision about all possible parameters , machine learning cares to achieve the best prediction among all possible parameters . Of course, one way to achieve accurate predictions is to aim for accurate predictions, but often your prediction measure and what frequentist methods are optimizing for are very different. For example, least-squares linear regression is the most simple active machine learning algorithm. I say active as it engages in some learning, whereas predicting the sample mean is technically simpler, but is learning very little if anything . The loss that determines the coefficients of the regressors is a squared-error loss . On the other hand, if your prediction loss function (or score function, which is the negative loss) is not a squared-error, like AUC, ROC, precision, etc., your least-squares line will not be optimal for the prediction loss function. This can lead to prediction results that are suboptimal . Finding Bayes rules is equivalent to finding parameters that optimize not parameter accuracy (!!) but an arbitrary performance measure (!!) , we wish to define performance (loss functions, AUC, ROC, precision/recall etc.). The next two examples demonstrate these ideas. The first example is a linear model where we can choose to predict using the least-squares loss or a novel, outcome-sensitive loss. The second example is adapted from a Kaggle data science project. The loss function associated with our predictions is incredibly complicated. $$\\\\[10pt]$$ 0.6.1 An Example: Financial prediction Suppose the future return of a stock price is very small, say 0.01 (or 1%). We have a model that predicts the stock's future price, and our profit and loss is directly tied to us acting on the prediction. How should we measure the loss associated with the model's predictions, and subsequent future predictions? A squared-error loss is agnostic to the signage and would penalize a prediction of -0.01 equally as bad a prediction of 0.03: $$ (0.01 - (-0.01))&#94;2 = (0.01 - 0.03)&#94;2 = 0.004$$ If you had made a bet based on your model's prediction, you would have earned money with a prediction of 0.03, and lost money with a prediction of -0.01, yet our loss did not capture this. We need a better loss that takes into account the sign of the prediction and true value. We design a new loss that is better for financial applications below: if the predicted return > true return , we earn money if the predicted return < true return , we loss money and it is very bad! def stock_loss(true_return, yhat, alpha=100.): ''' defines the loss on the stock price ''' if true_return * yhat < 0: # opposite signs, not good # loss is the function of the squared yhat and linear true_return return alpha * yhat ** 2 - np.sign(true_return) * yhat \\ + abs(true_return) else: return abs(true_return - yhat) In [29]: ''' Igor Marfin <Unister Gmb, 2014> igor.marfin@unister.de Decision Makers for the financial predictions ''' def stock_loss ( true_return , yhat , alpha = 100. ): ''' defines the loss on the stock price ''' if true_return * yhat < 0 : # opposite signs, not good # loss is the function of the squared yhat and linear true_return return alpha * yhat ** 2 - np . sign ( true_return ) * yhat \\ + abs ( true_return ) else : return abs ( true_return - yhat ) def plot_stock_loss ( true_value = . 05 ): ''' plot the stock loss ''' pred = np . linspace ( -. 04 , . 12 , 75 ) plt . plot ( pred , [ stock_loss ( true_value , _p ) for _p in pred ], label = \"Loss associated with \\n prediction if true value = %2.3f \" % true_value , lw=3) plt . vlines ( 0 , 0 , . 25 , linestyles = \"--\" ) plt . xlabel ( \"prediction\" ) plt . ylabel ( \"loss\" ) plt . xlim ( - 0.04 , . 12 ) plt . ylim ( 0 , 0.25 ) plt . legend () plt . title ( \"Stock returns loss \" ); plot_stock_loss ( true_value = . 05 ) plot_stock_loss ( true_value = -. 02 ) Note the change in the shape of the loss as the prediction crosses zero. This loss reflects that the user really does not want to guess the wrong sign, especially be wrong and a large magnitude. Financial institutions usualy operate with downside and upside risks: downside risk -- predicting a lot on the wrong side upside risk -- predicting a lot on the right side. Both are seen as risky behaviour and discouraged. Hence why we have an increasing loss as we move further away from the true price. (With less extreme loss in the direction of the correct sign.) We will perform a regression on a trading signal that we believe predicts future returns well. $$\\\\[10pt]$$ 0.6.2 Get the Financial data from Google: stock prices of Apple Inc. In Google Finance, intra-day data (stock prices down to two minute resolution) is available free for several stock markets. The list of the markets can be found here (GoogleFinance, 2015) . Maximillian Vitek has developed a simple python API that returns csv formatted intra-day data from the Google service (MaximillianVitek, 2014) . We are going to use this tool to obtain historical data observed yersterday (13.08.2015). In [28]: ! git clone https://github.com/maxvitek/intradata Cloning into 'intradata'... remote: Counting objects: 54, done.[K remote: Compressing objects: 100% (31/31), done.[K remote: Total 54 (delta 20), reused 54 (delta 20), pack-reused 0[K Unpacking objects: 100% (54/54), done. In [30]: # %load intradata/intradata.py import time import datetime import pandas import requests import csv import io import pytz PROTOCOL = 'http://' BASE_URL = 'www.google.com/finance/getprices' def get_google_data ( symbol , interval = 60 , lookback = 1 , end_time = time . time ()): \"\"\" Get intraday data for the symbol from google finance and return a pandas DataFrame :param symbol (str) :param interval (int) :param lookback (int) :param end_time (unix timestamp) :returns pandas.DataFrame \"\"\" resource_url = PROTOCOL + BASE_URL payload = { 'q' : symbol , 'i' : str ( interval ), 'p' : str ( lookback ) + 'd' , 'ts' : str ( int ( end_time * 1000 )), 'f' : 'd,o,h,l,c,v' } r = requests . get ( resource_url , params = payload ) quotes = [] with io . BytesIO ( r . content ) as csvfile : quote_reader = csv . reader ( csvfile ) timestamp_start = None timestamp_offset = None timezone_offset = 0 for row in quote_reader : if row [ 0 ][: 16 ] == 'TIMEZONE_OFFSET=' : timezone_offset = - 1 * int ( row [ 0 ][ 16 :]) elif row [ 0 ][ 0 ] not in 'a1234567890' : # discard headers continue elif row [ 0 ][ 0 ] == 'a' : # 'a' prepended to the timestamp that starts each day timestamp_start = pytz . utc . localize ( datetime . datetime . fromtimestamp ( float ( row [ 0 ][ 1 :])) + datetime . timedelta ( minutes = timezone_offset )) timestamp_offset = 0 elif timestamp_start : timestamp_offset = int ( row [ 0 ]) if not timestamp_start and not timestamp_offset : continue timestamp = timestamp_start + datetime . timedelta ( seconds = timestamp_offset * interval ) closing_price = float ( row [ 1 ]) high_price = float ( row [ 2 ]) low_price = float ( row [ 3 ]) open_price = float ( row [ 4 ]) volume = float ( row [ 5 ]) quotes . append (( timestamp , closing_price , high_price , low_price , open_price , volume )) df = pandas . DataFrame ( quotes , columns = [ 'datetime' , 'close' , 'high' , 'low' , 'open' , 'volume' ]) df = df . set_index ( 'datetime' ) return df Let's get the prices of the Apple Inc. stocks from yesterday (2015/09/25): In [31]: import time import datetime end_date_time = \"2015-09-25 00:00:00.00\" # interval=60 -- get ticks each minute aapl = get_google_data ( 'AAPL' , interval = 60 , lookback = 2 , end_time = time . mktime ( time . strptime ( end_date_time , \"%Y-%m- %d %H:%M:%S. %f \" ))) print \"total %d recordes downloaded from Google Finance\" % len (aapl) # remove todays data today_date_time = \"2015-09-26 00:00:00.00\" today_date_time = datetime . datetime . fromtimestamp ( time . mktime ( time . strptime ( today_date_time , \"%Y-%m- %d %H:%M:%S. %f \" ))) aapl = aapl . loc [ aapl [ aapl . index < today_date_time ] . index ,:] aapl = aapl . reset_index () aapl = aapl . set_index ( 'datetime' ) print \"total %d recordes selected \" % len (aapl) aapl [[ 'close' ]] . plot ( label = 'close price' ) plt . xlabel ( \"Date/Time\" ) plt . ylabel ( \"Price\" ) aapl . head ( 3 ) INFO:requests.packages.urllib3.connectionpool:Starting new HTTP connection (1): www.google.com total 782 recordes downloaded from Google Finance total 661 recordes selected Out[31]: close high low open volume datetime 2015-09-24 19:30:00+00:00 113.29 113.33 113.24 113.25 316231 2015-09-24 19:31:00+00:00 113.32 113.58 113.25 113.32 206457 2015-09-24 19:32:00+00:00 113.40 113.42 113.26 113.37 159438 $$\\\\[10pt]$$ 0.6.3 Ordinary Least Square Fit on the Apple stock prices Now let's make OLS on the close prices of ticks with numbers from interval [1,200] entries of our ticks and plot it: In [32]: # copy aapl dataset to make the new dataset aapl_ols _aapl = aapl . copy () # prepare the data for OLS, # on which we want to make a regression: we use first [0,200] ticks. # The regression formula is \"return ~ close +intercept\" tick_first = 1 tick_last = 200 # reset index aapl_ols = _aapl . reset_index ( drop = True ) _aapl [ 'tick_id' ] = aapl_ols . index . values # select first 200 ticks with id from ... _aapl = _aapl . loc [ _aapl [ ( _aapl [ 'tick_id' ] >= tick_first ) & ( _aapl [ 'tick_id' ] <= tick_last ) ] . index ] # make returns and rename columns aapl_ols = aapl_ols . pct_change () aapl_ols . columns = [ column + '_pct_change' for column in aapl_ols . columns ] # add tick_id aapl_ols [ 'tick_id' ] = aapl_ols . index . values # select first 200 ticks with id from ... aapl_ols = aapl_ols . loc [ aapl_ols [ ( aapl_ols [ 'tick_id' ] >= tick_first ) & ( aapl_ols [ 'tick_id' ] <= tick_last ) ] . index ] # drop the tick_id_pct_change column aapl_ols = pd . merge ( aapl_ols , _aapl , on = aapl_ols . tick_id , how = 'outer' ) aapl_ols = aapl_ols . drop ( 'tick_id_x' , 1 ) . rename ( columns = { 'tick_id_y' : 'tick_id' }) plt . scatter ( aapl_ols . close , aapl_ols . close_pct_change ) plt . xlabel ( \"Close price\" ) plt . ylabel ( \"Close return\" ) aapl_ols . head ( 1 ) Out[32]: close_pct_change high_pct_change low_pct_change open_pct_change volume_pct_change close high low open volume tick_id 0 0.000265 0.002206 0.000088 0.000618 -0.347132 113.32 113.58 113.25 113.32 206457 1 In [33]: import statsmodels.formula.api as sm result = sm . ols ( formula = \"close_pct_change ~ close\" , data = aapl_ols ) . fit () print result . params plt . scatter ( aapl_ols . close , aapl_ols . close_pct_change ) Y = aapl_ols . close_pct_change . values X = aapl_ols . close . values ls_intercept = result . params [ 'Intercept' ] ls_coef_ = result . params [ 'close' ] plt . plot ( np . sort ( X ), ls_coef_ * np . sort ( X ) + ls_intercept , label = \"Least-squares line\" ) plt . xlim ( X . min (), X . max ()) plt . ylim ( Y . min (), Y . max ()) plt . legend ( loc = \"upper left\" ) plt . xlabel ( \"Close Price\" ) plt . ylabel ( \"Close Return\" ) Intercept -0.029138 close 0.000258 dtype: float64 Out[33]: <matplotlib.text.Text at 0xd4b30ac> $$\\\\[10pt]$$ 0.6.4 Decision Maker with the Bayes rule to predict prices of the Apple stock Now we want to predict the return for the upcomming prices . To do this we create the DecisionMaker with the Bayes rule calculation. Our plan of next steps is Make a simple Bayesian linear regression (model) on this dataset; Develop DecisionMaker with the Bayes rule using the stock loss function. We look for the regression model like: $$ R = \\alpha + \\beta x + \\epsilon$$ where $\\alpha, \\beta$ are our unknown parameters and $\\epsilon \\sim \\text{Normal}(0, 1/\\tau)$. The most common priors on $\\beta$ and $\\alpha$ are Normal priors. We will also assign a prior on $\\tau$, so that $\\sigma = 1/\\sqrt{\\tau}$ is uniform over 0 to 100 (equivalently then $\\tau = 1/\\text{Uniform}(0, 100)&#94;2$). In [34]: ''' Igor Marfin <Unister Gmb, 2014> igor.marfin@unister.de A Model the Financial Example ''' #defines the class for the Modeling class ModelStock ( BaseModel ): \"\"\" A ModelStock class :param np.ndarray data: The data to use for learning the model. \"\"\" def __init__ ( self , data , std_value = 100. ): logging . info ( 'building the model....' ) super ( ModelStock , self ) . __init__ ( data ) # priors on the parameters of the model # std deviation parameter self . std = pm . Uniform ( \"std\" , 0 , std_value , trace = False ) # this needs to be explained. # tau -- a precision parameter @pm.deterministic def tau ( U = self . std ): return 1.0 / ( U ) ** 2 # alpha and beta parameters self . beta = pm . Normal ( \"beta\" , 0 , 1. / ( std_value ** 2 ) if std_value > 0. else 0.0001 ) self . alpha = pm . Normal ( \"alpha\" , 0 , 1. / ( std_value ** 2 ) if std_value > 0. else 0.0001 ) # the regression model self . X = self . data [ 'close' ] . values @pm.deterministic def mean ( X = self . X , alpha = self . alpha , beta = self . beta ): return alpha + beta * X # here we define what we observe self . Y = self . data [ 'close_pct_change' ] . values self . obs = pm . Normal ( \"obs\" , mean , tau , value = self . Y , observed = True ) # our model collects all definitions in self.model self . model = pm . MCMC ([ self . beta , self . alpha , self . obs , mean , tau , self . std , self . X , self . Y ]) logging . info ( 'done building the model' ) def _prepare_trace ( self ): ''' prepare a dataframe with traces of the parameters of our model ''' self . traces = { 'beta' : self . beta . trace (), 'alpha' : self . alpha . trace (), } for var in list ( self . model . deterministics ): if ( str ( var ) == 'mean' ): continue self . traces . update ( { str ( var ): np . array ( self . model . trace ( str ( var ))[:])} ) self . traces = pd . DataFrame ( self . traces ) return self . traces # Update the interface of our model ModelStock . prepare_trace = _prepare_trace _aapl_ols = aapl_ols . copy () _aapl_ols = _aapl_ols . sort ( columns = \"close\" ) modelstock_model = ModelStock ( _aapl_ols ) modelstock_model . model . sample ( 100000 , 80000 , 20 ) INFO:root:building the model.... INFO:root:done building the model [-----------------100%-----------------] 100000 of 100000 complete in 14.8 sec Let's make diagnostic of the MCMC chain: In [35]: from pymc.Matplot import plot as mcplot mcplot ( modelstock_model . model , path = \"plots/\" ) Plotting beta Plotting tau Plotting alpha Now let's repeat all steps of the creation of Decision Makers with the Bayes Risk Management, that we have made for the Showcase example. In [53]: ''' Igor Marfin <Unister Gmb, 2014> igor.marfin@unister.de Decision Makers. ''' # Define the DecisionMaker for Bayesians utilizing the Bayes rule class DecisionStockLossBayesians ( BaseDecision ): \"\"\" DecisionStockLossBayesians Class\"\"\" def __init__ ( self , prior_data , model = None ): logging . info ( 'building the DecisionStockLossBayesians....' ) super ( DecisionStockLossBayesians , self ) . __init__ ( prior_data ) self . model = model self . traces = None self . noises = None logging . info ( 'building the DecisionStockLossBayesians....done' ) def __str__ ( self ): return \" %s say: 'We predict the %3.2f return '\" % ( self . __class__ . __name__ , self . decide ()) def loss ( self , true_return , yhat , alpha = 100. ): ''' defines the loss on the stock price ''' if true_return * yhat < 0 : # opposite signs, not good # loss is the function of the squared yhat and linear true_return return alpha * yhat ** 2 - np . sign ( true_return ) * yhat \\ + abs ( true_return ) else : return abs ( true_return - yhat ) stocklossbayesians_decision_maker = DecisionStockLossBayesians ( modelstock_model . data , modelstock_model ) INFO:root:building the DecisionStockLossBayesians.... INFO:root:building the DecisionStockLossBayesians....done In [92]: ''' Igor Marfin <Unister Gmb, 2014> igor.marfin@unister.de Decision Makers. ''' stocklossbayesians_decision_maker . traces = None def _posterior_risk ( self , close_price , rule , alpha = 100. ): ''' here rule is a guess of the price of the prize suite ''' if ( self . traces is None ): self . traces = self . model . prepare_trace () self . alphas = self . traces [ 'alpha' ] . values self . betas = self . traces [ 'beta' ] . values self . taus = self . traces [ 'tau' ] . values N = self . taus . shape [ 0 ] self . noises = 1. / np . sqrt ( self . taus ) * np . random . randn ( N ) func = lambda x : self . loss ( x , rule , alpha ) return np . array ([ func ( x ) for x in self . alphas + self . betas * close_price + self . noises ]) #self.traces['posterior_risk'] = self.traces.apply( # lambda x: self.loss(x['alpha'] + x['beta']*close_price+\\ # 1./np.sqrt(x['tau'])* np.random.randn(), # rule,alpha), # axis=1 # ) #return self.traces['posterior_risk'].values def _bayes_loss ( self , rule , close_price , alpha = 100. ): ''' returns the bayes loss''' return np . mean ( self . posterior_risk ( close_price , rule , alpha )) def _bayes_risk ( self , close_price , alpha = 100. ): ''' returns the bayes risk and bayes rule ''' import scipy.optimize as sop bayes_rule , bayes_loss , _ , _ , _ = sop . fmin ( self . bayes_loss , 0. , args = ( close_price , alpha ,), full_output = True , disp = False ) bayes_rule = bayes_rule [ 0 ] return bayes_loss , bayes_rule # Update the definition of the DecisionLossBayesians DecisionStockLossBayesians . posterior_risk = _posterior_risk DecisionStockLossBayesians . bayes_loss = _bayes_loss DecisionStockLossBayesians . bayes_risk = _bayes_risk # test: Bayes Loss at our guess of -0.001 for the close_price=160. print stocklossbayesians_decision_maker . bayes_loss ( - 0.001 , 160. ) # test: Bayes Risk for the close_price=160. print stocklossbayesians_decision_maker . bayes_risk ( 160. ) 65.6006757262 (65.561155894085914, -0.00019043731685087546) Now, let's predict the return of the 'close' prices using the Bayes rule in our decision (it will take some time to calculate the result): In [72]: predicted_pct_change = [] for idx in range ( len ( stocklossbayesians_decision_maker . data )): if ( idx % 10 ==0): print 'Current entry %d'%idx close_price = stocklossbayesians_decision_maker . data . loc [ idx , 'close' ] predicted_pct_change += [ stocklossbayesians_decision_maker . bayes_risk ( close_price )[ 1 ]] #stocklossbayesians_decision_maker.data.loc[idx,'predicted_pct_change']= stocklossbayesians_decision_maker.bayes_risk(close_price) Current entry 0 Current entry 10 Current entry 20 Current entry 30 Current entry 40 Current entry 50 Current entry 60 Current entry 70 Current entry 80 Current entry 90 Current entry 100 Current entry 110 Current entry 120 Current entry 130 Current entry 140 Current entry 150 Current entry 160 Current entry 170 Current entry 180 Current entry 190 Here is the distribution of the predicted returns: In [73]: stocklossbayesians_decision_maker . data [ 'predicted_pct_change' ] = np . array ( predicted_pct_change ) stocklossbayesians_decision_maker . data [ 'predicted_pct_change' ] . plot ( kind = 'kde' ) Out[73]: <matplotlib.axes._subplots.AxesSubplot at 0xb37532ec> In order to use the obtained predicted returns as a function of the close prices, I will introduce a function which will not only interpolate predicted returns but also extrapolate them if the close price comes outside the region defined by the first 200 ticks. First, I sort in descending order the data according to the close prices. In [74]: _data = stocklossbayesians_decision_maker . data . copy () _data = _data . sort ( columns = \"close\" ) Then I study possible interpolation modes and introduce the extrapolation function: In [86]: import scipy.interpolate as sci kind = 'linear' interpolate = sci . interp1d ( stocklossbayesians_decision_maker . data [ 'close' ] . values , stocklossbayesians_decision_maker . data [ 'predicted_pct_change' ] . values , kind = kind ) min_X = stocklossbayesians_decision_maker . data [ 'close' ] . min () max_X = stocklossbayesians_decision_maker . data [ 'close' ] . max () print \"The (min,max) close price values = ( %f , %f )\" % ( min_X , max_X ) print \"Interpolation at %f gives %f \" % (( max_X + min_X ) / 2. , interpolate (( max_X + min_X ) / 2. )) def extrapolate ( x_orig , y_orig , X ): ''' makes interpolation or extrapolation ''' from scipy.interpolate import interp1d # Original data x = x_orig y = y_orig xmin = min ( x ) xmax = max ( x ) # Interpolator class f = interp1d ( x , y , kind = 'linear' ) f2 = interp1d ( x , y , kind = 'cubic' ) if ( X >= xmin ) and ( X <= xmax ): return f ( X ) # Output range (quite large) if ( X < xmin ): xo = np . arange ( X , xmax , 0.001 ) if ( X > xmax ): xo = np . arange ( xmin , X , 0.001 ) # Boolean indexing approach # Generate an empty output array for \"y\" values yo = np . empty_like ( xo ) # Values lower than the minimum \"x\" are extrapolated at the same time low = xo < f . x [ 0 ] yo [ low ] = f . y [ 0 ] + ( xo [ low ] - f . x [ 0 ]) * ( f . y [ 1 ] - f . y [ 0 ]) / ( f . x [ 1 ] - f . x [ 0 ]) # Values higher than the maximum \"x\" are extrapolated at same time high = xo > f . x [ - 1 ] yo [ high ] = f . y [ - 1 ] + ( xo [ high ] - f . x [ - 1 ]) * ( f . y [ - 1 ] - f . y [ - 2 ]) / ( f . x [ - 1 ] - f . x [ - 2 ]) # Values inside the interpolation range are interpolated directly inside = np . logical_and ( xo >= f . x [ 0 ], xo <= f . x [ - 1 ]) yo [ inside ] = f ( xo [ inside ]) if ( X < xmin ): return yo [ 0 ] if ( X > xmax ): return yo [ - 1 ] return yo [ 0 ] The (min,max) close price values = (112.440000,113.750000) Interpolation at 113.095000 gives 0.000281 Also I will smooth the interpolated result using the smooth function In [89]: def smooth ( x , window_len = 11 , window = 'hanning' ): if x . ndim != 1 : raise ValueError , \"smooth only accepts 1 dimension arrays.\" if x . size < window_len : raise ValueError , \"Input vector needs to be bigger than window size.\" if window_len < 3 : return x if not window in [ 'flat' , 'hanning' , 'hamming' , 'bartlett' , 'blackman' ]: raise ValueError , \"Window is on of 'flat', 'hanning', 'hamming', 'bartlett', 'blackman'\" s = numpy . r_ [ 2 * x [ 0 ] - x [ window_len - 1 :: - 1 ], x , 2 * x [ - 1 ] - x [ - 1 : - window_len : - 1 ]] if window == 'flat' : #moving average w = np . ones ( window_len , 'd' ) else : w = eval ( 'numpy.' + window + '(window_len)' ) y = np . convolve ( w / w . sum (), s , mode = 'same' ) return y [ window_len : - window_len + 1 ] smoothed = lambda y , window_len : smooth ( y , window_len = window_len ) In [90]: # x_orig (close price) and y_orig (return) contain the first 200 ticks x_orig , y_orig = _data [ 'close' ] . values , _data [ 'predicted_pct_change' ] . values # y is the return interpolated and extrapolated using 'predicted region' y = np . array ([ extrapolate ( x_orig , y_orig , x ) for x in _data [ 'close' ] . values ]) # plot obained result plot ( _data [ 'close' ] . values , smoothed ( y , 80 ), color = 'r' , label = 'return predicted and smoothed by Bayes Risk' ) scatter ( _data [ 'close' ] . values , _data [ 'close_pct_change' ] . values , color = 'g' , label = 'original returns' ) plot ( _data [ 'close' ] . values , ls_coef_ * _data [ 'close' ] . values + ls_intercept , color = 'b' , label = 'predicted by OLS' ) ylim ( min ( y_orig ), max ( y_orig )) legend () Out[90]: <matplotlib.legend.Legend at 0xd09998c> It is visibly seen the difference between predictions from the OLS and Bayesian rule. On average, the Bayesian-rule returns tend to be large then OLS. OLS is more pessimistic or in other words, conservative. Also, we can try to reproduce the predicted return using the method proposed by Cameron Davidson-Pilon in the book \"Bayesian Methods for Hackers\" (CameronDavidson-Pilon, 2015) . In [93]: from scipy.optimize import fmin def stock_loss ( price , pred , coef = 100 ): \"\"\"vectorized for numpy\"\"\" sol = np . zeros_like ( price ) ix = price * pred < 0 sol [ ix ] = coef * pred ** 2 - np . sign ( price [ ix ]) * pred + abs ( price [ ix ]) sol [ ~ ix ] = abs ( price [ ~ ix ] - pred ) return sol tau_samples = stocklossbayesians_decision_maker . taus alpha_samples = stocklossbayesians_decision_maker . alphas beta_samples = stocklossbayesians_decision_maker . betas X = _data [ 'close' ] . values N = tau_samples . shape [ 0 ] noise = 1. / np . sqrt ( tau_samples ) * np . random . randn ( N ) possible_outcomes = lambda signal : alpha_samples + beta_samples * signal \\ + noise opt_predictions = np . zeros ( 200 ) trading_signals = np . linspace ( min ( X ), max ( X ), 200 ) for i , _signal in enumerate ( trading_signals ): _possible_outcomes = possible_outcomes ( _signal ) #print _possible_outcomes.shape tomin = lambda pred : stock_loss ( _possible_outcomes , pred ) . mean () opt_predictions [ i ] = fmin ( tomin , 0 , disp = False ) plt . xlabel ( \"trading signal\" ) plt . ylabel ( \"prediction\" ) plt . title ( \"Least-squares prediction vs. Bayes action prediction\" ) plt . plot ( X , ls_coef_ * X + ls_intercept , color = 'r' , label = \"Least-squares prediction\" ) plt . xlim ( min ( X ), max ( X )) plt . plot ( trading_signals , opt_predictions , color = \"b\" , label = \"Bayes action prediction (from Cameron)\" ) plt . scatter ( X , _data [ 'close_pct_change' ] . values , color = 'g' , label = 'original data' ) plot ( _data [ 'close' ] . values , smoothed ( y , 80 ), color = 'm' , label = 'return predicted and smoothed by Bayes Risk (from me)' ) plt . legend ( loc = \"upper left\" ); The result from Cameron's method does not fit to observed return level well. The reason of that fact is not understood by me. What happens if we increase alpha parameter of the Loss function? In [94]: predicted_pct_change_600 = [] for idx in range ( len ( stocklossbayesians_decision_maker . data )): if ( idx % 20 ==0): print 'Current entry %d'%idx close_price = stocklossbayesians_decision_maker . data . loc [ idx , 'close' ] predicted_pct_change_600 += [ stocklossbayesians_decision_maker . bayes_risk ( close_price , 600 )[ 1 ]] stocklossbayesians_decision_maker . data [ 'predicted_pct_change_600' ] = np . array ( predicted_pct_change_600 ) stocklossbayesians_decision_maker . data [ 'predicted_pct_change_600' ] . plot ( kind = 'kde' , label = 'alpha=600' ) stocklossbayesians_decision_maker . data [ 'predicted_pct_change' ] . plot ( kind = 'kde' , label = 'alpha=100' ) legend () Current entry 0 Current entry 20 Current entry 40 Current entry 60 Current entry 80 Current entry 100 Current entry 120 Current entry 140 Current entry 160 Current entry 180 Out[94]: <matplotlib.legend.Legend at 0xd021b0c> Increasing the penalty factor $\\alpha$ narrows the range of the prediction values within zero. It was expected. Let's make a plot superimposing all predictions and real returns. In [95]: _data = stocklossbayesians_decision_maker . data . copy () _data = _data . sort ( columns = \"close\" ) x_orig , y_orig = _data [ 'close' ] . values , _data [ 'predicted_pct_change_600' ] . values y = np . array ([ extrapolate ( x_orig , y_orig , x ) for x in _data [ 'close' ] . values ]) x_orig , y_orig_2 = _data [ 'close' ] . values , _data [ 'predicted_pct_change' ] . values y2 = np . array ([ extrapolate ( x_orig , y_orig_2 , x ) for x in _data [ 'close' ] . values ]) # plot obained result plot ( _data [ 'close' ] . values , smoothed ( y , 80 ), color = 'r' , label = 'return predicted and smoothed by Bayes Risk with alpha=600' ) plot ( _data [ 'close' ] . values , smoothed ( y2 , 80 ), color = 'm' , label = 'return predicted and smoothed by Bayes Risk with alpha=100' ) scatter ( _data [ 'close' ] . values , _data [ 'close_pct_change' ] . values , color = 'g' , label = 'original returns' ) plot ( _data [ 'close' ] . values , ls_coef_ * _data [ 'close' ] . values + ls_intercept , color = 'b' , label = 'predicted by OLS' ) ylim ( min ( y_orig ), max ( y_orig )) legend () Out[95]: <matplotlib.legend.Legend at 0xc6e792c> We can try to predict returns for today (26.09.2015), using the Bayesian rule obtained on the yesterday's data. First, let's get the data. In [106]: end_date_time = \"2015-09-25 00:00:00.00\" # interval=60 -- get ticks each minute aapl = get_google_data ( 'AAPL' , interval = 60 , lookback = 1 , end_time = time . mktime ( time . strptime ( end_date_time , \"%Y-%m- %d %H:%M:%S. %f \" ))) print \"total %d recordes downloaded from Google Finance\" % len (aapl) # remove todays data today_date_time = \"2015-09-26 00:00:00.00\" today_date_time = datetime . datetime . fromtimestamp ( time . mktime ( time . strptime ( today_date_time , \"%Y-%m- %d %H:%M:%S. %f \" ))) aapl = aapl . loc [ aapl [ aapl . index < today_date_time ] . index ,:] aapl = aapl . reset_index () aapl = aapl . set_index ( 'datetime' ) print \"total %d recordes selected \" % len (aapl) aapl [[ 'close' ]] . plot ( label = 'close price' ) plt . xlabel ( \"Date/Time\" ) plt . ylabel ( \"Price\" ) INFO:requests.packages.urllib3.connectionpool:Starting new HTTP connection (1): www.google.com total 391 recordes downloaded from Google Finance total 270 recordes selected Out[106]: <matplotlib.text.Text at 0xd0f35cc> Then,we add returns to the dataframe. In [107]: # copy aapl dataset to make the new dataset aapl_ols _aapl = aapl . copy () # prepare the data for OLS, # on which we want to make a regression: we use first [0,200] ticks. # The regression formula is \"return ~ close +intercept\" tick_first = 1 tick_last = 200 # reset index aapl_ols = _aapl . reset_index ( drop = True ) _aapl [ 'tick_id' ] = aapl_ols . index . values # select first 100 ticks with id from ... _aapl = _aapl . loc [ _aapl [ ( _aapl [ 'tick_id' ] >= tick_first ) & ( _aapl [ 'tick_id' ] <= tick_last ) ] . index ] # make returns and rename columns aapl_ols = aapl_ols . pct_change () aapl_ols . columns = [ column + '_pct_change' for column in aapl_ols . columns ] # add tick_id aapl_ols [ 'tick_id' ] = aapl_ols . index . values # select first 200 ticks with id from ... aapl_ols = aapl_ols . loc [ aapl_ols [ ( aapl_ols [ 'tick_id' ] >= tick_first ) & ( aapl_ols [ 'tick_id' ] <= tick_last ) ] . index ] # drop the tick_id_pct_change column aapl_ols = pd . merge ( aapl_ols , _aapl , on = aapl_ols . tick_id , how = 'outer' ) aapl_ols = aapl_ols . drop ( 'tick_id_x' , 1 ) . rename ( columns = { 'tick_id_y' : 'tick_id' }) plt . scatter ( aapl_ols . close , aapl_ols . close_pct_change ) plt . xlabel ( \"Close price\" ) plt . ylabel ( \"Close return\" ) Out[107]: <matplotlib.text.Text at 0xcfdb76c> Then we plot predictions: In [110]: _aapl_ols = aapl_ols . copy () _aapl_ols = _aapl_ols . sort ( columns = \"close\" ) x_orig , y_orig = _data [ 'close' ] . values , _data [ 'predicted_pct_change' ] . values y = np . array ([ extrapolate ( x_orig , y_orig , x ) for x in _aapl_ols [ 'close' ] . values ]) # plot obained result plot ( _aapl_ols [ 'close' ] . values , smoothed ( y , 1 ), color = 'r' , label = 'return predicted and smoothed by Bayes Risk' ) scatter ( _aapl_ols [ 'close' ] . values , _aapl_ols [ 'close_pct_change' ] . values , color = 'g' , label = 'original returns' ) plot ( _aapl_ols [ 'close' ] . values , ls_coef_ * _aapl_ols [ 'close' ] . values + ls_intercept , color = 'b' , label = 'predicted by OLS' ) #ylim(min(y_orig)*1.2,max(y_orig)*1.2) legend () Out[110]: <matplotlib.legend.Legend at 0xaa0778cc> Definitely, the 'yesterday' model couldn't predict return today, because of the gap in the close prices (see yesterday plots of the AAPL timeseries) $$\\\\[10pt]$$ 0.6.5 An example of the Astrophysics: Observing Dark World Recently,in 2012, the Kaggle contest was about Observing Dark World (Kaggle, 2012) . There is more to the Universe than meets the eye. Out in the cosmos exists a form of matter that outnumbers the stuff we can see by almost 7 to 1, and we don't know what it is. What we do know is that it does not emit or absorb light, so we call it Dark Matter. Such a vast amount of aggregated matter does not go unnoticed. In fact we observe that this stuff aggregates and forms massive structures called Dark Matter Halos. Although dark, it warps and bends spacetime such that any light from a background galaxy which passes close to the Dark Matter will have its path altered and changed. This bending causes the galaxy to appear as an ellipse in the sky. Since there are many galaxies behind a Dark Matter halo, their shapes will correlate with its position. What's The Problem? Detecting these Dark Matter halos is hard, but possible using this data. If we can accurately estimate the positions of these halos, we can then understand the function they play in the Universe. There are various methods to attack the problem (we have given you some examples), however we have not been able to reach the level of precision required to understand exactly where this Dark Matter is for all Dark Matter halos. We challenge YOU to detect the most elusive, mysterious and yet most abundant matter in all existence. $$\\\\[10pt]$$ The Data The dataset is actually 300 separate files, each representing a sky . In each file, or sky, there are between 300 and 720 galaxies. Each galaxy has an $x$ and $y$ position associated with it, ranging from 0 to 4200, and measures of ellipticity: $e_1$ and $e_2$. Information about what these measures mean can be found here cite but for our purposes it does not matter besides for visualization purposes. Thus a typical sky might look like the following. In [111]: from draw_sky2 import draw_sky sns . set_context ( \"poster\" ) plt . figure ( figsize = ( 15 , 8 )) n_sky = 3 # choose a file/sky to examine. data = np . genfromtxt ( \"data/Train_Skies/Train_Skies/ \\ Training_Sky %d .csv\" % ( n_sky ), dtype = None , skip_header = 1 , delimiter = \",\" , usecols = [ 1 , 2 , 3 , 4 ]) print \"Data on galaxies in sky %d .\" % n_sky print \"position_x, position_y, e_1, e_2 \" fig = draw_sky ( data ) plt . title ( \"Galaxy positions and ellipcities of sky %d .\" % n_sky ) plt . xlabel ( \"x-position\" ) plt . ylabel ( \"y-position\" ); Data on galaxies in sky 3. position_x, position_y, e_1, e_2 <matplotlib.figure.Figure at 0xd101cac> $$\\\\[10pt]$$ The Model of the Dark Matter Each sky has one, two or three dark matter halos in it. I will follow details of the winning solution in the copmetetion proposed by Tim Salimans. Tim's solution details that his prior distribution of halo positions was uniform, i.e. \\begin{align} & x_i \\sim \\text{Uniform}( 0, 4200)\\\\\\\\ & y_i \\sim \\text{Uniform}( 0, 4200), \\;\\; i=1,2,3\\\\\\\\ \\end{align} Tim and other competitors noted that most skies had one large halo and other halos, if present, were much smaller. Larger halos, having more mass, will influence the surrounding galaxies more. He decided that the large halos would have a mass distributed as a log -uniform random variable between 40 and 180 i.e. $$ m_{\\text{large} } = \\log \\text{Uniform}( 40, 180 ) $$ and in PyMC, exp_mass_large = pm.Uniform(\"exp_mass_large\", 40, 180) @pm.deterministic def mass_large(u = exp_mass_large): return np.log(u) (This is what we mean when we say log -uniform.) For smaller galaxies, Tim set the mass to be the logarithm of 20. Why did Tim not create a prior for the smaller mass, nor treat it as a unknown? I believe this decision was made to speed up convergence of the algorithm. This is not too restrictive, as by construction the smaller halos have less influence on the galaxies. Tim logically assumed that the ellipticity of each galaxy is dependent on the position of the halos, the distance between the galaxy and halo, and the mass of the halos. Thus the vector of ellipticity of each galaxy, $\\mathbf{e}_i$, are children variables of the vector of halo positions $(\\mathbf{x},\\mathbf{y})$, distance (which we will formalize), and halo masses. Tim conceived a relationship to connect positions and ellipticity by reading literature and forum posts. He supposed the following was a reasonable relationship: $$ e_i | ( \\mathbf{x}, \\mathbf{y} ) \\sim \\text{Normal}( \\sum_{j = \\text{halo positions} }d_{i,j} m_j f( r_{i,j} ), \\sigma&#94;2 ) $$ where $d_{i,j}$ is the tangential direction (the direction in which halo $j$ bends the light of galaxy $i$), $m_j$ is the mass of halo $j$, $f(r_{i,j})$ is a decreasing function of the Euclidean distance between halo $j$ and galaxy $i$. Tim's function $f$ was defined: $$ f( r_{i,j} ) = \\frac{1}{\\min( r_{i,j}, 240 ) } $$ for large halos, and for small halos $$ f( r_{i,j} ) = \\frac{1}{\\min( r_{i,j}, 70 ) } $$ This fully bridges our observations and unknown. This model is incredibly simple, and Tim mentions this simplicity was purposefully designed: it prevents the model from overfitting. Training & PyMC implementation For each sky, we run our Bayesian model to find the posteriors for the halo positions — we ignore the (known) halo position. This is slightly different than perhaps traditional approaches to Kaggle competitions, where this model uses no data from other skies nor the known halo location. That does not mean other data are not necessary — in fact, the model was created by comparing different skies. In [112]: ''' Igor Marfin <Unister Gmb, 2014> igor.marfin@unister.de A Model for the Dark Matter Study ''' #defines the class for the Modeling class ModelDarkMatter ( BaseModel ): \"\"\" A ModelDarkMatter class :param np.ndarray data: The data to use for learning the model. \"\"\" def __init__ ( self , data ): logging . info ( 'building the model....' ) super ( ModelDarkMatter , self ) . __init__ ( data ) # a priory self . exp_mass_large = pm . Uniform ( \"exp_mass_large\" , 40 , 180 ) # set the initial prior position of the halos, it's a 2-d Uniform dist. self . halo_position = pm . Uniform ( \"halo_position\" , 0 , 4200 , size = ( 2 ,)) # LogUniform prior on the mass of halos @pm.deterministic def mass_large ( u = self . exp_mass_large ): return np . log ( u ) def euclidean_distance ( x , y ): return np . sqrt ((( x - y ) ** 2 ) . sum ( axis = 1 )) # this function plays the role of the loss function def f_distance ( gxy_pos , halo_pos , c ): # halo_pos should be a 2-d numpy array return np . maximum ( euclidean_distance ( gxy_pos , halo_pos ), c )[:, None ] def tangential_distance ( glxy_position , halo_position ): # foo_position should be a 2-d numpy array delta = glxy_position - halo_position t = ( 2 * np . arctan ( delta [:, 1 ] / delta [:, 0 ]))[:, None ] # [:,None] is needed # for this np.concatenate. Here we use axis=1 to make an array of tuples # (-np.cos(t), -np.sin(t)) return np . concatenate ([ - np . cos ( t ), - np . sin ( t )], axis = 1 ) @pm.deterministic # data[:, :2] -> data[:](0,1) -> x and y positions of the galaxies def mean ( mass = mass_large , h_pos = self . halo_position , glx_pos = data [:, : 2 ]): return mass / f_distance ( glx_pos , h_pos , 240 ) * \\ tangential_distance ( glx_pos , h_pos ) # Here we add the observed value: ellipcity self . ellipcity = pm . Normal ( \"ellipcity\" , mean , 1. / 0.005 , observed = True , value = data [:, 2 :]) # our model collects all definitions in self.model self . model = pm . MCMC ([ self . exp_mass_large , mean , self . halo_position , mass_large , self . ellipcity ]) self . map = pm . MAP ([ self . exp_mass_large , mean , self . halo_position , mass_large , self . ellipcity ]) self . map . fit () logging . info ( 'done building the model' ) def _prepare_trace ( self ): ''' prepare a dataframe with traces of the parameters of our model ''' self . traces = { 'exp_mass_large' : self . exp_mass_large . trace (), 'halo_position' : self . halo_position . trace () } return self . traces # Update the interface of our model ModelDarkMatter . prepare_trace = _prepare_trace darkmatter_model = ModelDarkMatter ( data ) darkmatter_model . model . sample ( 200000 , 140000 , 3 ) INFO:root:building the model.... INFO:root:done building the model [-----------------100%-----------------] 200000 of 200000 complete in 128.1 sec After the model has been built, we can introduce the machinary to make decisions. We are going to use the Bayes risk again to chose the most optimal prediction of the halo position. First, we define the basic functionality of the DecisionDarkMatterBayesians , namely the loss function. In [113]: ''' Igor Marfin <Unister Gmb, 2014> igor.marfin@unister.de Decision Makers. ''' # Define the DecisionMaker for Bayesians utilizing the Bayes rule class DecisionDarkMatterBayesians ( BaseDecision ): \"\"\" DecisionDarkMatterBayesians Class\"\"\" def __init__ ( self , prior_data , model = None ): logging . info ( 'building the DecisionLossBayesians....' ) super ( DecisionDarkMatterBayesians , self ) . __init__ ( prior_data ) self . model = model self . traces = None logging . info ( 'building the DecisionLossBayesians....done' ) def __str__ ( self ): return \" %s say: ....\" % ( self . __class__ . __name__ ) def loss ( self , theta_true , theta_guess ): ''' the squared-error loss function ''' # put some 'printings' of the debug information # for testing purpose # print \"theta_true=\",theta_true # print \"theta_guess=\",theta_guess # print \"a=\", (theta_true-theta_guess)**2 # print \"a=\", ((theta_true-theta_guess)**2).sum() return (( theta_true - theta_guess ) ** 2 ) . sum () # return ((theta_true-theta_guess)**2).sum(axis=1) darkmatterbayesians_decision_maker = DecisionDarkMatterBayesians ( data , darkmatter_model ) INFO:root:building the DecisionLossBayesians.... INFO:root:building the DecisionLossBayesians....done In [114]: ''' Igor Marfin <Unister Gmb, 2014> igor.marfin@unister.de Decision Makers. ''' def _posterior_risk ( self , rule ): ''' here rule is a guess of the price of the prize suite ''' if ( self . traces is None ): self . traces = self . model . prepare_trace () self . hallo_positions = self . traces [ 'halo_position' ] func = lambda x : self . loss ( x , rule ) return np . array ([ func ( x ) for x in self . hallo_positions ]) def _bayes_loss ( self , rule ): ''' returns the bayes loss''' return np . mean ( self . posterior_risk ( rule )) def _bayes_risk ( self ): ''' returns the bayes risk and bayes rule ''' import scipy.optimize as sop bayes_rule , bayes_loss , _ , _ , _ = sop . fmin ( self . bayes_loss , np . array ([ 0. , 0. ]), full_output = True , disp = False ) return bayes_loss , bayes_rule # Update the definition of the DecisionLossBayesians DecisionDarkMatterBayesians . posterior_risk = _posterior_risk DecisionDarkMatterBayesians . bayes_loss = _bayes_loss DecisionDarkMatterBayesians . bayes_risk = _bayes_risk print darkmatterbayesians_decision_maker . bayes_loss ( np . array ([ 0. , 0. ])) bayes_halo_poistion = darkmatterbayesians_decision_maker . bayes_risk () print bayes_halo_poistion 6796131.95899 (8683.4289535070984, array([ 2329.17118849, 1167.22325052])) Now, if we do the ad-hoc test, just averaging posterior halo positions, we obtain the same result, as DecisionDarkMatterBayesians returns. This happens because we have included the function $$ f( r_{i,j} ) = \\frac{1}{\\min( r_{i,j}, 240 ) } $$ in our Likelihood. In [115]: # a sanity test, showing that the Bayes Risk Management should gives us these values! print darkmatter_model . prepare_trace ()[ 'halo_position' ] . mean ( axis = 0 ) [ 2329.17117338 1167.22327562] Below we plot a \"heatmap\" of the posterior distribution (which is just a scatter plot of the posterior, but we can visualize it as a heatmap). In [116]: fig = draw_sky ( data ) plt . title ( \"Galaxy positions and ellipcities of sky %d .\" % n_sky ) plt . xlabel ( \"x-position\" ) plt . ylabel ( \"y-position\" ) # a \"heatmap\" of the posterior halo positions plt . scatter ( darkmatterbayesians_decision_maker . hallo_positions [:, 0 ], darkmatterbayesians_decision_maker . hallo_positions [:, 1 ], alpha = 0.015 , c = \"r\" , label = \"posterior halo positions\" ) #a positions of the posterior halo given by Bayes rule plt . scatter ( bayes_halo_poistion [ 1 ][ 0 ], bayes_halo_poistion [ 1 ][ 1 ], alpha = 0.78 , c = \"k\" , s = 100 , label = \"Bayes rule of the halo position\" ) # Let's get true positions of the halo halo_data = np . genfromtxt ( \"data/Training_halos.csv\" , delimiter = \",\" , usecols = [ 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 ], skip_header = 1 ) plt . scatter ( halo_data [ n_sky - 1 ][ 3 ], halo_data [ n_sky - 1 ][ 4 ], label = \"True halo position\" , c = \"m\" , s = 70 ) plt . legend ( scatterpoints = 1 , loc = \"lower left\" ) plt . xlim ( 0 , 4200 ) plt . ylim ( 0 , 4200 ); Our next step is to use the the score function from organizers to understand how small our score (as smaller as better). In [117]: from DarkWorldsMetric import main_score _halo_data = halo_data [ n_sky - 1 ] nhalo_all = _halo_data [ 0 ] . reshape ( 1 , 1 ) x_true_all = _halo_data [ 3 ] . reshape ( 1 , 1 ) y_true_all = _halo_data [ 4 ] . reshape ( 1 , 1 ) x_ref_all = _halo_data [ 1 ] . reshape ( 1 , 1 ) y_ref_all = _halo_data [ 2 ] . reshape ( 1 , 1 ) # Our result print \"Using the Bayes risk:\" main_score ( nhalo_all , x_true_all , y_true_all , x_ref_all , y_ref_all , bayes_halo_poistion [ 1 ] . reshape ( 1 , 2 )) # The 'random' result random_guess = np . random . randint ( 0 , 4200 , size = ( 1 , 2 )) print \"Using a random location:\" , random_guess main_score ( nhalo_all , x_true_all , y_true_all , x_ref_all , y_ref_all , random_guess ) Using the Bayes risk: Your average distance in pixels you are away from the true halo is 86.3183131368 Your average angular vector is 1.0 Your score for the training data is 1.08631831314 Using a random location: [[ 753 3691]] Your average distance in pixels you are away from the true halo is 3041.28644342 Your average angular vector is 1.0 Your score for the training data is 4.04128644342 Out[117]: 4.0412864434150233 There are only 86 pixels away from the true halo positions, not bad! This is a good guess, it is not very far from the true location. We can also extend our code to allow for up to two additional, smaller halos: Let's add a new prior function to the model caring information about small halos. We add two small masses. In [118]: ''' Igor Marfin <Unister Gmb, 2014> igor.marfin@unister.de A Model for the Dark Matter Study ''' #defines the class for the Modeling class ModelDarkMatterExtended ( BaseModel ): \"\"\" A ModelDarkMatterExtended class :param np.ndarray data: The data to use for learning the model. \"\"\" def __init__ ( self , data ): logging . info ( 'building the model....' ) super ( ModelDarkMatterExtended , self ) . __init__ ( data ) # a priory self . exp_mass_large = pm . Uniform ( \"exp_mass_large\" , 40 , 180 ) self . exp_mass_small = pm . Uniform ( \"exp_mass_small\" , 10 , 20 ) # new priory self . exp_mass_medium = pm . Uniform ( \"exp_mass_medium\" , 20 , 40 ) # new priory # set the initial prior position of the halos, it's a 2-d Uniform dist. # we consider 3 halos with 3 masses self . halo_position = pm . Uniform ( \"halo_position\" , 0 , 4200 , size = ( 3 , 2 )) # LogUniform prior on the mass of halos @pm.deterministic def mass_large ( u = self . exp_mass_large ): return np . log ( u ) # LogUniform prior on the mass of halos @pm.deterministic def mass_small ( u = self . exp_mass_small ): return np . log ( u ) # LogUniform prior on the mass of halos @pm.deterministic def mass_medium ( u = self . exp_mass_medium ): return np . log ( u ) def euclidean_distance ( x , y ): return np . sqrt ((( x - y ) ** 2 ) . sum ( axis = 1 )) self . fdist_constants = np . array ([ 240 , 70 , 50 ]) self . masses = np . array ([ mass_large , mass_medium , mass_small ], dtype = object ) # this function plays the role of the loss function def f_distance ( gxy_pos , halo_pos , c ): # halo_pos should be a 2-d numpy array return np . maximum ( euclidean_distance ( gxy_pos , halo_pos ), c )[:, None ] def tangential_distance ( glxy_position , halo_position ): # foo_position should be a 2-d numpy array delta = glxy_position - halo_position t = ( 2 * np . arctan ( delta [:, 1 ] / delta [:, 0 ]))[:, None ] # [:,None] is needed # for this np.concatenate. Here we use axis=1 to make an array of tuples (-np.cos(t), -np.sin(t)) return np . concatenate ([ - np . cos ( t ), - np . sin ( t )], axis = 1 ) @pm.deterministic # data[:, :2] -> data[:](0,1) -> x and y positions of the galaxies def mean ( masses = self . masses , h_pos = self . halo_position , glx_pos = data [:, : 2 ]): means = [ masses [ i ] / f_distance ( glx_pos , h_pos [ i ,:], self . fdist_constants [ i ]) * \\ tangential_distance ( glx_pos , h_pos [ i ,:]) for i in range ( 3 )] return sum ( means ) # Here we add the observed value: ellipcity self . ellipcity = pm . Normal ( \"ellipcity\" , mean , 1. / 0.005 , observed = True , value = data [:, 2 :]) # our model collects all definitions in self.model self . model = pm . MCMC ([ self . exp_mass_large , self . exp_mass_small , self . exp_mass_medium , mean , self . halo_position , mass_large , mass_medium , mass_small , self . ellipcity ]) self . map = pm . MAP ([ self . exp_mass_large , self . exp_mass_small , self . exp_mass_medium , mean , self . halo_position , mass_large , mass_medium , mass_small , self . ellipcity ]) self . map . fit () logging . info ( 'done building the model' ) def _prepare_trace ( self ): ''' prepare a dataframe with traces of the parameters of our model ''' self . traces = { 'exp_mass_large' : self . exp_mass_large . trace (), 'exp_mass_small' : self . exp_mass_small . trace (), 'exp_mass_medium' : self . exp_mass_medium . trace (), 'halo_position' : self . halo_position . trace () } return self . traces # Update the interface of our model ModelDarkMatterExtended . prepare_trace = _prepare_trace darkmatter_extendedmodel = ModelDarkMatterExtended ( data ) darkmatter_extendedmodel . model . sample ( 200000 , 140000 , 3 ) INFO:root:building the model.... INFO:root:done building the model [-----------------100%-----------------] 200000 of 200000 complete in 615.1 sec In [119]: darkmatterbayesians_decision_maker = DecisionDarkMatterBayesians ( data , darkmatter_extendedmodel ) print darkmatterbayesians_decision_maker . bayes_loss ( np . array ([ 0. , 0. ])) bayes_halo_poistion = darkmatterbayesians_decision_maker . bayes_risk () print bayes_halo_poistion INFO:root:building the DecisionLossBayesians.... INFO:root:building the DecisionLossBayesians....done 51280068.7381 (4249968.502866541, array([ 1806.91417868, 3523.03297678])) In [120]: print \"Using the Bayes risk:\" main_score ( nhalo_all , x_true_all , y_true_all , x_ref_all , y_ref_all , bayes_halo_poistion [ 1 ] . reshape ( 1 , 2 )) Using the Bayes risk: Your average distance in pixels you are away from the true halo is 2493.55780435 Your average angular vector is 1.0 Your score for the training data is 3.49355780435 Out[120]: 3.4935578043485913 We have got the worse result for the sky number 3, because this sky contains only one halo. On practice, we can introduce the factors, uniform distributed,in [0,1] range which would be fractions of mass contributions from large, medium and small hallos. The plot of the sky #3 and hallos supports my words. In [121]: fig = draw_sky ( data ) plt . title ( \"Galaxy positions and ellipcities of sky %d .\" % n_sky ) plt . xlabel ( \"x-position\" ) plt . ylabel ( \"y-position\" ) # a \"heatmap\" of the posterior halo positions plt . scatter ( darkmatterbayesians_decision_maker . hallo_positions [:, 0 ], darkmatterbayesians_decision_maker . hallo_positions [:, 1 ], alpha = 0.015 , c = \"r\" , label = \"posterior halo positions\" ) #a positions of the posterior halo given by Bayes rule plt . scatter ( bayes_halo_poistion [ 1 ][ 0 ], bayes_halo_poistion [ 1 ][ 1 ], alpha = 0.78 , c = \"k\" , s = 100 , label = \"Bayes rule of the halo position\" ) # Let's get true positions of the halo halo_data = np . genfromtxt ( \"data/Training_halos.csv\" , delimiter = \",\" , usecols = [ 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 ], skip_header = 1 ) plt . scatter ( halo_data [ n_sky - 1 ][ 3 ], halo_data [ n_sky - 1 ][ 4 ], label = \"True halo position\" , c = \"m\" , s = 70 ) plt . legend ( scatterpoints = 1 , loc = \"lower left\" ) plt . xlim ( 0 , 4200 ) plt . ylim ( 0 , 4200 ); Small and Medium hallos distort and reduce the influence of the large hallo, thus it is making predictions more vulnerable. That's it for this lecture. I hope that this lecture/tutorial came out good. In [ ]: ### Do not delete the following Markdown section! ### This is the BibTeX references! References &#94; &#94; &#94; D. WARNER NORTH,. 1968. A Tutorial Introduction to Decision Theory . URL &#94; Wikimedia Foundation, Inc. 2015. Risk aversion . URL &#94; James F.,. 2012. Decision Theory . URL &#94; Jordan I.M.,. 2014. Lecture3: Decision theory . URL &#94; &#94; www.datagrabber.org,. 2015. Facebook Price Is Right SHOWCASE (Retail Prices) . URL &#94; Google Finance,. 2015. Finance Data Listing and Disclaimers . URL &#94; Maximillian Vitek,. 2014. A python module for getting intraday data from Google Finance . URL &#94; Cameron Davidson-Pilon,. 2015. Bayesian Methods for Hackers: Chapter5 . URL &#94; Kaggle,. 2012. Can you find the Dark Matter that dominates our Universe? . URL if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML'; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: 'center',\" + \" displayIndent: '0em',\" + \" showMathMenu: true,\" + \" tex2jax: { \" + \" inlineMath: [ ['$','$'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }\" + \" } \" + \"}); \"; (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Statistics","title":"Bayesian Risk"},{"url":"http://igormarfin.github.io/blog/2016/02/07/probabilistic-matrix-factorization-for-making-personalized-recommendations/","text":"/*! * * IPython notebook * */.ansibold{font-weight:bold}.ansiblack{color:black}.ansired{color:darkred}.ansigreen{color:darkgreen}.ansiyellow{color:#c4a000}.ansiblue{color:darkblue}.ansipurple{color:darkviolet}.ansicyan{color:steelblue}.ansigray{color:gray}.ansibgblack{background-color:black}.ansibgred{background-color:red}.ansibggreen{background-color:green}.ansibgyellow{background-color:yellow}.ansibgblue{background-color:blue}.ansibgpurple{background-color:magenta}.ansibgcyan{background-color:cyan}.ansibggray{background-color:gray}div.cell{border:1px solid transparent;display:-webkit-box;-webkit-box-orient:vertical;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:vertical;-moz-box-align:stretch;display:box;box-orient:vertical;box-align:stretch;display:flex;flex-direction:column;align-items:stretch;border-radius:2px;box-sizing:border-box;-moz-box-sizing:border-box;-webkit-box-sizing:border-box;border-width:thin;border-style:solid;width:100%;padding:5px;margin:0;outline:none}div.cell.selected{border-color:#ababab}@media print{div.cell.selected{border-color:transparent}}.edit_mode div.cell.selected{border-color:green}@media print{.edit_mode div.cell.selected{border-color:transparent}}.prompt{min-width:14ex;padding:.4em;margin:0;font-family:monospace;text-align:right;line-height:1.21429em}@media (max-width:540px){.prompt{text-align:left}}div.inner_cell{display:-webkit-box;-webkit-box-orient:vertical;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:vertical;-moz-box-align:stretch;display:box;box-orient:vertical;box-align:stretch;display:flex;flex-direction:column;align-items:stretch;-webkit-box-flex:1;-moz-box-flex:1;box-flex:1;flex:1}@-moz-document url-prefix(){div.inner_cell{overflow-x:hidden}}div.input_area{border:1px solid #cfcfcf;border-radius:2px;background:#f7f7f7;line-height:1.21429em}div.prompt:empty{padding-top:0;padding-bottom:0}div.unrecognized_cell{padding:5px 5px 5px 0;display:-webkit-box;-webkit-box-orient:horizontal;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:horizontal;-moz-box-align:stretch;display:box;box-orient:horizontal;box-align:stretch;display:flex;flex-direction:row;align-items:stretch}div.unrecognized_cell .inner_cell{border-radius:2px;padding:5px;font-weight:bold;color:red;border:1px solid #cfcfcf;background:#eaeaea}div.unrecognized_cell .inner_cell a{color:inherit;text-decoration:none}div.unrecognized_cell .inner_cell a:hover{color:inherit;text-decoration:none}@media (max-width:540px){div.unrecognized_cell>div.prompt{display:none}}@media print{div.code_cell{page-break-inside:avoid}}div.input{page-break-inside:avoid;display:-webkit-box;-webkit-box-orient:horizontal;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:horizontal;-moz-box-align:stretch;display:box;box-orient:horizontal;box-align:stretch;display:flex;flex-direction:row;align-items:stretch}@media (max-width:540px){div.input{display:-webkit-box;-webkit-box-orient:vertical;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:vertical;-moz-box-align:stretch;display:box;box-orient:vertical;box-align:stretch;display:flex;flex-direction:column;align-items:stretch}}div.input_prompt{color:navy;border-top:1px solid transparent}div.input_area>div.highlight{margin:.4em;border:none;padding:0;background-color:transparent}div.input_area>div.highlight>pre{margin:0;border:none;padding:0;background-color:transparent}.CodeMirror{line-height:1.21429em;font-size:14px;height:auto;background:none}.CodeMirror-scroll{overflow-y:hidden;overflow-x:auto}.CodeMirror-lines{padding:.4em}.CodeMirror-linenumber{padding:0 8px 0 4px}.CodeMirror-gutters{border-bottom-left-radius:2px;border-top-left-radius:2px}.CodeMirror pre{padding:0;border:0;border-radius:0}.highlight-base{}.highlight-variable{}.highlight-variable-2{color:#1a1a1a}.highlight-variable-3{color:#333}.highlight-string{color:#ba2121}.highlight-comment{color:#408080;font-style:italic}.highlight-number{80}.highlight-atom{color:#88f}.highlight-keyword{8000;font-weight:bold}.highlight-builtin{8000}.highlight-error{color:#f00}.highlight-operator{color:#a2f;font-weight:bold}.highlight-meta{color:#a2f}.highlight-def{f}.highlight-string-2{color:#f50}.highlight-qualifier{color:#555}.highlight-bracket{color:#997}.highlight-tag{color:#170}.highlight-attribute{c}.highlight-header{color:blue}.highlight-quote{90}.highlight-link{c}.cm-s-ipython span.cm-keyword{8000;font-weight:bold}.cm-s-ipython span.cm-atom{color:#88f}.cm-s-ipython span.cm-number{80}.cm-s-ipython span.cm-def{f}.cm-s-ipython span.cm-variable{}.cm-s-ipython span.cm-operator{color:#a2f;font-weight:bold}.cm-s-ipython span.cm-variable-2{color:#1a1a1a}.cm-s-ipython span.cm-variable-3{color:#333}.cm-s-ipython span.cm-comment{color:#408080;font-style:italic}.cm-s-ipython span.cm-string{color:#ba2121}.cm-s-ipython span.cm-string-2{color:#f50}.cm-s-ipython span.cm-meta{color:#a2f}.cm-s-ipython span.cm-qualifier{color:#555}.cm-s-ipython span.cm-builtin{8000}.cm-s-ipython span.cm-bracket{color:#997}.cm-s-ipython span.cm-tag{color:#170}.cm-s-ipython span.cm-attribute{c}.cm-s-ipython span.cm-header{color:blue}.cm-s-ipython span.cm-quote{90}.cm-s-ipython span.cm-link{c}.cm-s-ipython span.cm-error{color:#f00}.cm-s-ipython span.cm-tab{background:url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADAAAAAMCAYAAAAkuj5RAAAAAXNSR0IArs4c6QAAAGFJREFUSMft1LsRQFAQheHPowAKoACx3IgEKtaEHujDjORSgWTH/ZOdnZOcM/sgk/kFFWY0qV8foQwS4MKBCS3qR6ixBJvElOobYAtivseIE120FaowJPN75GMu8j/LfMwNjh4HUpwg4LUAAAAASUVORK5CYII=);background-position:right;background-repeat:no-repeat}div.output_wrapper{position:relative;display:-webkit-box;-webkit-box-orient:vertical;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:vertical;-moz-box-align:stretch;display:box;box-orient:vertical;box-align:stretch;display:flex;flex-direction:column;align-items:stretch}div.output_scroll{height:24em;width:100%;overflow:auto;border-radius:2px;-webkit-box-shadow:inset 0 2px 8px rgba(0,0,0,0.8);box-shadow:inset 0 2px 8px rgba(0,0,0,0.8);display:block}div.output_collapsed{margin:0;padding:0;display:-webkit-box;-webkit-box-orient:vertical;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:vertical;-moz-box-align:stretch;display:box;box-orient:vertical;box-align:stretch;display:flex;flex-direction:column;align-items:stretch}div.out_prompt_overlay{height:100%;padding:0 .4em;position:absolute;border-radius:2px}div.out_prompt_overlay:hover{-webkit-box-shadow:inset 0 0 1px #000;box-shadow:inset 0 0 1px #000;background:rgba(240,240,240,0.5)}div.output_prompt{color:darkred}div.output_area{padding:0;page-break-inside:avoid;display:-webkit-box;-webkit-box-orient:horizontal;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:horizontal;-moz-box-align:stretch;display:box;box-orient:horizontal;box-align:stretch;display:flex;flex-direction:row;align-items:stretch}div.output_area .MathJax_Display{text-align:left !important}div.output_area div.output_area .output{display:-webkit-box;-webkit-box-orient:vertical;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:vertical;-moz-box-align:stretch;display:box;box-orient:vertical;box-align:stretch;display:flex;flex-direction:column;align-items:stretch}@media (max-width:540px){div.output_area{display:-webkit-box;-webkit-box-orient:vertical;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:vertical;-moz-box-align:stretch;display:box;box-orient:vertical;box-align:stretch;display:flex;flex-direction:column;align-items:stretch}}div.output_area pre{margin:0;padding:0;border:0;vertical-align:baseline;color:black;background-color:transparent;border-radius:0}div.output_subarea{padding:.4em;-webkit-box-flex:1;-moz-box-flex:1;box-flex:1;flex:1}div.output_text{text-align:left;line-height:1.21429em}div.output_stderr{background:#fdd}div.output_latex{text-align:left}div.output_javascript:empty{padding:0}.js-error{color:darkred}div.raw_input_container{font-family:monospace;padding-top:5px}input.raw_input{font-family:inherit;font-size:inherit;color:inherit;width:auto;vertical-align:baseline;padding:0 .25em;margin:0 .25em}input.raw_input:focus{box-shadow:none}p.p-space{margin-bottom:10px}div.output_unrecognized{padding:5px;font-weight:bold;color:red}div.output_unrecognized a{color:inherit;text-decoration:none}div.output_unrecognized a:hover{color:inherit;text-decoration:none}.rendered_html{}.rendered_html :link{text-decoration:underline}.rendered_html :visited{text-decoration:underline}.rendered_html h1:first-child{margin-top:.538em}.rendered_html h2:first-child{margin-top:.636em}.rendered_html h3:first-child{margin-top:.777em}.rendered_html h4:first-child{margin-top:1em}.rendered_html h5:first-child{margin-top:1em}.rendered_html h6:first-child{margin-top:1em}.rendered_html *+ul{margin-top:1em}.rendered_html *+ol{margin-top:1em}.rendered_html *+table{margin-top:1em}.rendered_html *+p{margin-top:1em}.rendered_html *+img{margin-top:1em}div.text_cell{display:-webkit-box;-webkit-box-orient:horizontal;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:horizontal;-moz-box-align:stretch;display:box;box-orient:horizontal;box-align:stretch;display:flex;flex-direction:row;align-items:stretch}@media (max-width:540px){div.text_cell>div.prompt{display:none}}div.text_cell_render{outline:none;resize:none;width:inherit;border-style:none;padding:.5em .5em .5em .4em;box-sizing:border-box;-moz-box-sizing:border-box;-webkit-box-sizing:border-box}a.anchor-link:link{text-decoration:none;padding:0 20px;visibility:hidden}h1:hover .anchor-link,h2:hover .anchor-link,h3:hover .anchor-link,h4:hover .anchor-link,h5:hover .anchor-link,h6:hover .anchor-link{visibility:visible}.text_cell.rendered .input_area{display:none}.text_cell.unrendered .text_cell_render{display:none}.cm-header-1,.cm-header-2,.cm-header-3,.cm-header-4,.cm-header-5,.cm-header-6{font-weight:bold;font-family:\"Helvetica Neue\",Helvetica,Arial,sans-serif}.cm-header-1{font-size:185.7%}.cm-header-2{font-size:157.1%}.cm-header-3{font-size:128.6%}.cm-header-4{font-size:110%}.cm-header-5{font-size:100%;font-style:italic}.cm-header-6{font-size:100%;font-style:italic}.widget-interact>div,.widget-interact>input{padding:2.5px}.widget-area{page-break-inside:avoid;display:-webkit-box;-webkit-box-orient:horizontal;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:horizontal;-moz-box-align:stretch;display:box;box-orient:horizontal;box-align:stretch;display:flex;flex-direction:row;align-items:stretch}.widget-area .widget-subarea{padding:.44em .4em .4em 1px;margin-left:6px;box-sizing:border-box;-moz-box-sizing:border-box;-webkit-box-sizing:border-box;display:-webkit-box;-webkit-box-orient:vertical;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:vertical;-moz-box-align:stretch;display:box;box-orient:vertical;box-align:stretch;display:flex;flex-direction:column;align-items:stretch;-webkit-box-flex:2;-moz-box-flex:2;box-flex:2;flex:2;-webkit-box-align:start;-moz-box-align:start;box-align:start;align-items:flex-start}.widget-area.connection-problems .prompt:after{content:\"\\f127\";font-family:'FontAwesome';color:#d9534f;font-size:14px;top:3px;padding:3px}.slide-track{border:1px solid #ccc;background:#fff;border-radius:2px}.widget-hslider{padding-left:8px;padding-right:2px;overflow:visible;width:350px;height:5px;max-height:5px;margin-top:13px;margin-bottom:10px;border:1px solid #ccc;background:#fff;border-radius:2px;display:-webkit-box;-webkit-box-orient:horizontal;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:horizontal;-moz-box-align:stretch;display:box;box-orient:horizontal;box-align:stretch;display:flex;flex-direction:row;align-items:stretch}.widget-hslider .ui-slider{border:0;background:none;display:-webkit-box;-webkit-box-orient:horizontal;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:horizontal;-moz-box-align:stretch;display:box;box-orient:horizontal;box-align:stretch;display:flex;flex-direction:row;align-items:stretch;-webkit-box-flex:1;-moz-box-flex:1;box-flex:1;flex:1}.widget-hslider .ui-slider .ui-slider-handle{width:12px;height:28px;margin-top:-8px;border-radius:2px}.widget-hslider .ui-slider .ui-slider-range{height:12px;margin-top:-4px;background:#eee}.widget-vslider{padding-bottom:5px;overflow:visible;width:5px;max-width:5px;height:250px;margin-left:12px;border:1px solid #ccc;background:#fff;border-radius:2px;display:-webkit-box;-webkit-box-orient:vertical;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:vertical;-moz-box-align:stretch;display:box;box-orient:vertical;box-align:stretch;display:flex;flex-direction:column;align-items:stretch}.widget-vslider .ui-slider{border:0;background:none;margin-left:-4px;margin-top:5px;display:-webkit-box;-webkit-box-orient:vertical;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:vertical;-moz-box-align:stretch;display:box;box-orient:vertical;box-align:stretch;display:flex;flex-direction:column;align-items:stretch;-webkit-box-flex:1;-moz-box-flex:1;box-flex:1;flex:1}.widget-vslider .ui-slider .ui-slider-handle{width:28px;height:12px;margin-left:-9px;border-radius:2px}.widget-vslider .ui-slider .ui-slider-range{width:12px;margin-left:-1px;background:#eee}.widget-text{width:350px;margin:0}.widget-listbox{width:350px;margin-bottom:0}.widget-numeric-text{width:150px;margin:0}.widget-progress{margin-top:6px;min-width:350px}.widget-progress .progress-bar{-webkit-transition:none;-moz-transition:none;-ms-transition:none;-o-transition:none;transition:none}.widget-combo-btn{min-width:125px}.widget_item .dropdown-menu li a{color:inherit}.widget-hbox{display:-webkit-box;-webkit-box-orient:horizontal;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:horizontal;-moz-box-align:stretch;display:box;box-orient:horizontal;box-align:stretch;display:flex;flex-direction:row;align-items:stretch}.widget-hbox input[type=\"checkbox\"]{margin-top:9px;margin-bottom:10px}.widget-hbox .widget-label{min-width:10ex;padding-right:8px;padding-top:5px;text-align:right;vertical-align:text-top}.widget-hbox .widget-readout{padding-left:8px;padding-top:5px;text-align:left;vertical-align:text-top}.widget-vbox{display:-webkit-box;-webkit-box-orient:vertical;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:vertical;-moz-box-align:stretch;display:box;box-orient:vertical;box-align:stretch;display:flex;flex-direction:column;align-items:stretch}.widget-vbox .widget-label{padding-bottom:5px;text-align:center;vertical-align:text-bottom}.widget-vbox .widget-readout{padding-top:5px;text-align:center;vertical-align:text-top}.widget-box{box-sizing:border-box;-moz-box-sizing:border-box;-webkit-box-sizing:border-box;-webkit-box-align:start;-moz-box-align:start;box-align:start;align-items:flex-start}.widget-radio-box{display:-webkit-box;-webkit-box-orient:vertical;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:vertical;-moz-box-align:stretch;display:box;box-orient:vertical;box-align:stretch;display:flex;flex-direction:column;align-items:stretch;box-sizing:border-box;-moz-box-sizing:border-box;-webkit-box-sizing:border-box;padding-top:4px}.widget-radio-box label{margin-top:0}.widget-radio{margin-left:20px} .highlight .hll { background-color: #ffffcc } .highlight { background: #f8f8f8; } .highlight .c { color: #408080; font-style: italic } /* Comment */ .highlight .err { border: 1px solid #FF0000 } /* Error */ .highlight .k { color: #008000; font-weight: bold } /* Keyword */ .highlight .o { color: #666666 } /* Operator */ .highlight .cm { color: #408080; font-style: italic } /* Comment.Multiline */ .highlight .cp { color: #BC7A00 } /* Comment.Preproc */ .highlight .c1 { color: #408080; font-style: italic } /* Comment.Single */ .highlight .cs { color: #408080; font-style: italic } /* Comment.Special */ .highlight .gd { color: #A00000 } /* Generic.Deleted */ .highlight .ge { font-style: italic } /* Generic.Emph */ .highlight .gr { color: #FF0000 } /* Generic.Error */ .highlight .gh { color: #000080; font-weight: bold } /* Generic.Heading */ .highlight .gi { color: #00A000 } /* Generic.Inserted */ .highlight .go { color: #888888 } /* Generic.Output */ .highlight .gp { color: #000080; font-weight: bold } /* Generic.Prompt */ .highlight .gs { font-weight: bold } /* Generic.Strong */ .highlight .gu { color: #800080; font-weight: bold } /* Generic.Subheading */ .highlight .gt { color: #0044DD } /* Generic.Traceback */ .highlight .kc { color: #008000; font-weight: bold } /* Keyword.Constant */ .highlight .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */ .highlight .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */ .highlight .kp { color: #008000 } /* Keyword.Pseudo */ .highlight .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */ .highlight .kt { color: #B00040 } /* Keyword.Type */ .highlight .m { color: #666666 } /* Literal.Number */ .highlight .s { color: #BA2121 } /* Literal.String */ .highlight .na { color: #7D9029 } /* Name.Attribute */ .highlight .nb { color: #008000 } /* Name.Builtin */ .highlight .nc { color: #0000FF; font-weight: bold } /* Name.Class */ .highlight .no { color: #880000 } /* Name.Constant */ .highlight .nd { color: #AA22FF } /* Name.Decorator */ .highlight .ni { color: #999999; font-weight: bold } /* Name.Entity */ .highlight .ne { color: #D2413A; font-weight: bold } /* Name.Exception */ .highlight .nf { color: #0000FF } /* Name.Function */ .highlight .nl { color: #A0A000 } /* Name.Label */ .highlight .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */ .highlight .nt { color: #008000; font-weight: bold } /* Name.Tag */ .highlight .nv { color: #19177C } /* Name.Variable */ .highlight .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */ .highlight .w { color: #bbbbbb } /* Text.Whitespace */ .highlight .mb { color: #666666 } /* Literal.Number.Bin */ .highlight .mf { color: #666666 } /* Literal.Number.Float */ .highlight .mh { color: #666666 } /* Literal.Number.Hex */ .highlight .mi { color: #666666 } /* Literal.Number.Integer */ .highlight .mo { color: #666666 } /* Literal.Number.Oct */ .highlight .sb { color: #BA2121 } /* Literal.String.Backtick */ .highlight .sc { color: #BA2121 } /* Literal.String.Char */ .highlight .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */ .highlight .s2 { color: #BA2121 } /* Literal.String.Double */ .highlight .se { color: #BB6622; font-weight: bold } /* Literal.String.Escape */ .highlight .sh { color: #BA2121 } /* Literal.String.Heredoc */ .highlight .si { color: #BB6688; font-weight: bold } /* Literal.String.Interpol */ .highlight .sx { color: #008000 } /* Literal.String.Other */ .highlight .sr { color: #BB6688 } /* Literal.String.Regex */ .highlight .s1 { color: #BA2121 } /* Literal.String.Single */ .highlight .ss { color: #19177C } /* Literal.String.Symbol */ .highlight .bp { color: #008000 } /* Name.Builtin.Pseudo */ .highlight .vc { color: #19177C } /* Name.Variable.Class */ .highlight .vg { color: #19177C } /* Name.Variable.Global */ .highlight .vi { color: #19177C } /* Name.Variable.Instance */ .highlight .il { color: #666666 } /* Literal.Number.Integer.Long */ Probabilistic Matrix Factorization for Making Personalized Recommendations Igor Marfin igor.marfin@unister.de Abstract The model discussed in this analysis was presented in the PyMC3 tutorial page . The aim of the work is implementaion and testing the Probabilistic Matrix Factorization model in terms of the Bayesian network with a help of the PyMC2. More details can be found at https://bitbucket.org/iggy_floyd/probabilistic-matrix-factorization . Motivation Say I download a handbook of a hundred jokes, and I'd like to know very quickly which ones will be my favorite. So maybe I read a few, I laugh, I read a few more, I stop laughing, and I indicate on a scale of -10 to 10 how funny I thought each joke was. Maybe I do this for 5 jokes out of the 100. Now I go to the back of the book, and there's a little program included for calculating my preferences for all the other jokes. I enter in my preference numbers and shazam! The program spits out a list of all 100 jokes, sorted in the order I'll like them. Some thoughts I split the making recomendations procedure in four ideas given in the tutorial: preparation for a recommendation: introduction of the ratings for each joke in different categories Normally if we want recommendations for something, we try to find people who are similar to us and ask their opinions. If Bob, Alice, and Monty are all similar to me, and they all like knock-knock jokes, I'll probably like knock-knock jokes. Now this isn't always true. It depends on what we consider to be \"similar\". In order to get the best bang for our buck, we really want to look for people who have the most similar sense of humor. Humor being a complex beast, we'd probably like to break it down into something more understandable. We might try to characterize each joke in terms of various factors. Perhaps jokes can be dry, sarcastic, crude, sexual, political, etc. Now imagine we go through our handbook of jokes and assign each joke a rating in each of the categories. How dry is it? How sarcastic is it? How much does it use sexual innuendos? Perhaps we use numbers between 0 and 1 for each category. Intuitively, we might call this the joke's humor profile. a social recommendation: make a decision using the recommendation from the closest social individuals (based on user-user similarity) Now let's suppose we go back to those 5 jokes we rated. At this point, we can get a richer picture of our own preferences by looking at the humor profiles of each of the jokes we liked and didn't like. Perhaps we take the averages across the 5 humor profiles and call this our ideal type of joke. In other words, we have computed some notion of our inherent preferences for various types of jokes. Suppose Bob, Alice, and Monty all do the same. Now we can compare our preferences and determine how similar each of us really are. I might find that Bob is the most similar and the other two are still more similar than other people, but not as much as Bob. So I want recommendations from all three people, but when I make my final decision, I'm going to put more weight on Bob's recommendation than those I get from Alice and Monty. an individual recommendation: make a decision using ourselves recommendations with the closest humor profiles (based on item-item similarity) While the above procedure sounds fairly effective as is, it also reveals an unexpected additional source of information. If we rated a particular joke highly, and we know its humor profile, we can compare with the profiles of other jokes. If we find one with very close numbers, it is probable we'll also enjoy this joke. Both this approach and the one above are commonly known as neighborhood approaches. Techniques that leverage both of these approaches simultaneously are often called collaborative filtering. combination of the two previous recommendation strategies Ideally, we'd like to use both sources of information. The idea is we have a lot of items available to us, and we'd like to work together with others to filter the list of items down to those we'll each like best. My list should have the items I'll like best at the top and those I'll like least at the bottom. Everyone else wants the same. If I get together with a bunch of other people, we all read 5 jokes, and we have some efficient computational process to determine similarity, we can very quickly order the jokes to our liking. Formalization A little bit of mathematics I begin with introduction of some notations used later in the code. They are $R$,$R&#94;*$, $U$, $V$,$I$. Let's take some time to make the intuitive notions we've been discussing more concrete. We have a set of M jokes, or items (M=100 in our example above). We also have N people, whom we'll call users of our recommender system. For each item, we'd like to find a D dimensional factor composition (humor profile above) to describe the item. Ideally, we'd like to do this without actually going through and manually labeling all of the jokes. Manual labeling would be both slow and error-prone, as different people will likely label jokes differently. So we model each joke as a D dimensional vector, which is its latent factor composition. Furthermore, we expect each user to have some preferences, but without our manual labeling and averaging procedure, we have to rely on the latent factor compositions to learn D dimensional latent preference vectors for each user. The only thing we get to observe is the N×M ratings matrix R provided by the users. Important to remember: Entry $R_{ij}$ is the rating user $i$ gave to item $j$. Many of these entries may be missing, since most users will not have rated all 100 jokes. Our goal is to fill in the missing values with predicted ratings based on the latent variables U and V. We denote the predicted ratings by $R&#94;∗_{ij}$. Important to remember: calculation of $R&#94;∗_{ij}$ is our aim, this what we model and estimate. We also define an indicator matrix I, with entry $I_{ij}=0$ if $R_{ij}$ is missing and $I_{ij}=1$ otherwise. Important: please carefully read the paragraph below So we have an N×D matrix of user preferences which we'll call U and an M×D factor composition matrix we'll call V. We also have a N×M rating matrix we'll call R. We can think of each row $U_i$ as indications of how much each user prefers each of the D latent factors. Each row $V_j$ can be thought of as how much each item can be described by each of the latent factors. In order to make a recommendation, we need a suitable prediction function which maps a user preference vector $U_i$ and an item latent factor vector $V_j$ to a predicted ranking. The choice of this prediction function is an important modeling decision, and a variety of prediction functions have been used. Perhaps the most common is the dot product of the two vectors, $U_i\\cdot V_j$ Important to remember: We calculate $R&#94;∗_{ij}$ as $R&#94;∗_{ij} = U\\cdot V&#94;T$ Also you have to understand that we replace $R_{ij}$ by $R&#94;∗_{ij}$ when we are going to make a decision on the recommendation of the particular joke. Why we do that? Well, because, the matrix $R_{ij}$ usually is the sparse matrix with many NaN values and we use this minimalistic set of the data to get a fully determined new rating matrix $R&#94;*_{ij}$. Initialization In [1]: import sys sys . path = [ '/usr/local/lib/python2.7/dist-packages' ] + sys . path # to fix the problem with numpy: this replaces 1.6 version by 1.9 % matplotlib inline % pylab inline import os import matplotlib import numpy as np import matplotlib.pyplot as pl import matplotlib as mpl import logging import pymc as pm # a plotter and dataframe modules import seaborn as sns # seaborn to make a nice plots of the data import pandas as pd # use a nice style for plots and the notebook import json s = json . load ( open ( \"styles/my_matplotlibrc.json\" ) ) matplotlib . rcParams . update ( s ) from IPython.core.display import HTML from IPython.display import display , Math , Latex import urllib2 def css_styling (): styles = open ( \"styles/custom_v3.css\" , \"r\" ) . read () return HTML ( styles ) css_styling () #HTML( urllib2.urlopen('http://bit.ly/1Bf5Hft').read() ) ion () # Set up logging. logger = logging . getLogger () logger . setLevel ( logging . INFO ) Populating the interactive namespace from numpy and matplotlib /usr/local/lib/python2.7/dist-packages/matplotlib/__init__.py:855: UserWarning: svg.embed_char_paths is deprecated and replaced with svg.fonttype; please use the latter. warnings.warn(self.msg_depr % (key, alt_key)) Data The v1 Jester dataset provides something very much like the handbook of jokes we have been discussing. At this point in time, v1 contains over 4.1 million continuous ratings in the range [-10, 10] of 100 jokes from 73,421 users. These ratings were collected between Apr. 1999 and May 2003. In order to reduce the training time of the model for illustrative purposes, 1,000 users who have rated all 100 jokes will be selected randomly. Let's get our data. In [7]: ! mkdir data ! wget -O data/jester-dataset-v1-dense-first-1000.csv https://raw.githubusercontent.com/pymc-devs/pymc3/master/pymc3/examples/data/jester-dataset-v1-dense-first-1000.csv ! ls data --2015-05-28 11:10:57-- https://raw.githubusercontent.com/pymc-devs/pymc3/master/pymc3/examples/data/jester-dataset-v1-dense-first-1000.csv Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.31.17.133 Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.31.17.133|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 529543 (517K) [text/plain] Saving to: `data/jester-dataset-v1-dense-first-1000.csv' 100%[======================================>] 529,543 352K/s in 1.5s 2015-05-28 11:11:00 (352 KB/s) - `data/jester-dataset-v1-dense-first-1000.csv' saved [529543/529543] jester-dataset-v1-dense-first-1000.csv In [2]: data = pd . read_csv ( 'data/jester-dataset-v1-dense-first-1000.csv' ) data . head ( 10 ) Out[2]: 1 2 3 4 5 6 7 8 9 10 ... 91 92 93 94 95 96 97 98 99 100 0 4.08 -0.29 6.36 4.37 -2.38 -9.66 -0.73 -5.34 8.88 9.22 ... 2.82 -4.95 -0.29 7.86 -0.19 -2.14 3.06 0.34 -4.32 1.07 1 -6.17 -3.54 0.44 -8.50 -7.09 -4.32 -8.69 -0.87 -6.65 -1.80 ... -3.54 -6.89 -0.68 -2.96 -2.18 -3.35 0.05 -9.08 -5.05 -3.45 2 6.84 3.16 9.17 -6.21 -8.16 -1.70 9.27 1.41 -5.19 -4.42 ... 7.23 -1.12 -0.10 -5.68 -3.16 -3.35 2.14 -0.05 1.31 0.00 3 -3.79 -3.54 -9.42 -6.89 -8.74 -0.29 -5.29 -8.93 -7.86 -1.60 ... 4.37 -0.29 4.17 -0.29 -0.29 -0.29 -0.29 -0.29 -3.40 -4.95 4 1.31 1.80 2.57 -2.38 0.73 0.73 -0.97 5.00 -7.23 -1.36 ... 1.46 1.70 0.29 -3.30 3.45 5.44 4.08 2.48 4.51 4.66 5 9.22 9.27 9.22 8.30 7.43 0.44 3.50 8.16 5.97 8.98 ... 8.11 -1.02 5.58 6.84 5.53 -5.92 8.20 8.98 -8.16 6.50 6 8.79 -5.78 6.02 3.69 7.77 -5.83 8.69 8.59 -5.92 7.52 ... 2.72 -5.49 -8.59 8.69 -8.74 -3.01 8.30 -4.81 -2.38 -5.97 7 -3.50 1.55 2.33 -4.13 4.22 -2.28 -2.96 -0.49 2.91 1.99 ... 3.11 1.70 0.24 -5.92 7.28 -1.36 3.74 2.82 -2.86 3.45 8 3.16 7.62 3.79 8.25 4.22 7.62 2.43 0.97 0.53 0.83 ... 0.83 5.68 3.69 0.19 0.29 3.59 0.49 8.06 0.49 7.62 9 2.09 -7.57 4.17 -8.40 -6.31 5.34 -5.19 -5.05 -8.20 2.28 ... 3.06 8.25 8.50 -9.51 8.45 8.16 7.82 8.50 8.35 -8.35 10 rows × 100 columns How many ratings do we have? In [9]: print '# of ratings: %d ' % len (data) # of ratings: 1000 Now we want to get the distribution of ratings: we plot a histogram and get some statistics from it In [13]: # Extract the ratings from the DataFrame all_ratings = np . ndarray . flatten ( data . values ) ratings = pd . Series ( all_ratings ) # Plot a density. fig , ( ax1 , ax2 ) = plt . subplots ( 1 , 2 , figsize = ( 16 , 7 )) ratings . plot ( kind = 'density' , ax = ax1 , grid = False ) ax1 . set_ylim ( 0 , 0.08 ) ax1 . set_xlim ( - 11 , 11 ) # Plot a histogram ratings . plot ( kind = 'hist' , ax = ax2 , bins = 20 , grid = False ) ax2 . set_xlim ( - 11 , 11 ) plt . show () # get some statistics ratings . describe () Out[13]: count 100000.000000 mean 0.996219 std 5.265215 min -9.950000 25% -2.860000 50% 1.650000 75% 5.290000 max 9.420000 dtype: float64 So we can find that the mean of all ratings is close to 1 (almost neutral). The distribution of mean values ( the average on the number of ratings) among jokes is In [16]: # mean ratings for 100 jokes joke_means = data . mean ( axis = 0 ) joke_means . plot ( kind = 'bar' , grid = False , figsize = ( 16 , 6 ), title = \"Mean Ratings for All 100 Jokes\" ) Out[16]: <matplotlib.axes._subplots.AxesSubplot at 0xad40414c> While the majority of the jokes generally get positive feedback from users, there are definitely a few that stand out as poor humor. Let's take a look at the users means. In [17]: # user means user_means = data . mean ( axis = 1 ) fig , ax = plt . subplots ( figsize = ( 16 , 6 )) user_means . plot ( kind = 'bar' , grid = False , ax = ax , title = \"Mean Ratings for All 1000 Users\" ) ax . set_xticklabels ( '' ) # 1000 labels is nonsensical fig . show () /usr/local/lib/python2.7/dist-packages/matplotlib/figure.py:387: UserWarning: matplotlib is currently using a non-GUI backend, so cannot show the figure \"matplotlib is currently using a non-GUI backend, \" We see even more significant trends here. Some users rate nearly everything highly, and some (though not as many) rate nearly everything negatively. These observations will come in handy when considering models to use for predicting user preferences on unseen jokes. Methods We're now ready to dig in and start addressing the problem. We want to predict how much each user is going to like all of the jokes he or she has not yet read, i.e. we want estimate $R&#94;*_{ij}$. Baselines Every good analysis needs some kind of baseline methods to compare against. It's difficult to claim we've produced good results if we have no reference point for what defines \"good\". We'll define three very simple baseline methods and find the RMSE using these methods. Our goal will be to obtain lower RMSE scores with whatever model we produce Important to remember: Baselines are different methods to calculate $R&#94;*_{ij}$ which are compared then. Uniform Random Baseline $R&#94;*_{ij} \\sim Uniform(min\\_rating,max\\_rating)$ Global Mean Baseline $R&#94;*_{ij} \\sim global\\_mean = \\frac{1}{N\\times M} \\sum_{i=1}&#94;N\\sum_{j=1}&#94;M I_{ij}(R_{ij})$, where $I_{ij}$ is the sparse mask of the $R_{ij}$. Mean of the Means Baseline $R&#94;*_{ij} = \\frac{1}{3}(user\\_means_i + joke\\_means_j + global\\_mean)$, $user\\_means_i =\\frac{1}{M} \\sum_{j=1}&#94;M I_{ij}(R_{ij}) $, $joke\\_means_j =\\frac{1}{N} \\sum_{i=1}&#94;N I_{ij}(R_{ij}) $ Let's make a Baseline class which will be the abstact class for all baselines. In [3]: class Baseline ( object ): \"\"\"Calculate baseline predictions.\"\"\" def __init__ ( self , train_data ): \"\"\"Simple heuristic-based transductive learning to fill in missing values in data matrix.\"\"\" self . predict ( train_data . copy ()) def predict ( self , train_data ): raise NotImplementedError ( 'baseline prediction not implemented for base class' ) def rmse ( self , test_data ): \"\"\"Calculate root mean squared error for predictions on test data.\"\"\" # rmse is a global function defined later return rmse ( test_data , self . predicted ) def split_title ( self , title ): \"\"\"Change \"BaselineMethod\" to \"Baseline Method\".\"\"\" words = [] tmp = [ title [ 0 ]] for c in title [ 1 :]: if c . isupper (): words . append ( '' . join ( tmp )) tmp = [ c ] else : tmp . append ( c ) words . append ( '' . join ( tmp )) return ' ' . join ( words ) def __str__ ( self ): return self . split_title ( self . __class__ . __name__ ) Here we implement three basic baselines discussed previuosly. Uniform Random Baseline In [4]: class UniformRandomBaseline ( Baseline ): \"\"\"Fill missing values with uniform random values.\"\"\" def predict ( self , train_data ): # get the mask where 'True' indicates np.nan in the data nan_mask = np . isnan ( train_data ) # get masked_array in order to find min and max values masked_train = np . ma . masked_array ( train_data , nan_mask ) # get min/max values for uniform distribution pmin , pmax = masked_train . min (), masked_train . max () # ge the number of 'True' in the mask N = nan_mask . sum () #replace all np.nan in the pandas dataframe by the uniformly generated value train_data [ nan_mask ] = np . random . uniform ( pmin , pmax , N ) self . predicted = train_data Global Mean Baseline In [5]: class GlobalMeanBaseline ( Baseline ): \"\"\"Fill in missing values using the global mean.\"\"\" def predict ( self , train_data ): nan_mask = np . isnan ( train_data ) train_data [ nan_mask ] = train_data [ ~ nan_mask ] . mean () self . predicted = train_data Mean of the Means Baseline In [6]: class MeanOfMeansBaseline ( Baseline ): \"\"\"Fill in missing values using mean of user/item/global means.\"\"\" def predict ( self , train_data ): # get a mask nan_mask = np . isnan ( train_data ) # get a masked_array to find out min, max, mean values. masked_train = np . ma . masked_array ( train_data , nan_mask ) # the global mean: a mean value of the masked_array global_mean = masked_train . mean () # using different axis, we can get 'user' and 'joke' means user_means = masked_train . mean ( axis = 1 ) # among jokes of one user joke_means = masked_train . mean ( axis = 0 ) # among users self . predicted = train_data . copy () n , m = train_data . shape for i in xrange ( n ): for j in xrange ( m ): if np . ma . isMA ( joke_means [ j ]): # it return true if joke_means[j] doesn't contain np.nan self . predicted [ i , j ] = np . mean ( ( global_mean , user_means [ i ])) else : self . predicted [ i , j ] = np . mean ( ( global_mean , user_means [ i ], joke_means [ j ])) Now, let's put all baselines in one dictionary, in order to test them later In [7]: from collections import OrderedDict baseline_methods = OrderedDict () baseline_methods [ 'ur' ] = UniformRandomBaseline baseline_methods [ 'gm' ] = GlobalMeanBaseline baseline_methods [ 'mom' ] = MeanOfMeansBaseline RMSE We're now ready to dig in and start addressing the problem. We want to predict how much each user is going to like all of the jokes he or she has not yet read, i.e. we want estimate $R&#94;*_{ij}$. The way we are going to use in order to test baselines and PMF bayesian framework is very simple: we will use the RMSE (root mean squared error) for predictions on test data and the method with the smallest RMSE would be defined as the best $RMSE = \\sqrt{\\frac{ \\sum_{i=1}&#94;N\\sum_{j=1}&#94;M I_{ij}(R_{ij}-R&#94;*_{ij})&#94;2}{\\sum_{i=1}&#94;N\\sum_{j=1}&#94;M I_{ij}}}$ Important to understand: the we use the test data to calculate $RMSE$. In [8]: # Define our evaluation function. def rmse ( test_data , predicted ): \"\"\"Calculate root mean squared error. Ignoring missing values in the test data. \"\"\" I = ~ np . isnan ( test_data ) # indicator for missing values N = I . sum () # number of non-missing values sqerror = abs ( test_data - predicted ) ** 2 # squared error array mse = sqerror [ I ] . sum () / N # mean squared error return np . sqrt ( mse ) Train/Test data Originally, authors of the tutorial say: So we produce a test set by taking a random sample of the cells in the full N×M data matrix $R_{ij}$. The values selected as test samples are replaced with nan values in a copy of the original data matrix to produce the training set. Since we'll be producing random splits, let's also write out the train/test sets generated. This will allow us to replicate our results. We'd like to be able to idenfity which split is which, so we'll take a hash of the indices selected for testing and use that to save the data. But do you understand this statement properly? I will try to summarize all points here: Use the origianl $R_{ij}$ uploaded at the beginning. Then randomly choose N cells in $R_{ij}$ and replace them by np.nan. This is a train dataset. The origianl $R_{ij}$ is a test dataset. Then create a hash code for chosen indexes and store it to the file. We can later to replicate the train/test splitting using this hash code. In [9]: import hashlib try : import ujson as json except ImportError : import json # First, define functions to save our parameters to the file. def save_np_vars ( vars , savedir ): \"\"\"Save a dictionary of numpy variables to `savedir`. We assume the directory does not exist; an OSError will be raised if it does. \"\"\" logging . info ( 'writing numpy vars to directory: %s ' % savedir ) os . mkdir ( savedir ) shapes = {} for varname in vars : data = vars [ varname ] var_file = os . path . join ( savedir , varname + '.txt' ) np . savetxt ( var_file , data . reshape ( - 1 , data . size )) shapes [ varname ] = data . shape ## Store shape information for reloading. shape_file = os . path . join ( savedir , 'shapes.json' ) with open ( shape_file , 'w' ) as sfh : json . dump ( shapes , sfh ) # Define the function to upload our parameters from the file def load_np_vars ( savedir ): \"\"\"Load numpy variables saved with `save_np_vars`.\"\"\" shape_file = os . path . join ( savedir , 'shapes.json' ) with open ( shape_file , 'r' ) as sfh : shapes = json . load ( sfh ) vars = {} for varname , shape in shapes . items (): var_file = os . path . join ( savedir , varname + '.txt' ) vars [ varname ] = np . loadtxt ( var_file ) . reshape ( shape ) return vars # Define a function for splitting train/test data. def split_train_test ( data , percent_test = 0.1 ): \"\"\"Split the data into train/test sets. :param int percent_test: Percentage of data to use for testing. Default 10%. \"\"\" n , m = data . shape # num. of users, num. of jokes N = n * m # num. of cells in matrix test_size = N * percent_test # use 10% of data as test set train_size = N - test_size # and remainder for training # Prepare train/test ndarrays. train = data . copy () . values # original Rij's values -> this is our 'train' dataset test = np . ones ( train . shape ) * np . nan # our 'test' dataset is the Rij matrix with np.nan everywhere # Draw random sample of training data to use for testing. #tosample = np.where(~np.isnan(train)) # ignore u nan values in data tosample = ( ~ np . isnan ( train )) . nonzero () # nonzero() return a pair of indexes for non np.nan values idx_pairs = zip ( tosample [ 0 ], tosample [ 1 ]) # tuples of row/col index pairs indices = np . arange ( len ( idx_pairs )) # indices of index pairs sample = np . random . choice ( indices , replace = False , size = test_size ) # here we use the sampling scheme without replacement # Transfer random sample from train set to test set. for idx in sample : idx_pair = idx_pairs [ idx ] test [ idx_pair ] = train [ idx_pair ] # transfer to test set train [ idx_pair ] = np . nan # remove from train set # Verify everything worked properly # what we have now: the train dataset contains np.nan at positions defined by the randomly selected indexes # the test dataset has all defined values at these positions but others are np.nan assert ( np . isnan ( train ) . sum () == test_size ) assert ( np . isnan ( test ) . sum () == train_size ) # Finally, hash the indices and save the train/test sets. index_string = '' . join ( map ( str , np . sort ( sample ))) name = hashlib . sha1 ( index_string ) . hexdigest () # saving train/test datasets to the folder 'name' savedir = os . path . join ( 'data' , name ) save_np_vars ({ 'train' : train , 'test' : test }, savedir ) print 'train/test datsets were stored in %s folder' % savedir # Return train set, test set, and unique hash of indices. return train , test , name # Finally, define the function to upload def load_train_test ( name ): \"\"\"Load the train/test sets.\"\"\" savedir = os . path . join ( 'data' , name ) vars = load_np_vars ( savedir ) return vars [ 'train' ], vars [ 'test' ] Later, we are going to use Bayesian approach to calculate Probability Matrix $R_{ij}$. Becasuse, we intend to use D=5, the dimension for hidden variables, it will bring D(N + M) = 5500 latent variables. It will take a huge memory to process the model. So, we restrict ourselves to 50 users, and 20 jokes: In [10]: data = pd . read_csv ( 'data/jester-dataset-v1-dense-first-1000.csv' ) data = data . loc [: 50 , data . columns . values [: 20 ]] In [11]: # test of splitting train , test , split_hash = split_train_test ( data ) train_reload , test_reload = load_train_test ( split_hash ) assert ( np . isnan ( train ) . sum () == np . isnan ( train_reload ) . sum () ) assert ( np . isnan ( test ) . sum () == np . isnan ( test_reload ) . sum () ) INFO:root:writing numpy vars to directory: data/9707154a0142b6aa18153ba96ee20c36f647bbf8 train/test datsets were stored in data/9707154a0142b6aa18153ba96ee20c36f647bbf8 folder Performance of the Baselines Now we can train our baselines, calculate their RMSE and compare them In [12]: # Let's see the results: train , test = load_train_test ( split_hash ) baselines = {} for name in baseline_methods : Method = baseline_methods [ name ] method = Method ( train ) baselines [ name ] = method . rmse ( test ) print ' %s RMSE: \\t %.5f ' % ( method , baselines [ name ]) # let's plot results size = 100 results = pd . DataFrame ({ 'uniform random' : np . repeat ( baselines [ 'ur' ], size ), 'global means' : np . repeat ( baselines [ 'gm' ], size ), 'mean of means' : np . repeat ( baselines [ 'mom' ], size ), }) fig , ax = plt . subplots ( figsize = ( 10 , 5 )) results . plot ( kind = 'line' , grid = False , ax = ax , title = 'RMSE for all methods' ) ax . set_xlabel ( \"Number of Samples\" ) ax . set_ylabel ( \"RMSE\" ) Uniform Random Baseline RMSE: 8.21210 Global Mean Baseline RMSE: 4.99941 Mean Of Means Baseline RMSE: 4.76052 Out[12]: <matplotlib.text.Text at 0xb6c32ec> Probabilistic Matrix Factorization Probabilistic Matrix Factorization (PMF) [3] is a probabilistic approach to the collaborative filtering problem that takes a Bayesian perspective. The ratings R are modeled as draws from a Gaussian distribution. The mean for $R_{ij}$ is $U_i\\cdot V&#94;T_j$. The precision α is a fixed parameter that reflects the uncertainty of the estimations; the normal distribution is commonly reparameterized in terms of precision, which is the inverse of the variance. Complexity is controlled by placing zero-mean spherical Gaussian priors on U and V. In other words, each $i&#94;{th}$ row of $U$ is drawn from a multivariate Gaussian with mean $\\mu_U=0$ and precision $\\alpha_U$ which is some multiple of the identity matrix I. Each $j&#94;{th}$ row of $V$ is drawn from a multivariate Gaussian with mean $\\mu_V=0$ and precision $\\alpha_V$ which is some multiple of the identity matrix I. Let's try to define the class realizing the model of PMF In [13]: class PMF ( object ): \"\"\"Probabilistic Matrix Factorization model using pymc3.\"\"\" def __init__ ( self , train , dim , alpha = 2 , std = 0.01 , bounds = ( - 10 , 10 )): \"\"\"Build the Probabilistic Matrix Factorization model using pymc2. :param np.ndarray train: The training data to use for learning the model. :param int dim: Dimensionality of the model; number of latent factors. :param int alpha: Fixed precision for the likelihood function. :param float std: Amount of noise to use for model initialization. :param (tuple of int) bounds: (lower, upper) bound of ratings. These bounds will simply be used to cap the estimates produced for R. \"\"\" self . dim = dim self . alpha = alpha self . std = np . sqrt ( 1.0 / alpha ) self . bounds = bounds # occupy a lot of memory #self.data = train.copy() self . data = train n , m = self . data . shape # Perform mean value imputation nan_mask = np . isnan ( self . data ) self . data [ nan_mask ] = self . data [ ~ nan_mask ] . mean () # Low precision reflects uncertainty; prevents overfitting. # Set to the mean variance across users and items. # var() gives std. variation self . alpha_u = 1 / self . data . var ( axis = 1 ) . mean () self . alpha_v = 1 / self . data . var ( axis = 0 ) . mean () # Specify the model. logging . info ( 'building the PMF model' ) # define our bayesian model here # define the U_i vector self . Ui = [ pm . MvNormal ( 'U %i ' % i ,np.zeros(dim),self.alpha_u * np.eye(dim)) for i in range(n) ] # define the V_j vector self . Vj = [ pm . MvNormal ( 'V %i ' % j ,np.zeros(dim),self.alpha_v * np.eye(dim)) for j in range(m) ] # define the mean value Rij = [] for i in range ( n ): _tmp = [] for j in range ( m ): _tmp += [ pm . Lambda ( '_R %i%i ' % ( i , j ), lambda U = self . Ui [ i ], V = self . Vj [ j ]: np . dot ( U , V . T ))] Rij += [ _tmp ] # define the observed self . Rij = [] for i in range ( n ): _tmp = [] for j in range ( m ): #_tmp +=[pm.Normal('R%i%i'%(i,j),np.dot(self.Ui[i].value,self.Vj[j].value.T),self.alpha,value=self.data[i,j],observed=True)] _tmp += [ pm . Normal ( 'R %i%i ' % ( i , j ), Rij [ i ][ j ], self . alpha , value = self . data [ i , j ], observed = True )] self . Rij += [ _tmp ] self . Rij = np . array ( self . Rij ) . flatten () . tolist () # define our model self . model = pm . Model ( self . Ui + self . Vj + self . Rij ) logging . info ( 'done building the PMF model' ) def __str__ ( self ): return self . name In [14]: # Test of Bayesian PMF # We use a fixed precision for the likelihood. # This reflects uncertainty in the dot product. # We choose 2 in the footsteps Salakhutdinov # Mnihof. ALPHA = 2 # The dimensionality D; the number of latent factors. # We can adjust this higher to try to capture more subtle # characteristics of each joke. However, the higher it is, # the more expensive our inference procedures will be. # Specifically, we have D(N + M) latent variables. For our # Jester dataset, this means we have D(1100), so for 5 # dimensions, we are sampling 5500 latent variables. DIM = 5 pmf = PMF ( train , DIM , ALPHA , std = 0.05 ) ? pmf INFO:root:building the PMF model INFO:root:done building the PMF model Ok. Let's make some tunning of our PMF class. First, we add 'MAP' (maximum a posteriori) property and functionality to read it and save it to the file In [15]: import scipy as sp import time # Now define the MAP estimation infrastructure. # property 'map_dir' def _map_dir ( self ): ''' return the dir name where we store the MAP''' basename = 'pmf-map-d %d ' % self . dim return os . path . join ( 'data' , basename ) # support for the property 'map' def _find_map ( self ): \"\"\"Find mode of posterior using Powell optimization.\"\"\" tstart = time . time () logging . info ( 'finding PMF MAP using Powell optimization...' ) self . _map = pm . MAP ( self . model ) self . _map . fit () elapsed = int ( time . time () - tstart ) logging . info ( 'found PMF MAP in %d seconds' % elapsed ) #pymc2 # This is going to take a good deal of time to find, so let's save it. savedir = os . path . join ( 'data' , name ) map_to_save = {} for var in list ( self . _map . variables ): map_to_save . update ({ str ( var ): var . value }) self . _map = map_to_save save_np_vars ( self . _map , self . map_dir ) print 'MAP was stored in %s folder' % savedir # support for the property 'map' def _load_map ( self ): self . _map = load_np_vars ( self . map_dir ) def _map ( self ): try : return self . _map except : if os . path . isdir ( self . map_dir ): self . load_map () else : self . find_map () return self . _map # Update our class with the new MAP infrastructure. PMF . find_map = _find_map PMF . load_map = _load_map PMF . map_dir = property ( _map_dir ) PMF . map = property ( _map ) Let's try to find MAP In [ ]: pmf . load_map () In [130]: # Find MAP for PMF. #pmf.find_map() INFO:root:finding PMF MAP using Powell optimization... INFO:root:found PMF MAP in 435 seconds INFO:root:writing numpy vars to directory: data/pmf-map-d5 MAP was stored in data/mom folder In [132]: print pmf . map [ 'U0' ] [ 2.61198177 0.03008766 -0.46349007 1.78042112 -2.81458536] Now we would like to introduce the MCMC sampling In [41]: # Draw MCMC samples. def _trace_dir ( self ): basename = 'pmf-mcmc-d %d ' % self . dim return os . path . join ( 'data' , basename ) # Sampler def _draw_samples ( self , nsamples = 1000 , njobs = 2 ): # First make sure the trace_dir does not already exist. if os . path . isdir ( self . trace_dir ): raise OSError ( 'trace directory %s already exists. Please move or delete.' % self . trace_dir ) # pymc2 logging . info ( 'drawing %d samples using %d jobs' % ( nsamples , njobs )) self . mcmc = pm . MCMC ( self . model , db = 'txt' , dbname = self . trace_dir ) logging . info ( 'backing up trace to directory: %s ' % self . trace_dir ) self . mcmc . sample ( nsamples ) self . traces = {} for var in list ( self . mcmc . stochastics ): self . traces . update ( { str ( var ): self . mcmc . trace ( str ( var ))[:] } ) self . mcmc . db . close () def _load_trace ( self ): self . mcmc = pm . MCMC ( self . model , db = pm . database . txt . load ( self . trace_dir )) self . traces = {} for var in list ( self . mcmc . stochastics ): self . traces . update ( { str ( var ): self . mcmc . trace ( str ( var ))[:] } ) In [42]: # Update our class with the sampling infrastructure. PMF . trace_dir = property ( _trace_dir ) PMF . draw_samples = _draw_samples PMF . load_trace = _load_trace Let's try to MCMC In [21]: pmf . draw_samples ( 5000 , njobs = 3 ) INFO:root:drawing 5000 samples using 3 jobs INFO:root:backing up trace to directory: data/pmf-mcmc-d5 [-----------------100%-----------------] 5000 of 5000 complete in 152.4 sec In [46]: pmf . load_trace () Finally, we need to define the 'predict' function. For user $i$ and joke $j$, a prediction is generated by drawing from $N(U_i\\cdot V&#94;T_j,\\alpha)$. To generate predictions from the sampler, we generate an $R_{ij}$ matrix for each $U$ and $V$ sampled, then we combine these by averaging over the $K$ samples. We introduce two versions of the 'predict': One version with averaging: $P(R&#94;*_{ij}|R,\\alpha,\\alpha_U,\\alpha_V) = \\frac{1}{K}\\sum_{k=1}&#94;K N(U_i\\cdot V&#94;T_j,\\alpha)$ Another version w/o averaging: $P(R&#94;*_{ij}|R,\\alpha,\\alpha_U,\\alpha_V) = N(U_i\\cdot V&#94;T_j,\\alpha)$ The version of $P(R&#94;*_{ij}|R,\\alpha,\\alpha_U,\\alpha_V)$ w/o averaging will be used in the diagnostics where diagnostic code will do averaging piece during evaluation. In [118]: # version w/o averaging def _predict ( self , U , V ): \"\"\"Estimate R from the given values of U and V.\"\"\" R = np . dot ( U , V . T ) n , m = R . shape sample_R = np . array ([ [ np . random . normal ( R [ i , j ], self . std ) for j in xrange ( m )] for i in xrange ( n ) ]) # bound ratings low , high = self . bounds sample_R [ sample_R < low ] = low sample_R [ sample_R > high ] = high return sample_R # version w/ averaging def _predict_trace ( self , trace_U , trace_V ): \"\"\"Estimate R from the given traces of U and V.\"\"\" R = np . zeros ( self . data . shape ) for sample in range ( len ( trace_U )): sample_R = self . predict ( trace_U [ sample ], trace_V [ sample ]) R += sample_R running_R = R / ( sample + 1 ) return running_R # here we define a function which construct a matrix 'U' from the list of 'U%i' def _getTraceMatrices ( self ): ''' construct the numpy.ndarray from the list of stochatisc variables for all traces''' Ui_names = map ( lambda x : str ( x ), self . Ui ) Vj_names = map ( lambda x : str ( x ), self . Vj ) _tmp_Ui = {} _tmp_Vj = {} nsample = None for i in Ui_names : _tmp_Ui . update ( { i :[ self . traces [ i ][ j ] for j in range ( len ( self . traces [ i ])) ] } ) if nsample is None : nsample = len ( self . traces [ i ]) for j in Vj_names : _tmp_Vj . update ( { j :[ self . traces [ j ][ i ] for i in range ( len ( self . traces [ j ])) ] } ) Ui = [] Vj = [] for i in range ( nsample ): _tmp_Ui_list = [] for row in Ui_names : _tmp_Ui_list += [ _tmp_Ui [ row ][ i ]] Ui += [ np . array ( _tmp_Ui_list )] _tmp_Vj_list = [] for row in Vj_names : _tmp_Vj_list += [ _tmp_Vj [ row ][ i ]] Vj += [ np . array ( _tmp_Vj_list )] return Ui , Vj # Update our class with the new Predict infrastructure PMF . getTraceMatrices = _getTraceMatrices PMF . predict = _predict PMF . predict_trace = _predict_trace Test of the predict function In [120]: traces_U , traces_V = pmf . getTraceMatrices () print '10 th sample R : ' , pmf . predict ( traces_U [ 9 ], traces_V [ 9 ]) print ' \\n\\n ' print 'averaging sampled R:' , pmf . predict_trace ( traces_U , traces_V ) 10 th sample R : [[ 0.73738038 2.8829047 4.15213768 ..., 0.90930744 -0.54034526 -1.37353771] [-4.66542936 -4.84431165 -4.0234287 ..., -3.47845611 -1.78232155 -3.3289911 ] [ 0.08347473 -0.16927429 -0.73111556 ..., -5.42224673 -4.75400413 -0.97107831] ..., [ 5.9403842 1.0674877 8.3158795 ..., -3.92076744 2.43819077 -4.45348299] [-3.40283744 -5.17037176 -3.23143203 ..., 0.55740903 -0.50587615 1.70313551] [ 1.8666256 2.66123111 3.16467822 ..., 0.16980542 1.15414995 -1.67072825]] averaging sampled R: [[ 1.02171491 2.42804436 2.90836343 ..., 1.94257543 1.57047715 -1.95718612] [-5.9003429 -5.91843978 -3.78319629 ..., -3.51430279 -2.18267165 -2.83427926] [ 1.18093384 0.24779066 -0.47771771 ..., -6.11247345 -5.17088762 -0.2462697 ] ..., [ 6.47288567 1.26964384 9.11230926 ..., -4.53319801 1.32721136 -4.26946043] [-2.95453856 -5.68951228 -2.75327014 ..., 1.02271064 -0.76349737 0.14336738] [ 2.44892463 1.68377936 3.6400361 ..., -0.31709323 1.84852407 -1.33852907]] Diagnostics and Posterior Predictive Check The next step is to check how many samples we should discard as burn-in. Normally, we'd do this using a traceplot to get some idea of where the sampled variables start to converge. In this case, we have high-dimensional samples, so we need to find a way to approximate them. One way was proposed by Salakhutdinov and Mnih, p.886. We can calculate the Frobenius norms of U and V at each step and monitor those for convergence. This essentially gives us some idea when the average magnitude of the latent variables is stabilizing. The equations for the Frobenius norms of U and V are shown below. We will use numpy's linalg package to calculate these. In [125]: def _norms ( pmf_model , ord = 'fro' ): \"\"\"Return norms of latent variables at each step in the sample trace. These can be used to monitor convergence of the sampler. \"\"\" traces_U , traces_V = pmf_model . getTraceMatrices () norms = { 'U' : traces_U , 'V' : traces_V } for var in norms : norms [ var ] = map ( lambda x : np . linalg . norm ( x , ord ), norms [ var ]) return norms def _traceplot ( pmf_model ): \"\"\"Plot Frobenius norms of U and V as a function of sample #.\"\"\" trace_norms = pmf_model . norms () u_series = pd . Series ( trace_norms [ 'U' ]) v_series = pd . Series ( trace_norms [ 'V' ]) fig , ( ax1 , ax2 ) = plt . subplots ( 1 , 2 , figsize = ( 15 , 7 )) u_series . plot ( kind = 'line' , ax = ax1 , grid = False , title = \"$\\|U\\|_{Fro}&#94;2$ at Each Sample\" ) v_series . plot ( kind = 'line' , ax = ax2 , grid = False , title = \"$\\|V\\|_{Fro}&#94;2$ at Each Sample\" ) ax1 . set_xlabel ( \"Sample Number\" ) ax2 . set_xlabel ( \"Sample Number\" ) PMF . norms = _norms PMF . traceplot = _traceplot pmf . traceplot () It appears we get convergence of U and V after about 4000 samples. Let's also do a traceplot of the RSME. We'll compute RMSE for both the train and the test set, even though the convergence is indicated by RMSE on the training set alone. In addition, let's compute a running RMSE on the train/test sets to see how aggregate performance improves or decreases as we continue to sample. In [138]: def _running_rmse ( pmf_model , test_data , train_data , burn_in = 0 , plot = True ): \"\"\"Calculate RMSE for each step of the trace to monitor convergence. \"\"\" traces_U , traces_V = pmf_model . getTraceMatrices () burn_in = burn_in if len ( traces_U ) >= burn_in else 0 results = { 'per-step-train' : [], 'running-train' : [], 'per-step-test' : [], 'running-test' : []} R = np . zeros ( test_data . shape ) for cnt in range ( burn_in , len ( traces_U )): sample_R = pmf_model . predict ( traces_U [ cnt ], traces_V [ cnt ]) R += sample_R running_R = R / ( cnt + 1 ) results [ 'per-step-train' ] . append ( rmse ( train_data , sample_R )) results [ 'running-train' ] . append ( rmse ( train_data , running_R )) results [ 'per-step-test' ] . append ( rmse ( test_data , sample_R )) results [ 'running-test' ] . append ( rmse ( test_data , running_R )) results = pd . DataFrame ( results ) if plot : results . plot ( kind = 'line' , grid = False , figsize = ( 15 , 7 ), title = 'Per-step and Running RMSE From Posterior Predictive' ) # Return the final predictions, and the RMSE calculations return running_R , results PMF . running_rmse = _running_rmse predicted , results = pmf . running_rmse ( test , train , burn_in = 200 ) Excellent. Another thing we want to do is make sure the MAP estimate we obtained is reasonable. We can do this by computing RMSE on the predicted ratings obtained from the MAP values of U and V. In [133]: # here we define a function which construct a matrix 'U' from the list of 'U%i' def _getMAPMatrices ( self ): ''' construct the numpy.ndarray from the list of stochatisc variables for all traces''' Ui_names = map ( lambda x : str ( x ), self . Ui ) Vj_names = map ( lambda x : str ( x ), self . Vj ) _tmp_Ui = {} _tmp_Vj = {} for i in Ui_names : _tmp_Ui . update ( { i : self . map [ i ] } ) for j in Vj_names : _tmp_Vj . update ( { j : self . map [ j ] } ) _tmp_Ui_list = [] for row in Ui_names : _tmp_Ui_list += [ _tmp_Ui [ row ]] Ui = np . array ( _tmp_Ui_list ) _tmp_Vj_list = [] for row in Vj_names : _tmp_Vj_list += [ _tmp_Vj [ row ]] Vj = np . array ( _tmp_Vj_list ) return Ui , Vj # estimation of the MAP performance def _eval_map ( pmf_model , train , test ): U , V = pmf_model . getMAPMatrices () # Make predictions and calculate RMSE on train & test sets. predictions = pmf_model . predict ( U , V ) train_rmse = rmse ( train , predictions ) test_rmse = rmse ( test , predictions ) overfit = test_rmse - train_rmse # Print report. print 'PMF MAP training RMSE: %.5f ' % train_rmse print 'PMF MAP testing RMSE: %.5f ' % test_rmse print 'Train/test difference: %.5f ' % overfit return test_rmse # Add eval function to PMF class. PMF . getMAPMatrices = _getMAPMatrices PMF . eval_map = _eval_map In [134]: # Evaluate PMF MAP estimates. pmf_map_rmse = pmf . eval_map ( train , test ) pmf_improvement = baselines [ 'mom' ] - pmf_map_rmse print 'PMF MAP Improvement: %.5f ' % pmf_improvement PMF MAP training RMSE: 3.31098 PMF MAP testing RMSE: 4.92116 Train/test difference: 1.61018 PMF MAP Improvement: -0.16064 Let's compare the MAP results with MCMC ones: In [135]: # And our final RMSE? final_test_rmse = results [ 'running-test' ] . values [ - 1 ] final_train_rmse = results [ 'running-train' ] . values [ - 1 ] print 'Posterior predictive train RMSE: %.5f ' % final_train_rmse print 'Posterior predictive test RMSE: %.5f ' % final_test_rmse print 'Train/test difference: %.5f ' % ( final_test_rmse - final_train_rmse ) print 'Improvement from MAP: %.5f ' % ( pmf_map_rmse - final_test_rmse ) print 'Improvement from Mean of Means: %.5f ' % ( baselines [ 'mom' ] - final_test_rmse ) Posterior predictive train RMSE: 3.26451 Posterior predictive test RMSE: 4.76826 Train/test difference: 1.50375 Improvement from MAP: 0.15290 Improvement from Mean of Means: -0.00774 Summary of Results Let's summarize our results. In [141]: # let's plot results size = 4800 final_results = pd . DataFrame ({ 'uniform random' : np . repeat ( baselines [ 'ur' ], size ), 'global means' : np . repeat ( baselines [ 'gm' ], size ), 'mean of means' : np . repeat ( baselines [ 'mom' ], size ), 'PMF MAP' : np . repeat ( pmf_map_rmse , size ), 'PMF MCMC' : results [ 'running-test' ][: size ] }) fig , ax = plt . subplots ( figsize = ( 10 , 5 )) final_results . plot ( kind = 'line' , grid = False , ax = ax , title = 'RMSE for all methods' ) ax . set_xlabel ( \"Number of Samples\" ) ax . set_ylabel ( \"RMSE\" ) Out[141]: <matplotlib.text.Text at 0xa081f0cc> Our results demonstrate that the mean of means method is our best baseline on our prediction task. We illustrated one way to monitor convergence of an MCMC sampler with a high-dimensionality sampling space using the Frobenius norms of the sampled variables. The traceplots using this method seem to indicate that our sampler converged to the posterior. Results using this posterior showed that attempting to improve the estimation using MCMC sampling actually overfit the training data and increased test RMSE. This was likely caused by the constraining of the posterior via fixed precision parameters $\\alpha$, $\\alpha_U$, and $\\alpha_V$. To solve the problem of the overfitting, perhaps, we could implement the fully Bayesian version of PMF (BPMF). which places hyperpriors on the model parameters to automatically learn ideal mean and precision parameters for U and V. This would likely resolve the issue we faced in this analysis. We would expect BPMF to improve upon the MAP estimation produced here by learning more suitable hyperparameters and parameters. Fully Bayesian Probabilistic Matrix Factorization(FBPMF) (optional) It was inspired by the gist . In [158]: class FBPMF ( object ): \"\"\"Fully Bayesian Probabilistic Matrix Factorization model using pymc2.\"\"\" def __init__ ( self , train , dim , alpha = 2 , std = 0.01 , bounds = ( - 10 , 10 )): \"\"\"Build the Probabilistic Matrix Factorization model using pymc2. :param np.ndarray train: The training data to use for learning the model. :param int dim: Dimensionality of the model; number of latent factors. :param int alpha: Fixed precision for the likelihood function. :param float std: Amount of noise to use for model initialization. :param (tuple of int) bounds: (lower, upper) bound of ratings. These bounds will simply be used to cap the estimates produced for R. \"\"\" self . dim = dim self . alpha = alpha self . std = np . sqrt ( 1.0 / alpha ) self . bounds = bounds # occupy a lot of memory #self.data = train.copy() self . data = train n , m = self . data . shape beta_0 = 1 # scaling factor for lambdas; unclear on its use # Perform mean value imputation nan_mask = np . isnan ( self . data ) self . data [ nan_mask ] = self . data [ ~ nan_mask ] . mean () # DELETED in FBMF # Low precision reflects uncertainty; prevents overfitting. # Set to the mean variance across users and jokes. # var() gives std. variation #self.alpha_u = 1 / self.data.var(axis=1).mean() #self.alpha_v = 1 / self.data.var(axis=0).mean() # Specify the model. logging . info ( 'building the FBPMF model' ) # define our bayesian model here # NEW in FBMF: self . lambda_u = pm . Wishart ( 'lambda_u' , dim , np . eye ( dim )) self . alpha_u = pm . Lambda ( 'alpha_u' , lambda inv_cov_matr = self . lambda_u : np . sqrt ( np . diag ( inv_cov_matr )) . mean ()) self . mu_u = pm . Normal ( 'mu_u' , np . zeros ( dim ), self . alpha_u , size = dim ) #self.mu_u = pm.Normal('mu_u', np.zeros(dim), 1, size=dim) # define the U_i vector: CHANGED in FBMF self . Ui = [ pm . MvNormal ( 'U %i ' % i ,self.mu_u,self.lambda_u) for i in range(n) ] # NEW in FBMF: self . lambda_v = pm . Wishart ( 'lambda_v' , dim , np . eye ( dim )) self . alpha_v = pm . Lambda ( 'alpha_v' , lambda inv_cov_matr = self . lambda_v : np . sqrt ( np . diag ( inv_cov_matr )) . mean ()) self . mu_v = pm . Normal ( 'mu_v' , np . zeros ( dim ), self . alpha_v , size = dim ) #self.mu_v = pm.Normal('mu_v', np.zeros(dim), self.lambda_v, size=dim) #self.mu_v = pm.Normal('mu_v', np.zeros(dim), 1, size=dim) # define the V_j vector: CHANGED in FBMF self . Vj = [ pm . MvNormal ( 'V %i ' % j ,self.mu_v,self.lambda_v) for j in range(m) ] # define the mean value Rij = [] for i in range ( n ): _tmp = [] for j in range ( m ): _tmp += [ pm . Lambda ( '_R %i%i ' % ( i , j ), lambda U = self . Ui [ i ], V = self . Vj [ j ]: np . dot ( U , V . T ))] Rij += [ _tmp ] # define the observed self . Rij = [] for i in range ( n ): _tmp = [] for j in range ( m ): #_tmp +=[pm.Normal('R%i%i'%(i,j),np.dot(self.Ui[i].value,self.Vj[j].value.T),self.alpha,value=self.data[i,j],observed=True)] _tmp += [ pm . Normal ( 'R %i%i ' % ( i , j ), Rij [ i ][ j ], self . alpha , value = self . data [ i , j ], observed = True )] self . Rij += [ _tmp ] self . Rij = np . array ( self . Rij ) . flatten () . tolist () # define our model self . model = pm . Model ( self . Ui + self . Vj + self . Rij ) logging . info ( 'done building the FBPMF model' ) def __str__ ( self ): return self . name In [159]: # Test of Fully Bayesian PMF # We use a fixed precision for the likelihood. # This reflects uncertainty in the dot product. # We choose 2 in the footsteps Salakhutdinov # Mnihof. ALPHA = 2 # The dimensionality D; the number of latent factors. # We can adjust this higher to try to capture more subtle # characteristics of each joke. However, the higher it is, # the more expensive our inference procedures will be. # Specifically, we have D(N + M) latent variables. For our # Jester dataset, this means we have D(1100), so for 5 # dimensions, we are sampling 5500 latent variables. DIM = 5 fbpmf = FBPMF ( train , DIM , ALPHA , std = 0.05 ) ? fbpmf INFO:root:building the FBPMF model INFO:root:done building the FBPMF model Here we add all functionality of the basic PMF: In [168]: # property 'map_dir' def _map_dir_bfpmf ( self ): ''' return the dir name where we store the MAP''' basename = 'bfpmf-map-d %d ' % self . dim return os . path . join ( 'data' , basename ) # Update our class with the new MAP infrastructure. FBPMF . find_map = _find_map FBPMF . load_map = _load_map FBPMF . map_dir = property ( _map_dir_bfpmf ) FBPMF . map = property ( _map ) # Add eval function to PMF class. FBPMF . getMAPMatrices = _getMAPMatrices FBPMF . eval_map = _eval_map # Draw MCMC samples. def _trace_dir_bfpmf ( self ): basename = 'bfpmf-mcmc-d %d ' % self . dim return os . path . join ( 'data' , basename ) # Update our class with the sampling infrastructure. FBPMF . trace_dir = property ( _trace_dir_bfpmf ) FBPMF . draw_samples = _draw_samples FBPMF . load_trace = _load_trace # Update our class with the new Predict infrastructure FBPMF . getTraceMatrices = _getTraceMatrices FBPMF . predict = _predict FBPMF . predict_trace = _predict_trace # plotting stuff FBPMF . norms = _norms FBPMF . traceplot = _traceplot FBPMF . running_rmse = _running_rmse In [166]: fbpmf . draw_samples ( 5000 , njobs = 3 ) INFO:root:drawing 5000 samples using 3 jobs INFO:root:backing up trace to directory: data/bfpmf-mcmc-d5 [-----------------100%-----------------] 5000 of 5000 complete in 168.1 sec Diagnostics In [169]: fbpmf . traceplot () In [170]: _ , results_fbpmf = fbpmf . running_rmse ( test , train , burn_in = 200 ) In [171]: # Find MAP for FBPMF. fbpmf . find_map () INFO:root:finding PMF MAP using Powell optimization... INFO:root:found PMF MAP in 1522 seconds INFO:root:writing numpy vars to directory: data/bfpmf-map-d5 MAP was stored in data/mom folder In [174]: # Evaluate PMF MAP estimates. fbpmf_map_rmse = fbpmf . eval_map ( train , test ) fbpmf_improvement = baselines [ 'mom' ] - fbpmf_map_rmse print 'PMF MAP Improvement: %.5f ' % fbpmf_improvement PMF MAP training RMSE: 3.30869 PMF MAP testing RMSE: 4.77023 Train/test difference: 1.46154 PMF MAP Improvement: -0.00970 In [175]: # And our final RMSE? fbpmb_final_test_rmse = results_fbpmf [ 'running-test' ] . values [ - 1 ] fbpmb_final_train_rmse = results_fbpmf [ 'running-train' ] . values [ - 1 ] print 'Posterior predictive train RMSE: %.5f ' % fbpmb_final_train_rmse print 'Posterior predictive test RMSE: %.5f ' % fbpmb_final_test_rmse print 'Train/test difference: %.5f ' % ( fbpmb_final_test_rmse - fbpmb_final_train_rmse ) print 'Improvement from MAP: %.5f ' % ( fbpmf_map_rmse - fbpmb_final_test_rmse ) print 'Improvement from Mean of Means: %.5f ' % ( baselines [ 'mom' ] - fbpmb_final_test_rmse ) Posterior predictive train RMSE: 3.46717 Posterior predictive test RMSE: 4.79397 Train/test difference: 1.32679 Improvement from MAP: -0.02374 Improvement from Mean of Means: -0.03345 In [176]: # let's plot results size = 4800 final_results = pd . DataFrame ({ 'uniform random' : np . repeat ( baselines [ 'ur' ], size ), 'global means' : np . repeat ( baselines [ 'gm' ], size ), 'mean of means' : np . repeat ( baselines [ 'mom' ], size ), 'FBPMF MAP' : np . repeat ( fbpmf_map_rmse , size ), 'FBPMF MCMC' : results_fbpmf [ 'running-test' ][: size ] }) fig , ax = plt . subplots ( figsize = ( 10 , 5 )) final_results . plot ( kind = 'line' , grid = False , ax = ax , title = 'RMSE for all methods' ) ax . set_xlabel ( \"Number of Samples\" ) ax . set_ylabel ( \"RMSE\" ) Out[176]: <matplotlib.text.Text at 0x1076beac> In [ ]: if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML'; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: 'center',\" + \" displayIndent: '0em',\" + \" showMathMenu: true,\" + \" tex2jax: { \" + \" inlineMath: [ ['$','$'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }\" + \" } \" + \"}); \"; (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Statistics","title":"Probabilistic Matrix Factorization for Making Personalized Recommendations"},{"url":"http://igormarfin.github.io/blog/2016/02/07/stochastic-volatility-model/","text":"/*! * * IPython notebook * */.ansibold{font-weight:bold}.ansiblack{color:black}.ansired{color:darkred}.ansigreen{color:darkgreen}.ansiyellow{color:#c4a000}.ansiblue{color:darkblue}.ansipurple{color:darkviolet}.ansicyan{color:steelblue}.ansigray{color:gray}.ansibgblack{background-color:black}.ansibgred{background-color:red}.ansibggreen{background-color:green}.ansibgyellow{background-color:yellow}.ansibgblue{background-color:blue}.ansibgpurple{background-color:magenta}.ansibgcyan{background-color:cyan}.ansibggray{background-color:gray}div.cell{border:1px solid transparent;display:-webkit-box;-webkit-box-orient:vertical;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:vertical;-moz-box-align:stretch;display:box;box-orient:vertical;box-align:stretch;display:flex;flex-direction:column;align-items:stretch;border-radius:2px;box-sizing:border-box;-moz-box-sizing:border-box;-webkit-box-sizing:border-box;border-width:thin;border-style:solid;width:100%;padding:5px;margin:0;outline:none}div.cell.selected{border-color:#ababab}@media print{div.cell.selected{border-color:transparent}}.edit_mode div.cell.selected{border-color:green}@media print{.edit_mode div.cell.selected{border-color:transparent}}.prompt{min-width:14ex;padding:.4em;margin:0;font-family:monospace;text-align:right;line-height:1.21429em}@media (max-width:540px){.prompt{text-align:left}}div.inner_cell{display:-webkit-box;-webkit-box-orient:vertical;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:vertical;-moz-box-align:stretch;display:box;box-orient:vertical;box-align:stretch;display:flex;flex-direction:column;align-items:stretch;-webkit-box-flex:1;-moz-box-flex:1;box-flex:1;flex:1}@-moz-document url-prefix(){div.inner_cell{overflow-x:hidden}}div.input_area{border:1px solid #cfcfcf;border-radius:2px;background:#f7f7f7;line-height:1.21429em}div.prompt:empty{padding-top:0;padding-bottom:0}div.unrecognized_cell{padding:5px 5px 5px 0;display:-webkit-box;-webkit-box-orient:horizontal;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:horizontal;-moz-box-align:stretch;display:box;box-orient:horizontal;box-align:stretch;display:flex;flex-direction:row;align-items:stretch}div.unrecognized_cell .inner_cell{border-radius:2px;padding:5px;font-weight:bold;color:red;border:1px solid #cfcfcf;background:#eaeaea}div.unrecognized_cell .inner_cell a{color:inherit;text-decoration:none}div.unrecognized_cell .inner_cell a:hover{color:inherit;text-decoration:none}@media (max-width:540px){div.unrecognized_cell>div.prompt{display:none}}@media print{div.code_cell{page-break-inside:avoid}}div.input{page-break-inside:avoid;display:-webkit-box;-webkit-box-orient:horizontal;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:horizontal;-moz-box-align:stretch;display:box;box-orient:horizontal;box-align:stretch;display:flex;flex-direction:row;align-items:stretch}@media (max-width:540px){div.input{display:-webkit-box;-webkit-box-orient:vertical;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:vertical;-moz-box-align:stretch;display:box;box-orient:vertical;box-align:stretch;display:flex;flex-direction:column;align-items:stretch}}div.input_prompt{color:navy;border-top:1px solid transparent}div.input_area>div.highlight{margin:.4em;border:none;padding:0;background-color:transparent}div.input_area>div.highlight>pre{margin:0;border:none;padding:0;background-color:transparent}.CodeMirror{line-height:1.21429em;font-size:14px;height:auto;background:none}.CodeMirror-scroll{overflow-y:hidden;overflow-x:auto}.CodeMirror-lines{padding:.4em}.CodeMirror-linenumber{padding:0 8px 0 4px}.CodeMirror-gutters{border-bottom-left-radius:2px;border-top-left-radius:2px}.CodeMirror pre{padding:0;border:0;border-radius:0}.highlight-base{}.highlight-variable{}.highlight-variable-2{color:#1a1a1a}.highlight-variable-3{color:#333}.highlight-string{color:#ba2121}.highlight-comment{color:#408080;font-style:italic}.highlight-number{80}.highlight-atom{color:#88f}.highlight-keyword{8000;font-weight:bold}.highlight-builtin{8000}.highlight-error{color:#f00}.highlight-operator{color:#a2f;font-weight:bold}.highlight-meta{color:#a2f}.highlight-def{f}.highlight-string-2{color:#f50}.highlight-qualifier{color:#555}.highlight-bracket{color:#997}.highlight-tag{color:#170}.highlight-attribute{c}.highlight-header{color:blue}.highlight-quote{90}.highlight-link{c}.cm-s-ipython span.cm-keyword{8000;font-weight:bold}.cm-s-ipython span.cm-atom{color:#88f}.cm-s-ipython span.cm-number{80}.cm-s-ipython span.cm-def{f}.cm-s-ipython span.cm-variable{}.cm-s-ipython span.cm-operator{color:#a2f;font-weight:bold}.cm-s-ipython span.cm-variable-2{color:#1a1a1a}.cm-s-ipython span.cm-variable-3{color:#333}.cm-s-ipython span.cm-comment{color:#408080;font-style:italic}.cm-s-ipython span.cm-string{color:#ba2121}.cm-s-ipython span.cm-string-2{color:#f50}.cm-s-ipython span.cm-meta{color:#a2f}.cm-s-ipython span.cm-qualifier{color:#555}.cm-s-ipython span.cm-builtin{8000}.cm-s-ipython span.cm-bracket{color:#997}.cm-s-ipython span.cm-tag{color:#170}.cm-s-ipython span.cm-attribute{c}.cm-s-ipython span.cm-header{color:blue}.cm-s-ipython span.cm-quote{90}.cm-s-ipython span.cm-link{c}.cm-s-ipython span.cm-error{color:#f00}.cm-s-ipython span.cm-tab{background:url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADAAAAAMCAYAAAAkuj5RAAAAAXNSR0IArs4c6QAAAGFJREFUSMft1LsRQFAQheHPowAKoACx3IgEKtaEHujDjORSgWTH/ZOdnZOcM/sgk/kFFWY0qV8foQwS4MKBCS3qR6ixBJvElOobYAtivseIE120FaowJPN75GMu8j/LfMwNjh4HUpwg4LUAAAAASUVORK5CYII=);background-position:right;background-repeat:no-repeat}div.output_wrapper{position:relative;display:-webkit-box;-webkit-box-orient:vertical;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:vertical;-moz-box-align:stretch;display:box;box-orient:vertical;box-align:stretch;display:flex;flex-direction:column;align-items:stretch}div.output_scroll{height:24em;width:100%;overflow:auto;border-radius:2px;-webkit-box-shadow:inset 0 2px 8px rgba(0,0,0,0.8);box-shadow:inset 0 2px 8px rgba(0,0,0,0.8);display:block}div.output_collapsed{margin:0;padding:0;display:-webkit-box;-webkit-box-orient:vertical;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:vertical;-moz-box-align:stretch;display:box;box-orient:vertical;box-align:stretch;display:flex;flex-direction:column;align-items:stretch}div.out_prompt_overlay{height:100%;padding:0 .4em;position:absolute;border-radius:2px}div.out_prompt_overlay:hover{-webkit-box-shadow:inset 0 0 1px #000;box-shadow:inset 0 0 1px #000;background:rgba(240,240,240,0.5)}div.output_prompt{color:darkred}div.output_area{padding:0;page-break-inside:avoid;display:-webkit-box;-webkit-box-orient:horizontal;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:horizontal;-moz-box-align:stretch;display:box;box-orient:horizontal;box-align:stretch;display:flex;flex-direction:row;align-items:stretch}div.output_area .MathJax_Display{text-align:left !important}div.output_area div.output_area .output{display:-webkit-box;-webkit-box-orient:vertical;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:vertical;-moz-box-align:stretch;display:box;box-orient:vertical;box-align:stretch;display:flex;flex-direction:column;align-items:stretch}@media (max-width:540px){div.output_area{display:-webkit-box;-webkit-box-orient:vertical;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:vertical;-moz-box-align:stretch;display:box;box-orient:vertical;box-align:stretch;display:flex;flex-direction:column;align-items:stretch}}div.output_area pre{margin:0;padding:0;border:0;vertical-align:baseline;color:black;background-color:transparent;border-radius:0}div.output_subarea{padding:.4em;-webkit-box-flex:1;-moz-box-flex:1;box-flex:1;flex:1}div.output_text{text-align:left;line-height:1.21429em}div.output_stderr{background:#fdd}div.output_latex{text-align:left}div.output_javascript:empty{padding:0}.js-error{color:darkred}div.raw_input_container{font-family:monospace;padding-top:5px}input.raw_input{font-family:inherit;font-size:inherit;color:inherit;width:auto;vertical-align:baseline;padding:0 .25em;margin:0 .25em}input.raw_input:focus{box-shadow:none}p.p-space{margin-bottom:10px}div.output_unrecognized{padding:5px;font-weight:bold;color:red}div.output_unrecognized a{color:inherit;text-decoration:none}div.output_unrecognized a:hover{color:inherit;text-decoration:none}.rendered_html{}.rendered_html :link{text-decoration:underline}.rendered_html :visited{text-decoration:underline}.rendered_html h1:first-child{margin-top:.538em}.rendered_html h2:first-child{margin-top:.636em}.rendered_html h3:first-child{margin-top:.777em}.rendered_html h4:first-child{margin-top:1em}.rendered_html h5:first-child{margin-top:1em}.rendered_html h6:first-child{margin-top:1em}.rendered_html *+ul{margin-top:1em}.rendered_html *+ol{margin-top:1em}.rendered_html *+table{margin-top:1em}.rendered_html *+p{margin-top:1em}.rendered_html *+img{margin-top:1em}div.text_cell{display:-webkit-box;-webkit-box-orient:horizontal;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:horizontal;-moz-box-align:stretch;display:box;box-orient:horizontal;box-align:stretch;display:flex;flex-direction:row;align-items:stretch}@media (max-width:540px){div.text_cell>div.prompt{display:none}}div.text_cell_render{outline:none;resize:none;width:inherit;border-style:none;padding:.5em .5em .5em .4em;box-sizing:border-box;-moz-box-sizing:border-box;-webkit-box-sizing:border-box}a.anchor-link:link{text-decoration:none;padding:0 20px;visibility:hidden}h1:hover .anchor-link,h2:hover .anchor-link,h3:hover .anchor-link,h4:hover .anchor-link,h5:hover .anchor-link,h6:hover .anchor-link{visibility:visible}.text_cell.rendered .input_area{display:none}.text_cell.unrendered .text_cell_render{display:none}.cm-header-1,.cm-header-2,.cm-header-3,.cm-header-4,.cm-header-5,.cm-header-6{font-weight:bold;font-family:\"Helvetica Neue\",Helvetica,Arial,sans-serif}.cm-header-1{font-size:185.7%}.cm-header-2{font-size:157.1%}.cm-header-3{font-size:128.6%}.cm-header-4{font-size:110%}.cm-header-5{font-size:100%;font-style:italic}.cm-header-6{font-size:100%;font-style:italic}.widget-interact>div,.widget-interact>input{padding:2.5px}.widget-area{page-break-inside:avoid;display:-webkit-box;-webkit-box-orient:horizontal;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:horizontal;-moz-box-align:stretch;display:box;box-orient:horizontal;box-align:stretch;display:flex;flex-direction:row;align-items:stretch}.widget-area .widget-subarea{padding:.44em .4em .4em 1px;margin-left:6px;box-sizing:border-box;-moz-box-sizing:border-box;-webkit-box-sizing:border-box;display:-webkit-box;-webkit-box-orient:vertical;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:vertical;-moz-box-align:stretch;display:box;box-orient:vertical;box-align:stretch;display:flex;flex-direction:column;align-items:stretch;-webkit-box-flex:2;-moz-box-flex:2;box-flex:2;flex:2;-webkit-box-align:start;-moz-box-align:start;box-align:start;align-items:flex-start}.widget-area.connection-problems .prompt:after{content:\"\\f127\";font-family:'FontAwesome';color:#d9534f;font-size:14px;top:3px;padding:3px}.slide-track{border:1px solid #ccc;background:#fff;border-radius:2px}.widget-hslider{padding-left:8px;padding-right:2px;overflow:visible;width:350px;height:5px;max-height:5px;margin-top:13px;margin-bottom:10px;border:1px solid #ccc;background:#fff;border-radius:2px;display:-webkit-box;-webkit-box-orient:horizontal;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:horizontal;-moz-box-align:stretch;display:box;box-orient:horizontal;box-align:stretch;display:flex;flex-direction:row;align-items:stretch}.widget-hslider .ui-slider{border:0;background:none;display:-webkit-box;-webkit-box-orient:horizontal;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:horizontal;-moz-box-align:stretch;display:box;box-orient:horizontal;box-align:stretch;display:flex;flex-direction:row;align-items:stretch;-webkit-box-flex:1;-moz-box-flex:1;box-flex:1;flex:1}.widget-hslider .ui-slider .ui-slider-handle{width:12px;height:28px;margin-top:-8px;border-radius:2px}.widget-hslider .ui-slider .ui-slider-range{height:12px;margin-top:-4px;background:#eee}.widget-vslider{padding-bottom:5px;overflow:visible;width:5px;max-width:5px;height:250px;margin-left:12px;border:1px solid #ccc;background:#fff;border-radius:2px;display:-webkit-box;-webkit-box-orient:vertical;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:vertical;-moz-box-align:stretch;display:box;box-orient:vertical;box-align:stretch;display:flex;flex-direction:column;align-items:stretch}.widget-vslider .ui-slider{border:0;background:none;margin-left:-4px;margin-top:5px;display:-webkit-box;-webkit-box-orient:vertical;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:vertical;-moz-box-align:stretch;display:box;box-orient:vertical;box-align:stretch;display:flex;flex-direction:column;align-items:stretch;-webkit-box-flex:1;-moz-box-flex:1;box-flex:1;flex:1}.widget-vslider .ui-slider .ui-slider-handle{width:28px;height:12px;margin-left:-9px;border-radius:2px}.widget-vslider .ui-slider .ui-slider-range{width:12px;margin-left:-1px;background:#eee}.widget-text{width:350px;margin:0}.widget-listbox{width:350px;margin-bottom:0}.widget-numeric-text{width:150px;margin:0}.widget-progress{margin-top:6px;min-width:350px}.widget-progress .progress-bar{-webkit-transition:none;-moz-transition:none;-ms-transition:none;-o-transition:none;transition:none}.widget-combo-btn{min-width:125px}.widget_item .dropdown-menu li a{color:inherit}.widget-hbox{display:-webkit-box;-webkit-box-orient:horizontal;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:horizontal;-moz-box-align:stretch;display:box;box-orient:horizontal;box-align:stretch;display:flex;flex-direction:row;align-items:stretch}.widget-hbox input[type=\"checkbox\"]{margin-top:9px;margin-bottom:10px}.widget-hbox .widget-label{min-width:10ex;padding-right:8px;padding-top:5px;text-align:right;vertical-align:text-top}.widget-hbox .widget-readout{padding-left:8px;padding-top:5px;text-align:left;vertical-align:text-top}.widget-vbox{display:-webkit-box;-webkit-box-orient:vertical;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:vertical;-moz-box-align:stretch;display:box;box-orient:vertical;box-align:stretch;display:flex;flex-direction:column;align-items:stretch}.widget-vbox .widget-label{padding-bottom:5px;text-align:center;vertical-align:text-bottom}.widget-vbox .widget-readout{padding-top:5px;text-align:center;vertical-align:text-top}.widget-box{box-sizing:border-box;-moz-box-sizing:border-box;-webkit-box-sizing:border-box;-webkit-box-align:start;-moz-box-align:start;box-align:start;align-items:flex-start}.widget-radio-box{display:-webkit-box;-webkit-box-orient:vertical;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:vertical;-moz-box-align:stretch;display:box;box-orient:vertical;box-align:stretch;display:flex;flex-direction:column;align-items:stretch;box-sizing:border-box;-moz-box-sizing:border-box;-webkit-box-sizing:border-box;padding-top:4px}.widget-radio-box label{margin-top:0}.widget-radio{margin-left:20px} .highlight .hll { background-color: #ffffcc } .highlight { background: #f8f8f8; } .highlight .c { color: #408080; font-style: italic } /* Comment */ .highlight .err { border: 1px solid #FF0000 } /* Error */ .highlight .k { color: #008000; font-weight: bold } /* Keyword */ .highlight .o { color: #666666 } /* Operator */ .highlight .cm { color: #408080; font-style: italic } /* Comment.Multiline */ .highlight .cp { color: #BC7A00 } /* Comment.Preproc */ .highlight .c1 { color: #408080; font-style: italic } /* Comment.Single */ .highlight .cs { color: #408080; font-style: italic } /* Comment.Special */ .highlight .gd { color: #A00000 } /* Generic.Deleted */ .highlight .ge { font-style: italic } /* Generic.Emph */ .highlight .gr { color: #FF0000 } /* Generic.Error */ .highlight .gh { color: #000080; font-weight: bold } /* Generic.Heading */ .highlight .gi { color: #00A000 } /* Generic.Inserted */ .highlight .go { color: #888888 } /* Generic.Output */ .highlight .gp { color: #000080; font-weight: bold } /* Generic.Prompt */ .highlight .gs { font-weight: bold } /* Generic.Strong */ .highlight .gu { color: #800080; font-weight: bold } /* Generic.Subheading */ .highlight .gt { color: #0044DD } /* Generic.Traceback */ .highlight .kc { color: #008000; font-weight: bold } /* Keyword.Constant */ .highlight .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */ .highlight .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */ .highlight .kp { color: #008000 } /* Keyword.Pseudo */ .highlight .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */ .highlight .kt { color: #B00040 } /* Keyword.Type */ .highlight .m { color: #666666 } /* Literal.Number */ .highlight .s { color: #BA2121 } /* Literal.String */ .highlight .na { color: #7D9029 } /* Name.Attribute */ .highlight .nb { color: #008000 } /* Name.Builtin */ .highlight .nc { color: #0000FF; font-weight: bold } /* Name.Class */ .highlight .no { color: #880000 } /* Name.Constant */ .highlight .nd { color: #AA22FF } /* Name.Decorator */ .highlight .ni { color: #999999; font-weight: bold } /* Name.Entity */ .highlight .ne { color: #D2413A; font-weight: bold } /* Name.Exception */ .highlight .nf { color: #0000FF } /* Name.Function */ .highlight .nl { color: #A0A000 } /* Name.Label */ .highlight .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */ .highlight .nt { color: #008000; font-weight: bold } /* Name.Tag */ .highlight .nv { color: #19177C } /* Name.Variable */ .highlight .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */ .highlight .w { color: #bbbbbb } /* Text.Whitespace */ .highlight .mb { color: #666666 } /* Literal.Number.Bin */ .highlight .mf { color: #666666 } /* Literal.Number.Float */ .highlight .mh { color: #666666 } /* Literal.Number.Hex */ .highlight .mi { color: #666666 } /* Literal.Number.Integer */ .highlight .mo { color: #666666 } /* Literal.Number.Oct */ .highlight .sb { color: #BA2121 } /* Literal.String.Backtick */ .highlight .sc { color: #BA2121 } /* Literal.String.Char */ .highlight .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */ .highlight .s2 { color: #BA2121 } /* Literal.String.Double */ .highlight .se { color: #BB6622; font-weight: bold } /* Literal.String.Escape */ .highlight .sh { color: #BA2121 } /* Literal.String.Heredoc */ .highlight .si { color: #BB6688; font-weight: bold } /* Literal.String.Interpol */ .highlight .sx { color: #008000 } /* Literal.String.Other */ .highlight .sr { color: #BB6688 } /* Literal.String.Regex */ .highlight .s1 { color: #BA2121 } /* Literal.String.Single */ .highlight .ss { color: #19177C } /* Literal.String.Symbol */ .highlight .bp { color: #008000 } /* Name.Builtin.Pseudo */ .highlight .vc { color: #19177C } /* Name.Variable.Class */ .highlight .vg { color: #19177C } /* Name.Variable.Global */ .highlight .vi { color: #19177C } /* Name.Variable.Instance */ .highlight .il { color: #666666 } /* Literal.Number.Integer.Long */ Stochastic Volatility model Igor Marfin igor.marfin@unister.de Abstract The model discussed in this analysis was presented in the PyMC3 tutorial page . The aim of the work is implementaion and testing theStochastic Volatility model in terms of the Bayesian network with a help of the PyMC2. More details can be found at https://bitbucket.org/iggy_floyd/stochastic-volatility-model . Motivation Asset prices have time-varying volatility (variance of day over day returns). In some periods, returns are highly variable, while in others very stable. Stochastic volatility models model this with a latent volatility variable, modeled as a stochastic process. The following model is similar to the one described in the No-U-Turn Sampler paper, Hoffman (2011) p21. Initialization In [1]: import sys sys . path = [ '/usr/local/lib/python2.7/dist-packages' ] + sys . path # to fix the problem with numpy: this replaces 1.6 version by 1.9 % matplotlib inline % pylab inline import os import matplotlib import numpy as np import matplotlib.pyplot as pl import matplotlib as mpl import logging import pymc as pm # a plotter and dataframe modules import seaborn as sns # seaborn to make a nice plots of the data import pandas as pd import scipy.stats as stats # use a nice style for plots and the notebook import json s = json . load ( open ( \"styles/my_matplotlibrc.json\" ) ) matplotlib . rcParams . update ( s ) from IPython.core.display import HTML from IPython.display import display , Math , Latex import urllib2 def css_styling (): styles = open ( \"styles/custom_v3.css\" , \"r\" ) . read () return HTML ( styles ) css_styling () #HTML( urllib2.urlopen('http://bit.ly/1Bf5Hft').read() ) ion () # Set up logging. logger = logging . getLogger () logger . setLevel ( logging . INFO ) Populating the interactive namespace from numpy and matplotlib /usr/local/lib/python2.7/dist-packages/matplotlib/__init__.py:855: UserWarning: svg.embed_char_paths is deprecated and replaced with svg.fonttype; please use the latter. warnings.warn(self.msg_depr % (key, alt_key)) Stock Returns Take note stock brokers: you're doing it wrong. When choosing which stocks to pick, an analyst will often look at the daily return of the stock. Suppose $S_t$ is the price of the stock on day $t$, then the daily return on day $t$ is : $$r_t = \\frac{ S_t - S_{t-1} }{ S_{t-1} } $$ The expected daily return of a stock is denoted $\\mu = E[ r_t ] $. Obviously, stocks with high expected returns are desirable. Unfortunately, stock returns are so filled with noise that it is very hard to estimate this parameter. Furthermore, the parameter might change over time (consider the rises and falls of AAPL stock), hence it is unwise to use a large historical dataset. Historically, the expected return has been estimated by using the sample mean. This is a bad idea. As mentioned, the sample mean of a small sized dataset has enormous potential to be very wrong (again, see Chapter 4 for full details). Thus Bayesian inference is the correct procedure here, since we are able to see our uncertainty along with probable values. For this exercise, we will be examining the daily returns of the GOOG (google). Before we pull in the data, suppose we ask our a stock fund manager (an expert in finance), What do you think the return profile looks like for each of these companies? Our stock broker, without needing to know the language of Normal distributions, or priors, or variances, etc. creates one distributions using the trial roulette. Suppose it looks enough like Normals, so we fit Normals to it. It may look like: In [3]: figsize ( 11. , 5 ) colors = [ \"#348ABD\" ] normal = stats . norm x = np . linspace ( - 0.15 , 0.15 , 100 ) expert_prior_params = { \"GOOG\" : ( - 0.03 , 0.04 ), } for i , ( name , params ) in enumerate ( expert_prior_params . iteritems ()): pl . subplot ( 2 , 2 , i ) y = normal . pdf ( x , params [ 0 ], scale = params [ 1 ]) pl . fill_between ( x , 0 , y , color = colors [ i ], linewidth = 2 , edgecolor = colors [ i ], alpha = 0.6 ) pl . title ( name + \" prior\" ) pl . vlines ( 0 , 0 , y . max (), \"k\" , \"--\" , linewidth = 0.5 ) pl . xlim ( - 0.15 , 0.15 ) pl . tight_layout () pl . savefig ( 'plots/google_return_prior_expert.png' ) Note that this is subjective prior: the expert has a personal opinion on the stock returns, and is expressing them in a distribution. He's not wishful thinking -- he's introducing domain knowledge. Next we pull historical data for this stock: In [4]: import datetime import ystockquote as ysq stocks = [ \"GOOG\" ] enddate = datetime . datetime . now () . strftime ( \"%Y-%m- %d \" ) # today's date. startdate = \"2014-09-01\" stock_closes = {} stock_returns = {} CLOSE = 6 import operator for stock in stocks : data = np . array ( sorted ( ysq . get_historical_prices ( stock , startdate , enddate ) . items (), key = operator . itemgetter ( 0 ))) x = data [:, 1 , None ] stock_closes [ stock ] = np . apply_along_axis ( lambda x : float ( x [ 0 ][ 'Close' ]), 1 , x ) n_observations = len ( stock_closes [ stock ]) # create returns: for stock in stocks : _previous_day = np . roll ( stock_closes [ stock ], - 1 ) stock_returns [ stock ] = (( stock_closes [ stock ] - _previous_day ) / _previous_day )[: n_observations ] dates = map ( lambda x : datetime . datetime . strptime ( x , \"%Y-%m- %d \" ), data [:, 0 ][ 1 : n_observations + 1 ]) Let's plot the stock returns In [5]: figsize ( 12.5 , 4 ) for _stock , _returns in stock_returns . iteritems (): p = plt . plot (( 1 + _returns )[::] . cumprod () - 1 , '-o' , label = \" %s : Cumulative effect of the profit/loss\" % _stock , markersize = 4 , markeredgecolor = \"none\" ) # calculate the cumulative effect plt . xticks ( np . arange ( n_observations )[:: 8 ], map ( lambda x : datetime . datetime . strftime ( x , \"%Y-%m- %d \" ), dates [:: 8 ]), rotation = 60 ); plt . plot ( stock_returns [ 'GOOG' ], label = 'GOOGLE Valotation of the Close Prices' ) plt . legend ( loc = \"upper left\" ) plt . title ( \"Return space\" ) plt . ylabel ( \"Return of $1 on first date, x100%\" ); pl . savefig ( 'plots/google_returns.png' ) Realization of the Stochastic Volatility model The following model is our choice: $$ \\sigma \\sim Exponential(50), \\\\ \\nu \\sim Exponential(.1), \\\\ s_i \\sim Normal(s_{i−1},\\sigma&#94;{−2}), \\\\ \\log(\\frac{y_i}{y_{i−1}}) \\sim t(\\nu,0,exp(−2s_i)) $$ It is easier to sample the scale of the log volatility process innovations, i.e. \\sigma on a log scale. In [6]: class SVM ( object ): \"\"\"Stochastic Volatility model using pymc2.\"\"\" def __init__ ( self , train_data , sigma_mu = 50. , nu_mu =. 1 ): \"\"\"Build the Probabilistic Matrix Factorization model using pymc2. \"\"\" # occupy a lot of memory self . data = train_data N = self . data . shape [ 0 ] # Specify the model. logging . info ( 'building the SVM model' ) # define our bayesian model here # sigma and logsigma self . sigma = pm . Exponential ( 'sigma' , beta = sigma_mu ) self . logsigma = pm . Lambda ( 'logsigma' , lambda x = self . sigma : np . log ( x )) # nu self . nu = pm . Exponential ( 'nu' , beta = nu_mu , size = N ) # here we define s_i \\sim Normal(s_{i−1},\\sigma&#94;{−2}) self . si = np . empty ( N , dtype = object ) self . si [ 0 ] = pm . Normal ( \"s %i \" % 0 , 0., 1.0 / self.sigma ** 2) for i in range ( 1 , N ): self . si [ i ] = pm . Normal ( \"s %i \" % i ,self.si[i-1], 1.0 / self.sigma ** 2) self . volatility_process = np . empty ( N , dtype = object ) for i in range ( 0 , N ): self . volatility_process [ i ] = pm . Lambda ( 'vp %i ' % i ,lambda x = self.si[i]: np.exp(-2.*x)) #Deterministic('volatility_process', exp(-2*s)) # here we define observed self . r = pm . NoncentralT ( 'obs' , np . zeros ( N ), self . volatility_process , self . nu , value = self . data , observed = True , size = N ) #T('r', nu, lam=volatility_process, observed=returns) #r = T('r', nu, lam=volatility_process, observed=returns) # define our model self . model = pm . Model ([ self . sigma , self . logsigma ] + self . si . tolist () + self . volatility_process . tolist () + [ self . r ] ) #self.model = pm.Model(self.si.tolist() + ) logging . info ( 'done building the SVM model' ) def __str__ ( self ): return self . name In [7]: # Test of Bayesian SVM svm = SVM ( stock_returns [ 'GOOG' ][ - 100 :]) ? svm INFO:root:building the SVM model INFO:root:done building the SVM model Let's add MCMC support to our model. In [8]: # Draw MCMC samples. def _trace_dir ( self ): basename = 'svm-mcmc' return os . path . join ( 'data' , basename ) # Sampler def _draw_samples ( self , nsamples = 1000 ): # First make sure the trace_dir does not already exist. #if os.path.isdir(self.trace_dir): # raise OSError( # 'trace directory %s already exists. Please move or delete.' % self.trace_dir) # pymc2 logging . info ( 'drawing %d samples ' % ( nsamples )) self . mcmc = pm . MCMC ( self . model , db = 'txt' , dbname = self . trace_dir ) logging . info ( 'backing up trace to directory: %s ' % self . trace_dir ) self . mcmc . sample ( nsamples ) self . traces = {} for var in list ( self . mcmc . stochastics ): self . traces . update ( { str ( var ): self . mcmc . trace ( str ( var ))[:] } ) self . mcmc . db . close () def _load_trace ( self ): self . mcmc = pm . MCMC ( self . model , db = pm . database . txt . load ( self . trace_dir )) self . traces = {} for var in list ( self . mcmc . stochastics ): self . traces . update ( { str ( var ): self . mcmc . trace ( str ( var ))[:] } ) In [9]: # Update our class with the sampling infrastructure. SVM . trace_dir = property ( _trace_dir ) SVM . draw_samples = _draw_samples SVM . load_trace = _load_trace Let's try to MCMC In [10]: if not os . path . isdir ( svm . trace_dir ): svm . draw_samples ( 50000 ) else : svm . load_trace () In [16]: print svm . traces {'s9': array([ 0.00835274, 0.01119426, 0.01880646, ..., -0.00697961, -0.02101692, -0.02101692]), 's8': array([ 0.05079409, 0.00147771, 0.00147771, ..., -0.00451087, -0.01208076, -0.01208076]), 's3': array([-0.05121066, -0.07201002, -0.13360883, ..., 0.00085611, -0.00329013, -0.00329013]), 's2': array([-0.01874881, -0.010271 , -0.03232809, ..., 0.00156825, -0.0004372 , -0.00486987]), 's1': array([-0.00421045, -0.00085151, -0.00345045, ..., -0.02290941, -0.02311715, -0.02311715]), 's0': array([ 0.03124273, 0.02905708, 0.03923831, ..., 0.00507513, 0.00507513, -0.00109334]), 's7': array([-0.03538399, -0.00717455, -0.00717455, ..., 0.02262506, 0.02262506, 0.01588623]), 's6': array([-0.04851927, -0.04851927, -0.04851927, ..., 0.01648653, 0.01648653, 0.01648653]), 's5': array([-0.10418081, -0.09653195, -0.0657978 , ..., 0.00303172, 0.00303172, 0.00303172]), 's4': array([-0.11135461, -0.11135461, -0.11135461, ..., 0.0071028 , 0.0071028 , 0.0071028 ]), 'sigma': array([ 0.03592898, 0.03592898, 0.05239611, ..., 0.01423297, 0.01423297, 0.01423297]), 'nu': array([ 29.37165944, 29.37165944, 29.37165944, ..., 5.08738683, 5.08738683, 8.08751587])} We can make control plots to look at auto-correlations. In [11]: from pymc.Matplot import plot as mcplot [ mcplot ( svm . sigma , path = 'plots/' ), mcplot ( svm . logsigma , path = 'plots/' )] Plotting sigma Plotting logsigma Out[11]: [None, None] Here we do the GoF of some parameters and other diagnostic plots In [12]: from pymc import Matplot , gelman_rubin #svm.draw_samples(50000) # plot CI on some parameters and Gelman-Rubin statistic Matplot . summary_plot ([ svm . sigma ] + svm . si [: 10 ] . tolist (), path = 'plots/' , rhat = True ) Could not calculate Gelman-Rubin statistics. Requires multiple chains of equal length. In [14]: #gelman_rubin(svm.mcmc.trace) #gelman_rubin(svm.mcmc.trace('sigma')[:]) In [124]: svm . si [ 10 ] . trace () Out[124]: array([-0.07652315, -0.07652315, -0.05681686, ..., -3.86964306, -3.86964306, -3.96117435]) In [126]: from pymc import discrepancy # check discrepancy for svm.si[10] #expected = svm.si[10].trace() #d = discrepancy(svm.data,svm.r, expected) #d[0][:10], d[1][:10] Now, we plot the volatility... In [15]: # plot the valotility figsize ( 12 , 6 ) #title(str(s)) plt . plot ( #np.array([svm.traces['s%i'%key][::1000] for key in range(len(svm.si))]), np . array ([ svm . traces [ 's %i ' % key ].mean() for key in range(len(svm.si))]), 'b' , alpha =. 1 ); xlabel ( 'time' ) ylabel ( 'log volatility' ) pl . savefig ( 'plots/google_volatility.png' ) Finally, we can superimpose the volatility predictions with dayly return of the GOOGle stock. In [16]: plt . plot ( stock_returns [ 'GOOG' ][ - 100 :]) plt . plot ( np . exp ( np . array ([ svm . traces [ 's %i ' % key ][::1000] for key in range(len(svm.si))]) ) , 'r' , alpha =. 03 ); plt . plot ( - np . exp ( np . array ([ svm . traces [ 's %i ' % key ][::1000] for key in range(len(svm.si))]) ) , 'r' , alpha =. 03 ); ylim ( - 0.3 , 0.3 ) xlabel ( 'time(days from the present)' ) ylabel ( 'returns' ) pl . savefig ( 'plots/google_volatility_returns.png' ) References Hoffman & Gelman. (2011). The No-U-Turn Sampler: Adaptively Setting Path Lengths in Hamiltonian Monte Carlo. if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML'; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: 'center',\" + \" displayIndent: '0em',\" + \" showMathMenu: true,\" + \" tex2jax: { \" + \" inlineMath: [ ['$','$'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }\" + \" } \" + \"}); \"; (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Statistics","title":"Stochastic Volatility Model"},{"url":"http://igormarfin.github.io/blog/2016/02/07/about-meI'm a data scientist and programmer currently based in Chemnitz, Germany./","text":"Previuosly, I was engaged with researches in particle and theoretical physics at CERN, Switzerland. My work was related not only to physics but also to statistical analyisis and software development for experiments at the Compact Muon Solenoid (CMS) detector at Large Hadron Collider. I was involved with my group in the search for the Higgs particle. I think we have all done a great job to discover the Higgs boson in 2012. As a final point of my \"physics\" story was the PhD thesis, which I defended with summa cum laude, i.e. with \"the highest honor\" in 2014. Now I am investigating and developing the techniques of the machine learning. Also I am a big fan of the Bayesian Statistics. My current projects are related to the Device Fingerprinting, Fraud Prevention System for online transactions and Predictive Customer Behavior Modeling. You can visit my repository to look at my projects. I'm interested in doing distance work in the field of the data analysis. You could contact me at iggy.floyd.de at gmail.com","tags":"About me","title":"About Me"},{"url":"http://igormarfin.github.io/blog/2016/02/07/how-to-make-this-blog/","text":"These are only few steps which you can do, if you want to have such blog: git clone https://iggy_floyd@bitbucket.org/iggy_floyd/blog_with_ipython_notebook.git cd blog_with_ipython_notebook make Then if make command is successful, you can initialize your blog and add first posts: make blog_ini make GitHubPages_ini make add_entry_rst About Me,about me,AboutMe echo >> src/content/*about-me.rst echo >> src/content/*about-me.rst echo \"Hi! My name is Igor Marfin. I am going to start my blog...\" >> src/content/*about-me.rst make add_entry_ipython ` pwd ` /backtesting-strategy-with-kalman-model.ipynb,Kalman Modeling,Ipython,Ipython make html make serve make publish That's it. More information can be found in the docs : firefox doc/README.html # or as an alternative way to get docs cat README.wiki","tags":"BuildingThisBlog","title":"HOW-TO make this blog"},{"url":"http://igormarfin.github.io/blog/2016/02/07/bayesian-hierarchical-linear-regression-model/","text":"/*! * * IPython notebook * */.ansibold{font-weight:bold}.ansiblack{color:black}.ansired{color:darkred}.ansigreen{color:darkgreen}.ansiyellow{color:#c4a000}.ansiblue{color:darkblue}.ansipurple{color:darkviolet}.ansicyan{color:steelblue}.ansigray{color:gray}.ansibgblack{background-color:black}.ansibgred{background-color:red}.ansibggreen{background-color:green}.ansibgyellow{background-color:yellow}.ansibgblue{background-color:blue}.ansibgpurple{background-color:magenta}.ansibgcyan{background-color:cyan}.ansibggray{background-color:gray}div.cell{border:1px solid transparent;display:-webkit-box;-webkit-box-orient:vertical;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:vertical;-moz-box-align:stretch;display:box;box-orient:vertical;box-align:stretch;display:flex;flex-direction:column;align-items:stretch;border-radius:2px;box-sizing:border-box;-moz-box-sizing:border-box;-webkit-box-sizing:border-box;border-width:thin;border-style:solid;width:100%;padding:5px;margin:0;outline:none}div.cell.selected{border-color:#ababab}@media print{div.cell.selected{border-color:transparent}}.edit_mode div.cell.selected{border-color:green}@media print{.edit_mode div.cell.selected{border-color:transparent}}.prompt{min-width:14ex;padding:.4em;margin:0;font-family:monospace;text-align:right;line-height:1.21429em}@media (max-width:540px){.prompt{text-align:left}}div.inner_cell{display:-webkit-box;-webkit-box-orient:vertical;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:vertical;-moz-box-align:stretch;display:box;box-orient:vertical;box-align:stretch;display:flex;flex-direction:column;align-items:stretch;-webkit-box-flex:1;-moz-box-flex:1;box-flex:1;flex:1}@-moz-document url-prefix(){div.inner_cell{overflow-x:hidden}}div.input_area{border:1px solid #cfcfcf;border-radius:2px;background:#f7f7f7;line-height:1.21429em}div.prompt:empty{padding-top:0;padding-bottom:0}div.unrecognized_cell{padding:5px 5px 5px 0;display:-webkit-box;-webkit-box-orient:horizontal;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:horizontal;-moz-box-align:stretch;display:box;box-orient:horizontal;box-align:stretch;display:flex;flex-direction:row;align-items:stretch}div.unrecognized_cell .inner_cell{border-radius:2px;padding:5px;font-weight:bold;color:red;border:1px solid #cfcfcf;background:#eaeaea}div.unrecognized_cell .inner_cell a{color:inherit;text-decoration:none}div.unrecognized_cell .inner_cell a:hover{color:inherit;text-decoration:none}@media (max-width:540px){div.unrecognized_cell>div.prompt{display:none}}@media print{div.code_cell{page-break-inside:avoid}}div.input{page-break-inside:avoid;display:-webkit-box;-webkit-box-orient:horizontal;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:horizontal;-moz-box-align:stretch;display:box;box-orient:horizontal;box-align:stretch;display:flex;flex-direction:row;align-items:stretch}@media (max-width:540px){div.input{display:-webkit-box;-webkit-box-orient:vertical;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:vertical;-moz-box-align:stretch;display:box;box-orient:vertical;box-align:stretch;display:flex;flex-direction:column;align-items:stretch}}div.input_prompt{color:navy;border-top:1px solid transparent}div.input_area>div.highlight{margin:.4em;border:none;padding:0;background-color:transparent}div.input_area>div.highlight>pre{margin:0;border:none;padding:0;background-color:transparent}.CodeMirror{line-height:1.21429em;font-size:14px;height:auto;background:none}.CodeMirror-scroll{overflow-y:hidden;overflow-x:auto}.CodeMirror-lines{padding:.4em}.CodeMirror-linenumber{padding:0 8px 0 4px}.CodeMirror-gutters{border-bottom-left-radius:2px;border-top-left-radius:2px}.CodeMirror pre{padding:0;border:0;border-radius:0}.highlight-base{}.highlight-variable{}.highlight-variable-2{color:#1a1a1a}.highlight-variable-3{color:#333}.highlight-string{color:#ba2121}.highlight-comment{color:#408080;font-style:italic}.highlight-number{80}.highlight-atom{color:#88f}.highlight-keyword{8000;font-weight:bold}.highlight-builtin{8000}.highlight-error{color:#f00}.highlight-operator{color:#a2f;font-weight:bold}.highlight-meta{color:#a2f}.highlight-def{f}.highlight-string-2{color:#f50}.highlight-qualifier{color:#555}.highlight-bracket{color:#997}.highlight-tag{color:#170}.highlight-attribute{c}.highlight-header{color:blue}.highlight-quote{90}.highlight-link{c}.cm-s-ipython span.cm-keyword{8000;font-weight:bold}.cm-s-ipython span.cm-atom{color:#88f}.cm-s-ipython span.cm-number{80}.cm-s-ipython span.cm-def{f}.cm-s-ipython span.cm-variable{}.cm-s-ipython span.cm-operator{color:#a2f;font-weight:bold}.cm-s-ipython span.cm-variable-2{color:#1a1a1a}.cm-s-ipython span.cm-variable-3{color:#333}.cm-s-ipython span.cm-comment{color:#408080;font-style:italic}.cm-s-ipython span.cm-string{color:#ba2121}.cm-s-ipython span.cm-string-2{color:#f50}.cm-s-ipython span.cm-meta{color:#a2f}.cm-s-ipython span.cm-qualifier{color:#555}.cm-s-ipython span.cm-builtin{8000}.cm-s-ipython span.cm-bracket{color:#997}.cm-s-ipython span.cm-tag{color:#170}.cm-s-ipython span.cm-attribute{c}.cm-s-ipython span.cm-header{color:blue}.cm-s-ipython span.cm-quote{90}.cm-s-ipython span.cm-link{c}.cm-s-ipython span.cm-error{color:#f00}.cm-s-ipython span.cm-tab{background:url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADAAAAAMCAYAAAAkuj5RAAAAAXNSR0IArs4c6QAAAGFJREFUSMft1LsRQFAQheHPowAKoACx3IgEKtaEHujDjORSgWTH/ZOdnZOcM/sgk/kFFWY0qV8foQwS4MKBCS3qR6ixBJvElOobYAtivseIE120FaowJPN75GMu8j/LfMwNjh4HUpwg4LUAAAAASUVORK5CYII=);background-position:right;background-repeat:no-repeat}div.output_wrapper{position:relative;display:-webkit-box;-webkit-box-orient:vertical;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:vertical;-moz-box-align:stretch;display:box;box-orient:vertical;box-align:stretch;display:flex;flex-direction:column;align-items:stretch}div.output_scroll{height:24em;width:100%;overflow:auto;border-radius:2px;-webkit-box-shadow:inset 0 2px 8px rgba(0,0,0,0.8);box-shadow:inset 0 2px 8px rgba(0,0,0,0.8);display:block}div.output_collapsed{margin:0;padding:0;display:-webkit-box;-webkit-box-orient:vertical;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:vertical;-moz-box-align:stretch;display:box;box-orient:vertical;box-align:stretch;display:flex;flex-direction:column;align-items:stretch}div.out_prompt_overlay{height:100%;padding:0 .4em;position:absolute;border-radius:2px}div.out_prompt_overlay:hover{-webkit-box-shadow:inset 0 0 1px #000;box-shadow:inset 0 0 1px #000;background:rgba(240,240,240,0.5)}div.output_prompt{color:darkred}div.output_area{padding:0;page-break-inside:avoid;display:-webkit-box;-webkit-box-orient:horizontal;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:horizontal;-moz-box-align:stretch;display:box;box-orient:horizontal;box-align:stretch;display:flex;flex-direction:row;align-items:stretch}div.output_area .MathJax_Display{text-align:left !important}div.output_area div.output_area .output{display:-webkit-box;-webkit-box-orient:vertical;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:vertical;-moz-box-align:stretch;display:box;box-orient:vertical;box-align:stretch;display:flex;flex-direction:column;align-items:stretch}@media (max-width:540px){div.output_area{display:-webkit-box;-webkit-box-orient:vertical;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:vertical;-moz-box-align:stretch;display:box;box-orient:vertical;box-align:stretch;display:flex;flex-direction:column;align-items:stretch}}div.output_area pre{margin:0;padding:0;border:0;vertical-align:baseline;color:black;background-color:transparent;border-radius:0}div.output_subarea{padding:.4em;-webkit-box-flex:1;-moz-box-flex:1;box-flex:1;flex:1}div.output_text{text-align:left;line-height:1.21429em}div.output_stderr{background:#fdd}div.output_latex{text-align:left}div.output_javascript:empty{padding:0}.js-error{color:darkred}div.raw_input_container{font-family:monospace;padding-top:5px}input.raw_input{font-family:inherit;font-size:inherit;color:inherit;width:auto;vertical-align:baseline;padding:0 .25em;margin:0 .25em}input.raw_input:focus{box-shadow:none}p.p-space{margin-bottom:10px}div.output_unrecognized{padding:5px;font-weight:bold;color:red}div.output_unrecognized a{color:inherit;text-decoration:none}div.output_unrecognized a:hover{color:inherit;text-decoration:none}.rendered_html{}.rendered_html :link{text-decoration:underline}.rendered_html :visited{text-decoration:underline}.rendered_html h1:first-child{margin-top:.538em}.rendered_html h2:first-child{margin-top:.636em}.rendered_html h3:first-child{margin-top:.777em}.rendered_html h4:first-child{margin-top:1em}.rendered_html h5:first-child{margin-top:1em}.rendered_html h6:first-child{margin-top:1em}.rendered_html *+ul{margin-top:1em}.rendered_html *+ol{margin-top:1em}.rendered_html *+table{margin-top:1em}.rendered_html *+p{margin-top:1em}.rendered_html *+img{margin-top:1em}div.text_cell{display:-webkit-box;-webkit-box-orient:horizontal;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:horizontal;-moz-box-align:stretch;display:box;box-orient:horizontal;box-align:stretch;display:flex;flex-direction:row;align-items:stretch}@media (max-width:540px){div.text_cell>div.prompt{display:none}}div.text_cell_render{outline:none;resize:none;width:inherit;border-style:none;padding:.5em .5em .5em .4em;box-sizing:border-box;-moz-box-sizing:border-box;-webkit-box-sizing:border-box}a.anchor-link:link{text-decoration:none;padding:0 20px;visibility:hidden}h1:hover .anchor-link,h2:hover .anchor-link,h3:hover .anchor-link,h4:hover .anchor-link,h5:hover .anchor-link,h6:hover .anchor-link{visibility:visible}.text_cell.rendered .input_area{display:none}.text_cell.unrendered .text_cell_render{display:none}.cm-header-1,.cm-header-2,.cm-header-3,.cm-header-4,.cm-header-5,.cm-header-6{font-weight:bold;font-family:\"Helvetica Neue\",Helvetica,Arial,sans-serif}.cm-header-1{font-size:185.7%}.cm-header-2{font-size:157.1%}.cm-header-3{font-size:128.6%}.cm-header-4{font-size:110%}.cm-header-5{font-size:100%;font-style:italic}.cm-header-6{font-size:100%;font-style:italic}.widget-interact>div,.widget-interact>input{padding:2.5px}.widget-area{page-break-inside:avoid;display:-webkit-box;-webkit-box-orient:horizontal;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:horizontal;-moz-box-align:stretch;display:box;box-orient:horizontal;box-align:stretch;display:flex;flex-direction:row;align-items:stretch}.widget-area .widget-subarea{padding:.44em .4em .4em 1px;margin-left:6px;box-sizing:border-box;-moz-box-sizing:border-box;-webkit-box-sizing:border-box;display:-webkit-box;-webkit-box-orient:vertical;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:vertical;-moz-box-align:stretch;display:box;box-orient:vertical;box-align:stretch;display:flex;flex-direction:column;align-items:stretch;-webkit-box-flex:2;-moz-box-flex:2;box-flex:2;flex:2;-webkit-box-align:start;-moz-box-align:start;box-align:start;align-items:flex-start}.widget-area.connection-problems .prompt:after{content:\"\\f127\";font-family:'FontAwesome';color:#d9534f;font-size:14px;top:3px;padding:3px}.slide-track{border:1px solid #ccc;background:#fff;border-radius:2px}.widget-hslider{padding-left:8px;padding-right:2px;overflow:visible;width:350px;height:5px;max-height:5px;margin-top:13px;margin-bottom:10px;border:1px solid #ccc;background:#fff;border-radius:2px;display:-webkit-box;-webkit-box-orient:horizontal;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:horizontal;-moz-box-align:stretch;display:box;box-orient:horizontal;box-align:stretch;display:flex;flex-direction:row;align-items:stretch}.widget-hslider .ui-slider{border:0;background:none;display:-webkit-box;-webkit-box-orient:horizontal;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:horizontal;-moz-box-align:stretch;display:box;box-orient:horizontal;box-align:stretch;display:flex;flex-direction:row;align-items:stretch;-webkit-box-flex:1;-moz-box-flex:1;box-flex:1;flex:1}.widget-hslider .ui-slider .ui-slider-handle{width:12px;height:28px;margin-top:-8px;border-radius:2px}.widget-hslider .ui-slider .ui-slider-range{height:12px;margin-top:-4px;background:#eee}.widget-vslider{padding-bottom:5px;overflow:visible;width:5px;max-width:5px;height:250px;margin-left:12px;border:1px solid #ccc;background:#fff;border-radius:2px;display:-webkit-box;-webkit-box-orient:vertical;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:vertical;-moz-box-align:stretch;display:box;box-orient:vertical;box-align:stretch;display:flex;flex-direction:column;align-items:stretch}.widget-vslider .ui-slider{border:0;background:none;margin-left:-4px;margin-top:5px;display:-webkit-box;-webkit-box-orient:vertical;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:vertical;-moz-box-align:stretch;display:box;box-orient:vertical;box-align:stretch;display:flex;flex-direction:column;align-items:stretch;-webkit-box-flex:1;-moz-box-flex:1;box-flex:1;flex:1}.widget-vslider .ui-slider .ui-slider-handle{width:28px;height:12px;margin-left:-9px;border-radius:2px}.widget-vslider .ui-slider .ui-slider-range{width:12px;margin-left:-1px;background:#eee}.widget-text{width:350px;margin:0}.widget-listbox{width:350px;margin-bottom:0}.widget-numeric-text{width:150px;margin:0}.widget-progress{margin-top:6px;min-width:350px}.widget-progress .progress-bar{-webkit-transition:none;-moz-transition:none;-ms-transition:none;-o-transition:none;transition:none}.widget-combo-btn{min-width:125px}.widget_item .dropdown-menu li a{color:inherit}.widget-hbox{display:-webkit-box;-webkit-box-orient:horizontal;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:horizontal;-moz-box-align:stretch;display:box;box-orient:horizontal;box-align:stretch;display:flex;flex-direction:row;align-items:stretch}.widget-hbox input[type=\"checkbox\"]{margin-top:9px;margin-bottom:10px}.widget-hbox .widget-label{min-width:10ex;padding-right:8px;padding-top:5px;text-align:right;vertical-align:text-top}.widget-hbox .widget-readout{padding-left:8px;padding-top:5px;text-align:left;vertical-align:text-top}.widget-vbox{display:-webkit-box;-webkit-box-orient:vertical;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:vertical;-moz-box-align:stretch;display:box;box-orient:vertical;box-align:stretch;display:flex;flex-direction:column;align-items:stretch}.widget-vbox .widget-label{padding-bottom:5px;text-align:center;vertical-align:text-bottom}.widget-vbox .widget-readout{padding-top:5px;text-align:center;vertical-align:text-top}.widget-box{box-sizing:border-box;-moz-box-sizing:border-box;-webkit-box-sizing:border-box;-webkit-box-align:start;-moz-box-align:start;box-align:start;align-items:flex-start}.widget-radio-box{display:-webkit-box;-webkit-box-orient:vertical;-webkit-box-align:stretch;display:-moz-box;-moz-box-orient:vertical;-moz-box-align:stretch;display:box;box-orient:vertical;box-align:stretch;display:flex;flex-direction:column;align-items:stretch;box-sizing:border-box;-moz-box-sizing:border-box;-webkit-box-sizing:border-box;padding-top:4px}.widget-radio-box label{margin-top:0}.widget-radio{margin-left:20px} .highlight .hll { background-color: #ffffcc } .highlight { background: #f8f8f8; } .highlight .c { color: #408080; font-style: italic } /* Comment */ .highlight .err { border: 1px solid #FF0000 } /* Error */ .highlight .k { color: #008000; font-weight: bold } /* Keyword */ .highlight .o { color: #666666 } /* Operator */ .highlight .cm { color: #408080; font-style: italic } /* Comment.Multiline */ .highlight .cp { color: #BC7A00 } /* Comment.Preproc */ .highlight .c1 { color: #408080; font-style: italic } /* Comment.Single */ .highlight .cs { color: #408080; font-style: italic } /* Comment.Special */ .highlight .gd { color: #A00000 } /* Generic.Deleted */ .highlight .ge { font-style: italic } /* Generic.Emph */ .highlight .gr { color: #FF0000 } /* Generic.Error */ .highlight .gh { color: #000080; font-weight: bold } /* Generic.Heading */ .highlight .gi { color: #00A000 } /* Generic.Inserted */ .highlight .go { color: #888888 } /* Generic.Output */ .highlight .gp { color: #000080; font-weight: bold } /* Generic.Prompt */ .highlight .gs { font-weight: bold } /* Generic.Strong */ .highlight .gu { color: #800080; font-weight: bold } /* Generic.Subheading */ .highlight .gt { color: #0044DD } /* Generic.Traceback */ .highlight .kc { color: #008000; font-weight: bold } /* Keyword.Constant */ .highlight .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */ .highlight .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */ .highlight .kp { color: #008000 } /* Keyword.Pseudo */ .highlight .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */ .highlight .kt { color: #B00040 } /* Keyword.Type */ .highlight .m { color: #666666 } /* Literal.Number */ .highlight .s { color: #BA2121 } /* Literal.String */ .highlight .na { color: #7D9029 } /* Name.Attribute */ .highlight .nb { color: #008000 } /* Name.Builtin */ .highlight .nc { color: #0000FF; font-weight: bold } /* Name.Class */ .highlight .no { color: #880000 } /* Name.Constant */ .highlight .nd { color: #AA22FF } /* Name.Decorator */ .highlight .ni { color: #999999; font-weight: bold } /* Name.Entity */ .highlight .ne { color: #D2413A; font-weight: bold } /* Name.Exception */ .highlight .nf { color: #0000FF } /* Name.Function */ .highlight .nl { color: #A0A000 } /* Name.Label */ .highlight .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */ .highlight .nt { color: #008000; font-weight: bold } /* Name.Tag */ .highlight .nv { color: #19177C } /* Name.Variable */ .highlight .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */ .highlight .w { color: #bbbbbb } /* Text.Whitespace */ .highlight .mb { color: #666666 } /* Literal.Number.Bin */ .highlight .mf { color: #666666 } /* Literal.Number.Float */ .highlight .mh { color: #666666 } /* Literal.Number.Hex */ .highlight .mi { color: #666666 } /* Literal.Number.Integer */ .highlight .mo { color: #666666 } /* Literal.Number.Oct */ .highlight .sb { color: #BA2121 } /* Literal.String.Backtick */ .highlight .sc { color: #BA2121 } /* Literal.String.Char */ .highlight .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */ .highlight .s2 { color: #BA2121 } /* Literal.String.Double */ .highlight .se { color: #BB6622; font-weight: bold } /* Literal.String.Escape */ .highlight .sh { color: #BA2121 } /* Literal.String.Heredoc */ .highlight .si { color: #BB6688; font-weight: bold } /* Literal.String.Interpol */ .highlight .sx { color: #008000 } /* Literal.String.Other */ .highlight .sr { color: #BB6688 } /* Literal.String.Regex */ .highlight .s1 { color: #BA2121 } /* Literal.String.Single */ .highlight .ss { color: #19177C } /* Literal.String.Symbol */ .highlight .bp { color: #008000 } /* Name.Builtin.Pseudo */ .highlight .vc { color: #19177C } /* Name.Variable.Class */ .highlight .vg { color: #19177C } /* Name.Variable.Global */ .highlight .vi { color: #19177C } /* Name.Variable.Instance */ .highlight .il { color: #666666 } /* Literal.Number.Integer.Long */ Hierarchical Linear Regression in PyMC2 $$\\\\[2pt]$$ Igor Marfin [Unister Gmb@2014] < igor.marfin@unister.de > $$\\\\[40pt]$$ Abstract More details on this study can be found in my bitbucket repository https://bitbucket.org/iggy_floyd/bayesian-pymc-linear-regression-model . Writting this tutorial was inspired by the blog [1] . I would like to quote a few statements as my motivation: In this blog post we will: provide and intuitive explanation of hierarchical/multi-level Bayesian modeling; show how this type of model can easily be built and estimated in PyMC2; demonstrate the advantage of using hierarchical Bayesian modelling as opposed to non-hierarchical Bayesian modelling by comparing the two; visualize the \"shrinkage effect\" (explained below); and Having multiple sets of related measurements comes up all the time. In mathematical psychology, for example, you test multiple subjects on the same task. We then want to estimate a computational/mathematical model that describes the behavior on the task by a set of parameters. We could thus fit a model to each subject individually, assuming they share no similarities; or, pool all the data and estimate one model assuming all subjects are identical. Hierarchical modeling allows the best of both worlds by modeling subjects' similarities but also allowing estimiation of individual parameters. As an aside, software from our lab, HDDM, allows hierarchical Bayesian estimation of a widely used decision making model in psychology. In this blog post, however, we will use a more classical example of hierarchical linear regression to predict radon levels in houses. $$\\\\[5pt]$$ Outline Abstract Initialization of the notebook Getting and cleaning the data The Model Model based on pooled measurements Diagnostics Model based on unpooled measurements Model based on partly-pooled measurements Visualization of predictions Plotting of the fitted parameters Shrinkage plot Plotting of radon levels of different counties. Goodness of the Fit(GoF) Geweke statistics 95% CI and Gelman-Rubin statistics plots Freeman-Tukey statistics and discrepancy plots Optional matherial The model with group-level predictors References $$\\\\[5pt]$$ Initialization To set up the python environment for the data analysis and make a nicer style of the notebook, one can run the following commands in the beginning of our modeling: In [11]: import sys sys . path = [ '/usr/local/lib/python2.7/dist-packages' ] + sys . path # to fix the problem with numpy: this replaces 1.6 version by 1.9 % matplotlib inline % pylab inline ion () import os import matplotlib import numpy as np import matplotlib.pyplot as pl import matplotlib as mpl import logging import pymc as pm # a plotter and dataframe modules import seaborn as sns # seaborn to make a nice plots of the data import pandas as pd import scipy.stats as stats # Set up logging. logger = logging . getLogger () logger . setLevel ( logging . INFO ) from book_format import load_style , figsize , set_figsize load_style ( \".\" , \"/styles/custom2.css\" ) Populating the interactive namespace from numpy and matplotlib WARNING: pylab import has clobbered these variables: ['figsize'] `%matplotlib` prevents importing * from pylab and numpy Out[11]: @import url('http://fonts.googleapis.com/css?family=Source+Code+Pro'); @import url('http://fonts.googleapis.com/css?family=Vollkorn'); @import url('http://fonts.googleapis.com/css?family=Arimo'); div.cell{ width: 1200px; margin-left: 0% !important; margin-right: auto; } div.text_cell code { background: transparent; color: #000000; font-weight: 600; font-size: 11pt; font-style: bold; font-family: 'Source Code Pro', Consolas, monocco, monospace; } h1 { font-family: 'Open sans',verdana,arial,sans-serif; } div.input_area { background: #F6F6F9; border: 1px solid #586e75; } .text_cell_render h1 { font-weight: 200; font-size: 30pt; line-height: 100%; color:#c76c0c; margin-bottom: 0.5em; margin-top: 1em; display: block; white-space: wrap; } h2 { font-family: 'Open sans',verdana,arial,sans-serif; } .text_cell_render h2 { font-weight: 200; font-size: 16pt; font-style: italic; line-height: 100%; color:#c76c0c; margin-bottom: 0.5em; margin-top: 1.5em; display: inline; white-space: wrap; } h3 { font-family: 'Open sans',verdana,arial,sans-serif; } .text_cell_render h3 { font-weight: 200; font-size: 14pt; line-height: 100%; color:#d77c0c; margin-bottom: 0.5em; margin-top: 2em; display: block; white-space: nowrap; } h4 { font-family: 'Open sans',verdana,arial,sans-serif; } .text_cell_render h4 { font-weight: 100; font-size: 14pt; color:#d77c0c; margin-bottom: 0.5em; margin-top: 0.5em; display: block; white-space: nowrap; } h5 { font-family: 'Open sans',verdana,arial,sans-serif; } .text_cell_render h5 { font-weight: 200; font-style: normal; color: #1d3b84; font-size: 16pt; margin-bottom: 0em; margin-top: 0.5em; display: block; white-space: nowrap; } div.text_cell_render{ font-family: 'Arimo',verdana,arial,sans-serif; line-height: 125%; font-size: 120%; text-align:justify; text-justify:inter-word; } div.output_subarea.output_text.output_pyout { overflow-x: auto; overflow-y: scroll; max-height: 50000px; } div.output_subarea.output_stream.output_stdout.output_text { overflow-x: auto; overflow-y: scroll; max-height: 50000px; } div.output_wrapper{ margin-top:0.2em; margin-bottom:0.2em; } code{ font-size: 70%; } .rendered_html code{ background-color: transparent; } ul{ margin: 2em; } ul li{ padding-left: 0.5em; margin-bottom: 0.5em; margin-top: 0.5em; } ul li li{ padding-left: 0.2em; margin-bottom: 0.2em; margin-top: 0.2em; } ol{ margin: 2em; } ol li{ padding-left: 0.5em; margin-bottom: 0.5em; margin-top: 0.5em; } ul li{ padding-left: 0.5em; margin-bottom: 0.5em; margin-top: 0.2em; } a:link{ font-weight: bold; color:#447adb; } a:visited{ font-weight: bold; color: #1d3b84; } a:hover{ font-weight: bold; color: #1d3b84; } a:focus{ font-weight: bold; color:#447adb; } a:active{ font-weight: bold; color:#447adb; } .rendered_html :link { text-decoration: underline; } .rendered_html :hover { text-decoration: none; } .rendered_html :visited { text-decoration: none; } .rendered_html :focus { text-decoration: none; } .rendered_html :active { text-decoration: none; } .warning{ color: rgb( 240, 20, 20 ) } hr { color: #f3f3f3; background-color: #f3f3f3; height: 1px; } blockquote{ display:block; background: #fcfcfc; border-left: 5px solid #c76c0c; font-family: 'Open sans',verdana,arial,sans-serif; width:1000px; padding: 10px 10px 10px 10px; text-align:justify; text-justify:inter-word; } blockquote p { margin-bottom: 0; line-height: 125%; font-size: 100%; } MathJax.Hub.Config({ TeX: { extensions: [\"AMSmath.js\"] }, tex2jax: { inlineMath: [ ['$','$'], [\"\\\\(\",\"\\\\)\"] ], displayMath: [ ['$$','$$'], [\"\\\\[\",\"\\\\]\"] ] }, displayAlign: 'center', // Change this to 'center' to center equations. \"HTML-CSS\": { scale:100, availableFonts: [\"Neo-Euler\"], preferredFont: \"Neo-Euler\", webFont: \"Neo-Euler\", styles: {'.MathJax_Display': {\"margin\": 4}} } }); $$\\\\[5pt]$$ Getting and cleaning the data Today we are going to explore the presence of the radon gas in housesholds of the USA. Radon gas is known to be the highest cause of lung cancer in non-smokers. It is believed to be more strongly present in households containing a basement and to differ in amount present among types of soil. The Gelman et al.'s (2007) radon dataset would be a classic for hierarchical modeling. We'll try to make predictions of radon levels in different counties based on the county itself and the presence of a basement. In this tutorial we'll look at Minnesota, a state that contains 85 counties in which different measurements are taken, ranging from 2 to 116 measurements per county. First, we'll load the data from the github.com repository of the blog author: In [2]: ! mkdir data ! wget https://raw.githubusercontent.com/twiecki/WhileMyMCMCGentlySamples/master/content/downloads/notebooks/radon.csv -O data/radon.csv mkdir: cannot create directory `data': File exists --2015-08-08 13:28:29-- https://raw.githubusercontent.com/twiecki/WhileMyMCMCGentlySamples/master/content/downloads/notebooks/radon.csv Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.31.17.133 Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.31.17.133|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 157833 (154K) [text/plain] Saving to: `data/radon.csv' 100%[======================================>] 157,833 636K/s in 0.2s 2015-08-08 13:28:32 (636 KB/s) - `data/radon.csv' saved [157833/157833] In [2]: radon_data = pd . read_csv ( 'data/radon.csv' ) county_names = radon_data . county . unique () county_idx = radon_data [ 'county_code' ] . values n_counties = len ( radon_data . county . unique ()) county_data = pd . DataFrame ({ \"county_names\" : county_names , \"county_idx\" : radon_data [ 'county_code' ] . unique () }) # example of the data we have radon_data . head ( 4 ) Out[2]: Unnamed: 0 idnum state state2 stfips zip region typebldg floor room ... pcterr adjwt dupflag zipflag cntyfips county fips Uppm county_code log_radon 0 0 5081 MN MN 27 55735 5 1 1 3 ... 9.7 1146.499190 1 0 1 AITKIN 27001 0.502054 0 0.832909 1 1 5082 MN MN 27 55748 5 1 0 4 ... 14.5 471.366223 0 0 1 AITKIN 27001 0.502054 0 0.832909 2 2 5083 MN MN 27 55748 5 1 0 4 ... 9.6 433.316718 0 0 1 AITKIN 27001 0.502054 0 1.098612 3 3 5084 MN MN 27 56469 5 1 0 4 ... 24.3 461.623670 0 0 1 AITKIN 27001 0.502054 0 0.095310 4 rows × 30 columns We are interested only in a few fields of this dataset: 'county' -- the name of the county where the measurements were done 'log_radon' -- the logarithm of the radon level 'floor' -- a binary variable, telling whether the house has a basement (floor == 0) or not (floor == 1). In [4]: columns_of_interest = [ 'county' , 'log_radon' , 'floor' ] radon_data [ columns_of_interest ] . head ( 4 ) Out[4]: county log_radon floor 0 AITKIN 0.832909 1 1 AITKIN 0.832909 0 2 AITKIN 1.098612 0 3 AITKIN 0.095310 0 As you can see, we have multiple radon measurements -- one row for each house -- in a county and whether the house has a basement (floor == 0) or not (floor == 1). We are interested in whether having a basement increases the radon measured in the house. $$\\\\[5pt]$$ The Model There three possibilities. $$\\\\[5pt]$$ Pooling of measurements Our model is defined as the following formula: $$radon_{i,c}=\\alpha+\\beta\\times floor_{i,c}+\\varepsilon, $$ where $i$ represents the measurement, $c$ the county and floor contains a 0 or 1 if the house has a basement or not, respectively. Critically, we are only estimating one intercept and one slope for all measurements over all counties pooled together. $$\\\\[5pt]$$ Unpooled measurements: separate regressions Our model is: $$radon_{i,c}=\\alpha_c+\\beta_c\\times floor_{i,c}+\\varepsilon_c, $$ where $i$ represents the measurement, $c$ the county and floor contains a 0 or 1 if the house has a basement or not, respectively. Note that we added the subindex $c$ so we are estimating $n$ different $\\alpha$s and $\\beta$s -- one for each county. $$\\\\[5pt]$$ Partial pooling: Hierarchical Regression aka the best of both worlds Fortunately, there is a middle ground to both of these extremes. Specifically, we may assume that while $\\alpha$s and $\\beta$s are different for each county as in the unpooled case, the coefficients all share some similarity. We can model this by assuming that each individual coefficient comes from a common group distribution: $$\\alpha_c∼N(\\mu_\\alpha,\\sigma&#94;2_\\alpha),$$$$\\beta_c∼N(\\mu_\\beta,\\sigma&#94;2_\\beta).$$ We thus assume the intercepts $\\alpha_c$ and slopes $\\beta_c$ to come from a normal distribution centered around their respective group mean $\\mu$ with a certain standard deviation $\\sigma&#94;2$, the values (or rather posteriors) of which we also estimate. That's why this is called a multilevel, hierarchical or partial-pooling modeling. Before we consider all three cases, we want to create the skeleton (basic) class of our models as well as the code splitting the data on train/test datasets. In [5]: ''' Igor Marfin <Unister Gmb, 2014> igor.marfin@unister.de A Base Class definitions. ''' # Define a function for splitting train/test data. def split_train_test ( data , percent_test = 0.1 ): \"\"\"Split the data into train/test sets. :param int percent_test: Percentage of data to use for testing. Default 10%. \"\"\" from sklearn.cross_validation import train_test_split train , test = train_test_split ( data , test_size = percent_test ) return train , test # Define our evaluation function: RMSE - root mean squared error function def rmse ( test_data , predicted ): \"\"\" Calculate root mean squared error. Ignoring missing values in the test data. \"\"\" I = ~ np . isnan ( test_data ) # indicator for missing values N = I . sum () # number of non-missing values sqerror = abs ( test_data - predicted ) ** 2 # squared error array mse = sqerror [ I ] . sum () / N # mean squared error return np . sqrt ( mse ) # Define the Basic class class BaseModel ( object ): \"\"\" Base Class\"\"\" def __init__ ( self , train_data ): # how many periods of the measurement we have self . num_entries = len ( train_data ) self . data = train_data self . traces = None self . predicted = None def predict ( self , train_data ): raise NotImplementedError ( 'prediction not implemented for base class' ) def prepare_trace ( self ): raise NotImplementedError ( 'preparing the traces not implemented for base class' ) def rmse ( self , test_data ): \"\"\"Calculate root mean squared error for predictions on test data.\"\"\" # rmse is a global function defined before return rmse ( test_data , self . predicted ) $$\\\\[5pt]$$ Model based on pooled measurements The code of the model is given below. In [6]: ''' Igor Marfin <Unister Gmb, 2014> igor.marfin@unister.de A Model based on the pooled data ''' #defines the class for the Modeling class ModelPooled ( BaseModel ): \"\"\" A ModelPooled class :param np.ndarray data: The data to use for learning the model. \"\"\" def __init__ ( self , data ): logging . info ( 'building the model....' ) super ( ModelPooled , self ) . __init__ ( data ) alpha_value_init = np . mean ( data . loc [ data [ 'floor' ] == 0 , 'log_radon' ] . values ) beta_value_init = np . mean ( data . loc [ data [ 'floor' ] == 1 , 'log_radon' ] . values ) beta_value_init -= alpha_value_init # alpha and beta priors self . alpha = pm . Normal ( \"alpha\" , mu = 0. , #tau=100, tau = 100 **- 1 , # tau is singma&#94;{-1} value = alpha_value_init ) self . beta = pm . Normal ( \"beta\" , mu = 0. , #tau=100, tau = 100 **- 1 , value = beta_value_init ) # error prior self . eps = pm . Uniform ( 'eps' , lower = 0 , upper = 100 ) # log of the radon level # Linear model of the log of the radon level @pm.deterministic def log_radon ( alpha = self . alpha , beta = self . beta , floor = self . data [ \"floor\" ] . values ): return alpha + beta * floor # what we observed is self . radon_observed = pm . Normal ( 'radon_observed' , mu = log_radon , tau = self . eps , value = self . data [ \"log_radon\" ] . values , observed = True ) # our model collects all definitions in self.model self . model = pm . MCMC ([ self . alpha , self . beta , self . eps , log_radon , self . radon_observed ]) logging . info ( 'done building the model' ) def _prepare_trace ( self ): ''' prepare a dataframe with traces of the alpha,beta and eps parameters of our model ''' self . traces = pd . DataFrame ({ 'alpha' : self . alpha . trace (), 'beta' : self . beta . trace (), 'eps' : self . eps . trace (), }) return self . traces # Update the interface of our model ModelPooled . prepare_trace = _prepare_trace train , test = split_train_test ( radon_data [ columns_of_interest ]) # fix of bug in the old sklearn train , test = pd . DataFrame ( train , columns = columns_of_interest ), pd . DataFrame ( test , columns = columns_of_interest ) train , test = train . convert_objects ( convert_numeric = True ), test . convert_objects ( convert_numeric = True ), pooled_model = ModelPooled ( train ) pooled_model . model . sample ( 100000 , 5000 , 20 ) INFO:root:building the model.... INFO:root:done building the model [-----------------100%-----------------] 100000 of 100000 complete in 22.0 sec $$\\\\[5pt]$$ Diagnostics Let's see if/how the model converged. In [7]: sns . set ( style = \"darkgrid\" ) test [ \"log_radon\" ] . hist () # alpha parameter pm . Matplot . plot ( pooled_model . alpha , path = 'plots/' ) # beta parameter pm . Matplot . plot ( pooled_model . beta , path = 'plots/' ) Plotting alpha Plotting beta We need to extend our model by prediction method to calculate the RMSE. In [8]: def _predict ( self , test_data , alpha = None , beta = None , eps = None ): ''' predicts results on the test data ''' if ( alpha is None ): alpha = self . alpha . value if ( beta is None ): beta = self . beta . value if ( eps is None ): eps = self . eps . value self . predicted = np . random . normal ( alpha + beta * test_data [ 'floor' ] . values , eps ) return self . predicted # Update the interface of our model ModelPooled . predict = _predict In [9]: pooled_model . predict ( train ) print \"RMSE on train data: \" , pooled_model . rmse ( train [ \"log_radon\" ] . values ) pooled_model . predict ( test ) print \"RMSE on test data: \" , pooled_model . rmse ( test [ \"log_radon\" ] . values ) RMSE on train data: 1.86659266777 RMSE on test data: 1.85661665164 It is has a particular interest to see how RMSE develops in MCMC chain. We will introduce the function running_rmse to plot such behavior. This is a so-called traceplot of the RSME. We'll compute RMSE for both the train and the test set, even though the convergence is indicated by RMSE on the training set alone. In addition, let's compute a running RMSE on the train/test sets to see how aggregate performance improves or decreases as we continue to sample. In [10]: def _running_rmse ( self , test_data , train_data , burn_in = 0 , plot = True ): \"\"\"Calculate RMSE for each step of the trace to monitor convergence. \"\"\" traces = self . prepare_trace () burn_in = burn_in if len ( traces ) >= burn_in else 0 results = { 'per-step-train' : [], 'running-train' : [], 'per-step-test' : [], 'running-test' : []} R_train = np . zeros ( len ( train_data )) R_test = np . zeros ( len ( test_data )) for cnt in range ( burn_in , len ( traces )): alpha = traces . loc [ cnt , \"alpha\" ] beta = traces . loc [ cnt , \"beta\" ] eps = traces . loc [ cnt , \"eps\" ] predicted_train = self . predict ( train_data , alpha , beta , eps ) predicted_test = self . predict ( test_data , alpha , beta , eps ) R_train += predicted_train R_test += predicted_test running_R_train = R_train / ( cnt + 1 ) running_R_test = R_test / ( cnt + 1 ) results [ 'per-step-train' ] . append ( rmse ( train_data [ \"log_radon\" ] . values , predicted_train )) results [ 'running-train' ] . append ( rmse ( train_data [ \"log_radon\" ] . values , running_R_train )) results [ 'per-step-test' ] . append ( rmse ( test_data [ \"log_radon\" ] . values , predicted_test )) results [ 'running-test' ] . append ( rmse ( test_data [ \"log_radon\" ] . values , running_R_test )) results = pd . DataFrame ( results ) if plot : results . plot ( kind = 'line' , grid = False , figsize = ( 15 , 7 ), title = 'Per-step and Running RMSE From Posterior Predictive' ) # Return the final predictions, and the RMSE calculations return running_R_train , running_R_test , results # Update the interface of ModelPooled ModelPooled . running_rmse = _running_rmse In [11]: _ , _ , results_pooled = pooled_model . running_rmse ( test , train , burn_in = 5000 ) In [12]: results_pooled . tail ( 3 ) Out[12]: per-step-test per-step-train running-test running-train 4747 1.767851 1.741515 0.831463 0.784821 4748 1.691075 1.716562 0.831458 0.784822 4749 1.914687 1.889621 0.831503 0.784813 $$\\\\[5pt]$$ Model based on unpooled measurements The code of the model is given below. In [182]: ''' Igor Marfin <Unister Gmb, 2014> igor.marfin@unister.de A Model based on the unpooled data ''' #defines the class for the Modeling class ModelUnPooled ( BaseModel ): \"\"\" A ModelUnPooled class :param np.ndarray data: The data to use for learning the model. :param np.ndarray aux_data: The auxiliary data to use for learning the model. \"\"\" def __init__ ( self , data , aux_data ): logging . info ( 'building the model....' ) super ( ModelUnPooled , self ) . __init__ ( data ) self . aux_data = aux_data # the data contain name of the county and its id self . alphas = {} # the container of alpha parameters for each county self . betas = {} # the container of beta parameters for each county self . epses = {} # the container of eps parameters for each county self . radon_observeds = {} # the container of observed for each county self . models = {} # the container of models for each county alpha_value_init = np . mean ( data . loc [ data [ 'floor' ] == 0 , 'log_radon' ] . values ) beta_value_init = np . mean ( data . loc [ data [ 'floor' ] == 1 , 'log_radon' ] . values ) beta_value_init -= alpha_value_init for idx in self . aux_data . index : county_name = self . aux_data . loc [ idx , \"county_names\" ] county_idx = self . aux_data . loc [ idx , \"county_idx\" ] print \"Training %s ....\" % county_name data_selected = self . data . ix [ self . data . county == county_name ] alpha_values = data_selected . loc [ data_selected [ 'floor' ] == 0 , 'log_radon' ] . values alpha_value_init_local = np . mean ( alpha_values ) if ( len ( alpha_values ) > 0 ) else alpha_value_init beta_values = data_selected . loc [ data_selected [ 'floor' ] == 1 , 'log_radon' ] . values beta_value_init_local = np . mean ( beta_values ) if ( len ( beta_values ) > 0 ) else beta_value_init beta_value_init_local -= alpha_value_init_local # alpha and beta priors self . alphas [ \"alpha_ %d \" % county_idx ] = pm.Normal(\"alpha_%d\"%county_idx, mu = 0. , #tau=100, tau = 100 **- 1 , # tau is singma&#94;{-1} value = alpha_value_init_local ) self . betas [ \"beta_ %d \" % county_idx ] = pm.Normal(\"beta_%d\"%county_idx, mu = 0. , #tau=100, tau = 100 **- 1 , # tau is singma&#94;{-1} value = beta_value_init_local ) self . epses [ \"eps_ %d \" % county_idx ] = pm.Uniform(\"eps_%d\"%county_idx, lower = 0 , upper = 100 ) # log of the radon level # Linear model of the log of the radon level @pm.deterministic def log_radon ( alpha = self . alphas [ \"alpha_ %d \" % county_idx ], beta = self . betas [ \"beta_ %d \" % county_idx ], floor = data_selected [ \"floor\" ] . values ): return alpha + beta * floor # what we observed is self . radon_observeds [ \"radon_observed_ %d \" % county_idx ] = pm.Normal(\"radon_observed_%d\"%county_idx, mu = log_radon , tau = self . epses [ \"eps_ %d \" % county_idx ], value = data_selected [ \"log_radon\" ] . values , observed = True ) # our model collects all definitions in self.model self . models [ \"model_ %d \" % county_idx ] = pm.MCMC([ self . alphas [ \"alpha_ %d \" % county_idx ], self . betas [ \"beta_ %d \" % county_idx ], self . epses [ \"eps_ %d \" % county_idx ], log_radon , self . radon_observeds [ \"radon_observed_ %d \" % county_idx ] ]) self . models [ \"model_ %d \" % county_idx ].sample(20000, 5000, 20) #self.models[\"model_%d\"%county_idx].sample(100000) print print \"Remains: %d ....\" % ( len ( self . aux_data ) - idx - 1 ) logging . info ( '...done building the model' ) unpooled_model = ModelUnPooled ( train , county_data ) INFO:root:building the model.... Training AITKIN .... [-----------------100%-----------------] 20000 of 20000 complete in 2.4 sec Remains: 84 .... Training ANOKA .... [-----------------100%-----------------] 20000 of 20000 complete in 2.5 sec Remains: 83 .... Training BECKER .... [-----------------100%-----------------] 20000 of 20000 complete in 2.4 sec Remains: 82 .... Training BELTRAMI .... [-----------------100%-----------------] 20000 of 20000 complete in 2.4 sec Remains: 81 .... Training BENTON .... [-----------------100%-----------------] 20000 of 20000 complete in 2.4 sec Remains: 80 .... Training BIG STONE .... [-----------------100%-----------------] 20000 of 20000 complete in 2.4 sec Remains: 79 .... Training BLUE EARTH .... [-----------------100%-----------------] 20000 of 20000 complete in 2.4 sec Remains: 78 .... Training BROWN .... [-----------------100%-----------------] 20000 of 20000 complete in 2.3 sec Remains: 77 .... Training CARLTON .... [-----------------100%-----------------] 20000 of 20000 complete in 2.4 sec Remains: 76 .... Training CARVER .... [-----------------100%-----------------] 20000 of 20000 complete in 2.4 sec Remains: 75 .... Training CASS .... [-----------------100%-----------------] 20000 of 20000 complete in 2.4 sec Remains: 74 .... Training CHIPPEWA .... [-----------------100%-----------------] 20000 of 20000 complete in 2.4 sec Remains: 73 .... Training CHISAGO .... [-----------------100%-----------------] 20000 of 20000 complete in 2.3 sec Remains: 72 .... Training CLAY .... [-----------------100%-----------------] 20000 of 20000 complete in 2.3 sec Remains: 71 .... Training CLEARWATER .... [-----------------100%-----------------] 20000 of 20000 complete in 2.3 sec Remains: 70 .... Training COOK .... [-----------------100%-----------------] 20000 of 20000 complete in 2.3 sec Remains: 69 .... Training COTTONWOOD .... [-----------------100%-----------------] 20000 of 20000 complete in 2.3 sec Remains: 68 .... Training CROW WING .... [-----------------100%-----------------] 20000 of 20000 complete in 2.4 sec Remains: 67 .... Training DAKOTA .... [-----------------100%-----------------] 20000 of 20000 complete in 2.6 sec Remains: 66 .... Training DODGE .... [-----------------100%-----------------] 20000 of 20000 complete in 2.4 sec Remains: 65 .... Training DOUGLAS .... [-----------------100%-----------------] 20000 of 20000 complete in 2.4 sec Remains: 64 .... Training FARIBAULT .... [-----------------100%-----------------] 20000 of 20000 complete in 2.3 sec Remains: 63 .... Training FILLMORE .... [-----------------100%-----------------] 20000 of 20000 complete in 2.3 sec Remains: 62 .... Training FREEBORN .... [-----------------100%-----------------] 20000 of 20000 complete in 2.3 sec Remains: 61 .... Training GOODHUE .... [-----------------100%-----------------] 20000 of 20000 complete in 2.4 sec Remains: 60 .... Training HENNEPIN .... [-----------------100%-----------------] 20000 of 20000 complete in 2.6 sec Remains: 59 .... Training HOUSTON .... [-----------------100%-----------------] 20000 of 20000 complete in 2.3 sec Remains: 58 .... Training HUBBARD .... [-----------------100%-----------------] 20000 of 20000 complete in 2.3 sec Remains: 57 .... Training ISANTI .... [-----------------100%-----------------] 20000 of 20000 complete in 2.3 sec Remains: 56 .... Training ITASCA .... [-----------------100%-----------------] 20000 of 20000 complete in 2.3 sec Remains: 55 .... Training JACKSON .... [-----------------100%-----------------] 20000 of 20000 complete in 2.3 sec Remains: 54 .... Training KANABEC .... [-----------------100%-----------------] 20000 of 20000 complete in 2.3 sec Remains: 53 .... Training KANDIYOHI .... [-----------------100%-----------------] 20000 of 20000 complete in 2.3 sec Remains: 52 .... Training KITTSON .... [-----------------100%-----------------] 20000 of 20000 complete in 2.2 sec Remains: 51 .... Training KOOCHICHING .... [-----------------100%-----------------] 20000 of 20000 complete in 2.3 sec Remains: 50 .... Training LAC QUI PARLE .... [-----------------100%-----------------] 20000 of 20000 complete in 2.2 sec Remains: 49 .... Training LAKE .... [-----------------100%-----------------] 20000 of 20000 complete in 2.3 sec Remains: 48 .... Training LAKE OF THE WOODS .... [-----------------100%-----------------] 20000 of 20000 complete in 2.3 sec Remains: 47 .... Training LE SUEUR .... [-----------------100%-----------------] 20000 of 20000 complete in 2.3 sec Remains: 46 .... Training LINCOLN .... [-----------------100%-----------------] 20000 of 20000 complete in 2.3 sec Remains: 45 .... Training LYON .... [-----------------100%-----------------] 20000 of 20000 complete in 2.3 sec Remains: 44 .... Training MAHNOMEN .... [-----------------100%-----------------] 20000 of 20000 complete in 2.2 sec Remains: 43 .... Training MARSHALL .... [-----------------100%-----------------] 20000 of 20000 complete in 2.3 sec Remains: 42 .... Training MARTIN .... [-----------------100%-----------------] 20000 of 20000 complete in 2.3 sec Remains: 41 .... Training MCLEOD .... [-----------------100%-----------------] 20000 of 20000 complete in 2.3 sec Remains: 40 .... Training MEEKER .... [-----------------100%-----------------] 20000 of 20000 complete in 2.3 sec Remains: 39 .... Training MILLE LACS .... [-----------------100%-----------------] 20000 of 20000 complete in 2.3 sec Remains: 38 .... Training MORRISON .... [-----------------100%-----------------] 20000 of 20000 complete in 2.4 sec Remains: 37 .... Training MOWER .... [-----------------100%-----------------] 20000 of 20000 complete in 2.3 sec Remains: 36 .... Training MURRAY .... [-----------------100%-----------------] 20000 of 20000 complete in 2.2 sec Remains: 35 .... Training NICOLLET .... [-----------------100%-----------------] 20000 of 20000 complete in 2.3 sec Remains: 34 .... Training NOBLES .... [-----------------100%-----------------] 20000 of 20000 complete in 2.2 sec Remains: 33 .... Training NORMAN .... [-----------------100%-----------------] 20000 of 20000 complete in 2.4 sec Remains: 32 .... Training OLMSTED .... [-----------------100%-----------------] 20000 of 20000 complete in 2.4 sec Remains: 31 .... Training OTTER TAIL .... [-----------------100%-----------------] 20000 of 20000 complete in 2.3 sec Remains: 30 .... Training PENNINGTON .... [-----------------100%-----------------] 20000 of 20000 complete in 2.3 sec Remains: 29 .... Training PINE .... [-----------------100%-----------------] 20000 of 20000 complete in 2.4 sec Remains: 28 .... Training PIPESTONE .... [-----------------100%-----------------] 20000 of 20000 complete in 2.3 sec Remains: 27 .... Training POLK .... [-----------------100%-----------------] 20000 of 20000 complete in 2.3 sec Remains: 26 .... Training POPE .... [-----------------100%-----------------] 20000 of 20000 complete in 2.3 sec Remains: 25 .... Training RAMSEY .... [-----------------100%-----------------] 20000 of 20000 complete in 2.3 sec Remains: 24 .... Training REDWOOD .... [-----------------100%-----------------] 20000 of 20000 complete in 2.3 sec Remains: 23 .... Training RENVILLE .... [-----------------100%-----------------] 20000 of 20000 complete in 2.2 sec Remains: 22 .... Training RICE .... [-----------------100%-----------------] 20000 of 20000 complete in 2.3 sec Remains: 21 .... Training ROCK .... [-----------------100%-----------------] 20000 of 20000 complete in 2.2 sec Remains: 20 .... Training ROSEAU .... [-----------------100%-----------------] 20000 of 20000 complete in 2.3 sec Remains: 19 .... Training SCOTT .... [-----------------100%-----------------] 20000 of 20000 complete in 2.3 sec Remains: 18 .... Training SHERBURNE .... [-----------------100%-----------------] 20000 of 20000 complete in 2.3 sec Remains: 17 .... Training SIBLEY .... [-----------------100%-----------------] 20000 of 20000 complete in 2.3 sec Remains: 16 .... Training ST LOUIS .... [-----------------100%-----------------] 20000 of 20000 complete in 2.6 sec Remains: 15 .... Training STEARNS .... [-----------------100%-----------------] 20000 of 20000 complete in 2.3 sec Remains: 14 .... Training STEELE .... [-----------------100%-----------------] 20000 of 20000 complete in 2.3 sec Remains: 13 .... Training STEVENS .... [-----------------100%-----------------] 20000 of 20000 complete in 2.3 sec Remains: 12 .... Training SWIFT .... [-----------------100%-----------------] 20000 of 20000 complete in 2.3 sec Remains: 11 .... Training TODD .... [-----------------100%-----------------] 20000 of 20000 complete in 2.2 sec Remains: 10 .... Training TRAVERSE .... [-----------------100%-----------------] 20000 of 20000 complete in 2.3 sec Remains: 9 .... Training WABASHA .... [-----------------100%-----------------] 20000 of 20000 complete in 2.3 sec Remains: 8 .... Training WADENA .... [-----------------100%-----------------] 20000 of 20000 complete in 2.3 sec Remains: 7 .... Training WASECA .... [-----------------100%-----------------] 20000 of 20000 complete in 2.3 sec Remains: 6 .... Training WASHINGTON .... [-----------------100%-----------------] 20000 of 20000 complete in 2.4 sec Remains: 5 .... Training WATONWAN .... [-----------------100%-----------------] 20000 of 20000 complete in 2.3 sec Remains: 4 .... Training WILKIN .... [-----------------100%-----------------] 20000 of 20000 complete in 2.2 sec Remains: 3 .... Training WINONA .... [-----------------100%-----------------] 20000 of 20000 complete in 2.3 sec Remains: 2 .... Training WRIGHT .... [-----------------100%-----------------] 20000 of 20000 complete in 2.3 sec Remains: 1 .... Training YELLOW MEDICINE .... [-----------------100%-----------------] 20000 of 20000 complete in 2.2 sec INFO:root:...done building the model Remains: 0 .... We need to define the prepare_trace method differently as we did for the pooled data. In [193]: def _prepare_trace ( self ): ''' prepare a dataframe with traces of the alpha,beta and eps parameters of our model ''' traces = {} for key in self . alphas . keys (): traces . update ({ key : self . alphas [ key ] . trace ()}) for key in self . betas . keys (): traces . update ({ key : self . betas [ key ] . trace ()}) for key in self . epses . keys (): traces . update ({ key : self . epses [ key ] . trace ()}) self . traces = pd . DataFrame ( traces ) return self . traces # Update the interface of our model ModelUnPooled . prepare_trace = _prepare_trace unpooled_model . prepare_trace () . head ( 2 ) Out[193]: alpha_0 alpha_1 alpha_10 alpha_11 alpha_12 alpha_13 alpha_14 alpha_15 alpha_16 alpha_17 ... eps_77 eps_78 eps_79 eps_8 eps_80 eps_81 eps_82 eps_83 eps_84 eps_9 0 1.114638 0.807775 1.432208 2.170566 1.254748 2.383997 1.214638 0.377129 8.958147 1.33757 ... 7.322074 1.061659 1.814589 4.225521 4.173853 20.558249 3.961421 3.131659 18.787163 0.116077 1 0.501935 0.982796 1.333656 1.967648 1.390526 1.965162 1.267309 0.682184 6.870853 1.22605 ... 2.689773 1.282816 1.633417 5.156670 78.630107 15.606603 5.697033 5.203274 5.427840 0.950671 2 rows × 255 columns Here is our new prediction method: In [194]: def _predict ( self , test_data , aux_data , alphas = None , betas = None , epses = None ): ''' predicts results on the test data ''' test_data_copy = test_data . copy () test_data_copy = pd . merge ( test_data_copy , aux_data , left_on = 'county' , right_on = 'county_names' , how = 'left' ) test_data_copy = test_data_copy . drop ( 'county_names' , 1 ) if ( alphas is None ): alphas = self . alphas if ( betas is None ): betas = self . betas if ( epses is None ): epses = self . epses test_data_copy [ 'log_radon_predict' ] = test_data_copy . apply ( lambda x : np . random . normal ( alphas [ \"alpha_ %d \" % ( x [ 'county_idx' ])] . value if hasattr ( alphas [ \"alpha_ %d \" % ( x [ 'county_idx' ])], \"value\" ) else alphas [ \"alpha_ %d \" % ( x [ 'county_idx' ])] + ( betas [ \"beta_ %d \" % ( x [ 'county_idx' ])] . value if hasattr ( betas [ \"beta_ %d \" % ( x [ 'county_idx' ])], \"value\" ) else betas [ \"beta_ %d \" % ( x [ 'county_idx' ])]) * x [ 'floor' ], epses [ \"eps_ %d \" % ( x [ 'county_idx' ])] . value if hasattr ( epses [ \"eps_ %d \" % ( x [ 'county_idx' ])], \"value\" ) else epses [ \"eps_ %d \" % ( x [ 'county_idx' ])] ), axis = 1 ) #self.predicted =np.random.normal(alpha + beta*test_data['floor'].values, eps) self . predicted = test_data_copy [ 'log_radon_predict' ] . values return self . predicted # Update the interface of our model ModelUnPooled . predict = _predict In [195]: unpooled_model . predict ( train , county_data ) print \"RMSE on train data: \" , unpooled_model . rmse ( train [ \"log_radon\" ] . values ) unpooled_model . predict ( test , county_data ) print \"RMSE on test data: \" , unpooled_model . rmse ( test [ \"log_radon\" ] . values ) RMSE on train data: 12.4104463716 RMSE on test data: 12.1123567478 Running RMSE for this Model we define as follows: In [196]: def _running_rmse ( self , test_data , train_data , aux_data , burn_in = 0 , plot = True ): \"\"\"Calculate RMSE for each step of the trace to monitor convergence. \"\"\" traces = self . prepare_trace () burn_in = burn_in if len ( traces ) >= burn_in else 0 results = { 'per-step-train' : [], 'running-train' : [], 'per-step-test' : [], 'running-test' : []} R_train = np . zeros ( len ( train_data )) R_test = np . zeros ( len ( test_data )) for cnt in range ( burn_in , len ( traces )): #if (cnt%1000 == 0): print \"Trace %d\"%cnt alphas = {} betas = {} epses = {} for key in self . alphas . keys (): alphas . update ({ key : traces . loc [ cnt , key ]}) for key in self . betas . keys (): betas . update ({ key : traces . loc [ cnt , key ]}) for key in self . epses . keys (): epses . update ({ key : traces . loc [ cnt , key ]}) predicted_train = self . predict ( train_data , aux_data , alphas , betas , epses ) predicted_test = self . predict ( test_data , aux_data , alphas , betas , epses ) R_train += predicted_train R_test += predicted_test running_R_train = R_train / ( cnt + 1 ) running_R_test = R_test / ( cnt + 1 ) results [ 'per-step-train' ] . append ( rmse ( train_data [ \"log_radon\" ] . values , predicted_train )) results [ 'running-train' ] . append ( rmse ( train_data [ \"log_radon\" ] . values , running_R_train )) results [ 'per-step-test' ] . append ( rmse ( test_data [ \"log_radon\" ] . values , predicted_test )) results [ 'running-test' ] . append ( rmse ( test_data [ \"log_radon\" ] . values , running_R_test )) results = pd . DataFrame ( results ) if plot : results . plot ( kind = 'line' , grid = False , figsize = ( 15 , 7 ), title = 'Per-step and Running RMSE From Posterior Predictive' ) # Return the final predictions, and the RMSE calculations return running_R_train , running_R_test , results # Update the interface of ModelPooled ModelUnPooled . running_rmse = _running_rmse In [197]: _ , _ , results_unpooled = unpooled_model . running_rmse ( test , train , county_data , burn_in = 5000 ) In [198]: results_unpooled . tail ( 3 ) Out[198]: per-step-test per-step-train running-test running-train 747 14.850666 12.968655 0.992773 0.771092 748 10.439846 10.874049 0.990719 0.772236 749 20.002211 9.972424 0.982994 0.771789 $$\\\\[5pt]$$ Model based on partly-pooled measurements This is the hierarchical model, which creates group parameters that consider the countys not as completely different but as having an underlying similarity. These distributions are subsequently used to influence the distribution of each county's $\\alpha$ and $\\beta$. The code of the model is given below. In [13]: ''' Igor Marfin <Unister Gmb, 2014> igor.marfin@unister.de A Model based on the partly-pooled data ''' #defines the class for the Modeling class ModelHierarchical ( BaseModel ): \"\"\" A ModelHierarchical class :param np.ndarray data: The data to use for learning the model. :param np.ndarray aux_data: The data to use for learning the model. \"\"\" def __init__ ( self , data , aux_data ): logging . info ( 'building the model....' ) super ( ModelHierarchical , self ) . __init__ ( data ) self . aux_data = aux_data # the data contain name of the county and its id self . county_idx = aux_data [ 'county_idx' ] . values alpha_value_init = np . mean ( data . loc [ data [ 'floor' ] == 0 , 'log_radon' ] . values ) beta_value_init = np . mean ( data . loc [ data [ 'floor' ] == 1 , 'log_radon' ] . values ) beta_value_init -= alpha_value_init # alpha and beta priors self . mu_alpha = pm . Normal ( \"mu_alpha\" , mu = 0. , #tau=100, tau = 100 **- 1 , # tau is singma&#94;{-1} value = alpha_value_init ) self . sigma_alpha = pm . Uniform ( 'sigma_alpha' , lower = 0 , upper = 100 ) self . mu_beta = pm . Normal ( \"mu_beta\" , mu = 0. , #tau=100, tau = 100 **- 1 , value = beta_value_init ) self . sigma_beta = pm . Uniform ( 'sigma_beta' , lower = 0 , upper = 100 ) self . alpha = pm . Normal ( \"alpha\" , mu = self . mu_alpha , tau = self . sigma_alpha , size = len ( self . aux_data ), value = np . zeros ( len ( self . aux_data )) ) self . beta = pm . Normal ( \"beta\" , mu = self . mu_beta , tau = self . sigma_beta , size = len ( self . aux_data ), value = np . zeros ( len ( self . aux_data )) ) # error prior self . eps = pm . Uniform ( 'eps' , lower = 0 , upper = 100 ) # new dataframe self . data_update = self . data . copy () self . data_update = pd . merge ( self . data_update , aux_data , left_on = 'county' , right_on = 'county_names' , how = 'left' ) self . data_update = self . data_update . drop ( 'county_names' , 1 ) # log of the radon level # Linear model of the log of the radon level @pm.deterministic def log_radon ( alpha = self . alpha , beta = self . beta , county_idx = self . data_update [ \"county_idx\" ] . values , floor = self . data_update [ \"floor\" ] . values ): return alpha [ county_idx ] + beta [ county_idx ] * floor # what we observed is self . radon_observed = pm . Normal ( 'radon_observed' , mu = log_radon , tau = self . eps , value = self . data [ \"log_radon\" ] . values , observed = True ) # our model collects all definitions in self.model self . model = pm . MCMC ([ self . mu_alpha , self . sigma_alpha , self . mu_beta , self . sigma_beta , self . alpha , self . beta , self . eps , log_radon , self . radon_observed ]) logging . info ( 'done building the model' ) def _prepare_trace ( self ): ''' prepare a dataframe with traces of the alpha,beta and eps parameters of our model ''' self . traces = { 'mu_alpha' : self . mu_alpha . trace (), 'sigma_alpha' : self . sigma_alpha . trace (), 'mu_beta' : self . mu_beta . trace (), 'sigma_beta' : self . sigma_beta . trace (), 'eps' : self . eps . trace (), } alphas = self . model . trace ( 'alpha' ) . gettrace () betas = self . model . trace ( 'beta' ) . gettrace () for indx in range ( self . alpha . value . shape [ 0 ]): self . traces . update ({ \"alpha_ %d \" % indx :alphas[:,indx]}) for indx in range ( self . beta . value . shape [ 0 ]): self . traces . update ({ \"beta_ %d \" % indx :betas[:,indx]}) self . traces = pd . DataFrame ( self . traces ) return self . traces # Update our Model ModelHierarchical . prepare_trace = _prepare_trace hierarchical_model = ModelHierarchical ( train , county_data ) hierarchical_model . model . sample ( 20000 , 5000 , 20 ) INFO:root:building the model.... INFO:root:done building the model [-----------------100%-----------------] 20000 of 20000 complete in 8.8 sec In [40]: # test of the shapes: variables and their traces print hierarchical_model . model . trace ( 'alpha' ) . gettrace () . shape print hierarchical_model . mu_alpha . trace () . shape print hierarchical_model . alpha . value . shape (750, 85) (750,) (85,) The predict and Running-RMSE methods are defined accordingly. In [14]: def _predict ( self , test_data , aux_data , alphas = None , betas = None , epses = None ): ''' predicts results on the test data ''' test_data_copy = test_data . copy () test_data_copy = pd . merge ( test_data_copy , aux_data , left_on = 'county' , right_on = 'county_names' , how = 'left' ) test_data_copy = test_data_copy . drop ( 'county_names' , 1 ) if ( alphas is None ): alphas = self . alpha . value if ( betas is None ): betas = self . beta . value if ( epses is None ): epses = self . eps . value test_data_copy [ 'log_radon_predict' ] = test_data_copy . apply ( lambda x : np . random . normal ( alphas [ x [ 'county_idx' ]] + betas [ x [ 'county_idx' ]] * x [ 'floor' ], epses ), axis = 1 ) self . predicted = test_data_copy [ 'log_radon_predict' ] . values return self . predicted # Update the interface of our model ModelHierarchical . predict = _predict In [15]: hierarchical_model . predict ( train , county_data ) print \"RMSE on train data: \" , hierarchical_model . rmse ( train [ \"log_radon\" ] . values ) hierarchical_model . predict ( test , county_data ) print \"RMSE on test data: \" , hierarchical_model . rmse ( test [ \"log_radon\" ] . values ) RMSE on train data: 1.99558981897 RMSE on test data: 2.32077127218 In [16]: def _running_rmse ( self , test_data , train_data , aux_data , burn_in = 0 , plot = True ): \"\"\"Calculate RMSE for each step of the trace to monitor convergence. \"\"\" traces = self . prepare_trace () burn_in = burn_in if len ( traces ) >= burn_in else 0 results = { 'per-step-train' : [], 'running-train' : [], 'per-step-test' : [], 'running-test' : []} R_train = np . zeros ( len ( train_data )) R_test = np . zeros ( len ( test_data )) for cnt in range ( burn_in , len ( traces )): alphas = [] betas = [] for indx in range ( self . alpha . value . shape [ 0 ]): alphas += [ traces . loc [ cnt , \"alpha_ %d \" % indx ]] for indx in range ( self . beta . value . shape [ 0 ]): betas += [ traces . loc [ cnt , \"beta_ %d \" % indx ]] eps = traces . loc [ cnt , \"eps\" ] predicted_train = self . predict ( train_data , aux_data , alphas , betas , eps ) predicted_test = self . predict ( test_data , aux_data , alphas , betas , eps ) R_train += predicted_train R_test += predicted_test running_R_train = R_train / ( cnt + 1 ) running_R_test = R_test / ( cnt + 1 ) results [ 'per-step-train' ] . append ( rmse ( train_data [ \"log_radon\" ] . values , predicted_train )) results [ 'running-train' ] . append ( rmse ( train_data [ \"log_radon\" ] . values , running_R_train )) results [ 'per-step-test' ] . append ( rmse ( test_data [ \"log_radon\" ] . values , predicted_test )) results [ 'running-test' ] . append ( rmse ( test_data [ \"log_radon\" ] . values , running_R_test )) results = pd . DataFrame ( results ) if plot : results . plot ( kind = 'line' , grid = False , figsize = ( 15 , 7 ), title = 'Per-step and Running RMSE From Posterior Predictive' ) # Return the final predictions, and the RMSE calculations return running_R_train , running_R_test , results # Update the interface of ModelPooled ModelHierarchical . running_rmse = _running_rmse In [17]: _ , _ , results_hierarchical = hierarchical_model . running_rmse ( test , train , county_data , burn_in = 5000 ) In [18]: results_hierarchical . tail ( 3 ) Out[18]: per-step-test per-step-train running-test running-train 747 2.356270 2.266139 0.783504 0.698728 748 2.348432 2.087505 0.784100 0.698816 749 2.287256 2.240132 0.784028 0.698708 As can be seen above the hierarchical model performs better than the non-hierarchical models in predicting the radon values. Following this, we'll plot some examples of county's models showing the actual radon measurements, the hierarchial predictions and the non-hierarchical predictions. $$\\\\[5pt]$$ Visualization We define several plot functions to demonstrate the predictions. $$\\\\[5pt]$$ Plotting of the fitted parameters In [57]: ''' Igor Marfin <Unister Gmb, 2014> igor.marfin@unister.de plotting support for discussed models ''' def plot_predictions ( models , data , aux_data , selection ): ''' plots predictions ''' sns . set_context ( \"poster\" ) plt . figure ( figsize = ( 15 , 8 )) fig = plt . figure ( figsize = ( 12 , 8 )) data_copy = data . copy () data_copy = pd . merge ( data_copy , aux_data , left_on = 'county' , right_on = 'county_names' , how = 'left' ) data_copy = data_copy . drop ( 'county_names' , 1 ) xvals = np . linspace ( - 0.2 , 1.2 ) # range on the x-axis for i , c in enumerate ( selection ): c_data = data_copy . ix [ data_copy [ \"county\" ] == c ] c_data = c_data . reset_index ( drop = True ) idx = list ( c_data [ 'county_idx' ])[ 0 ] # contains county_idx for the selected county fig . add_subplot ( str ( len ( selection )) + '1' + str ( i + 1 )) for model in models : if type ( model ) == ModelPooled : traces = model . prepare_trace () alpha_trace = traces [ \"alpha\" ] . values [ 500 :: 10 ] beta_trace = traces [ \"beta\" ] . values [ 500 :: 10 ] for j in range ( len ( alpha_trace )): plot ( xvals , alpha_trace [ j ] + beta_trace [ j ] * xvals , 'b' , alpha =. 1 ) plot ( xvals , alpha_trace . mean () + beta_trace . mean () * xvals , 'b' , label = \"pooled\" ) elif type ( model ) == ModelUnPooled : traces = model . prepare_trace () alpha_trace = traces [ \"alpha_ %d \" % idx ].values[500::10] beta_trace = traces [ \"beta_ %d \" % idx ].values[500::10] #alpha_trace=traces[\"alpha_%d\"%idx].values[50000::300] #beta_trace=traces[\"beta_%d\"%idx].values[50000::300] for j in range ( len ( alpha_trace )): plot ( xvals , alpha_trace [ j ] + beta_trace [ j ] * xvals , 'r' , alpha =. 1 ) plot ( xvals , alpha_trace . mean () + beta_trace . mean () * xvals , 'r' , label = \"unpooled\" ) elif type ( model ) == ModelHierarchical : traces = model . prepare_trace () alpha_trace = traces [ \"alpha_ %d \" % idx ].values[500::10] beta_trace = traces [ \"beta_ %d \" % idx ].values[500::10] for j in range ( len ( alpha_trace )): plot ( xvals , alpha_trace [ j ] + beta_trace [ j ] * xvals , 'g' , alpha =. 1 ) plot ( xvals , alpha_trace . mean () + beta_trace . mean () * xvals , 'g' , label = \"hierarchical\" ) scatter ( c_data . floor + np . random . randn ( len ( c_data )) * 0.01 , c_data . log_radon , alpha = 1 , color = 'k' , marker = '.' , s = 80 , label = 'original data' ) axis = plt . gca () axis . set_xticks ([ 0 , 1 ]) axis . set_xticklabels ([ 'basement' , 'no basement' ]) #axis.set_ylim(-0.1, 0.1) axis . set_title ( c ) if not i % 100 : axis . legend () axis . set_ylabel ( 'log radon level' ) #print test plot_predictions ([ unpooled_model , pooled_model , hierarchical_model ], train , county_data ,[ 'DAKOTA' , 'ST LOUIS' , 'CROW WING' , 'MARTIN' ]) <matplotlib.figure.Figure at 0x1404668c> $$\\\\[5pt]$$ Shrinkage Shrinkage describes the process by which our estimates are \"pulled\" towards the group-mean as a result of the common group distribution -- county-coefficients very far away from the group mean have very low probability under the normality assumption, moving them closer to the group mean gives them higher probability. In the non-hierachical model every county is allowed to differ completely from the others by just using each county's data, resulting in a model more prone to outliers (as shown above). $$\\\\[5pt]$$ Shrinkage plot In [45]: ''' Igor Marfin <Unister Gmb, 2014> igor.marfin@unister.de Plotting of the shrinkage effect ''' def plot_shrinkage ( models , data , aux_data ): ''' plots the shrinkage effect ''' sns . set_context ( \"poster\" ) plt . figure ( figsize = ( 15 , 8 )) fig = plt . figure ( figsize = ( 12 , 8 )) data_copy = data . copy () data_copy = pd . merge ( data_copy , aux_data , left_on = 'county' , right_on = 'county_names' , how = 'left' ) data_copy = data_copy . drop ( 'county_names' , 1 ) alpha_trace_all_unpooled = [] beta_trace_all_unpooled = [] alpha_trace_all_hierarchical = [] beta_trace_all_hierarchical = [] for model in models : if type ( model ) == ModelPooled : traces = model . prepare_trace () alpha_trace = traces [ \"alpha\" ] . values [ 500 :: 10 ] beta_trace = traces [ \"beta\" ] . values [ 500 :: 10 ] scatter ( alpha_trace . mean (), beta_trace . mean (), c = 'blue' , s = 136 , alpha = 1 , label = \"pooled\" ) elif type ( model ) == ModelUnPooled : traces = model . prepare_trace () for idx in aux_data [ 'county_idx' ] . values : alpha_trace = traces [ \"alpha_ %d \" % idx ].values[500::10] beta_trace = traces [ \"beta_ %d \" % idx ].values[500::10] alpha_trace_all_unpooled += [ alpha_trace . mean ()] beta_trace_all_unpooled += [ beta_trace . mean ()] if idx == len ( aux_data [ 'county_idx' ] . values ) - 1 : # scatter(alpha_trace_all_unpooled,beta_trace_all_unpooled, c='red', s=26, alpha=0.4, label = 'unpooled') scatter ( alpha_trace . mean (), beta_trace . mean (), c = 'red' , s = 26 , alpha = 0.4 , label = 'unpooled' ) else : # scatter(alpha_trace_all_unpooled,beta_trace_all_unpooled, c='red', s=26, alpha=0.4) scatter ( alpha_trace . mean (), beta_trace . mean (), c = 'red' , s = 26 , alpha = 0.4 ) elif type ( model ) == ModelHierarchical : traces = model . prepare_trace () for idx in aux_data [ 'county_idx' ] . values : alpha_trace = traces [ \"alpha_ %d \" % idx ].values[500::10] beta_trace = traces [ \"beta_ %d \" % idx ].values[500::10] alpha_trace_all_hierarchical += [ alpha_trace . mean ( axis = 0 )] beta_trace_all_hierarchical += [ beta_trace . mean ( axis = 0 )] if idx == len ( aux_data [ 'county_idx' ] . values ) - 1 : # scatter(alpha_trace_all_hierarchical,beta_trace_all_hierarchical, c='green', s=26, alpha=0.4, label = 'hierarchical') scatter ( alpha_trace . mean (), beta_trace . mean (), c = 'green' , s = 26 , alpha = 0.4 , label = 'hierarchical' ) else : # scatter(alpha_trace_all_hierarchical,beta_trace_all_hierarchical, c='green', s=26, alpha=0.4) scatter ( alpha_trace . mean (), beta_trace . mean (), c = 'green' , s = 26 , alpha = 0.4 ) for i in range ( len ( aux_data [ 'county_idx' ] . values )): arrow ( alpha_trace_all_unpooled [ i ], beta_trace_all_unpooled [ i ], alpha_trace_all_hierarchical [ i ] - alpha_trace_all_unpooled [ i ], beta_trace_all_hierarchical [ i ] - beta_trace_all_unpooled [ i ], fc = \"k\" , ec = \"k\" , length_includes_head = True , alpha = 0.5 , head_width =. 05 ) axis = plt . gca () axis . set_title ( \"shrinkage\" ) axis . legend () axis . set_xlabel ( 'Intercept' ) axis . set_ylabel ( 'Floor' ) plot_shrinkage ([ pooled_model , unpooled_model , hierarchical_model ], test , county_data ) <matplotlib.figure.Figure at 0x13bdba4c> To show the effect of shrinkage on a single coefficient-pair (alpha and beta) we connect the red and green points belonging to the same county by an arrow. Some non-hierarchical posteriors are so far out that we couldn't display them in this plot (it makes the axes too wide). $$\\\\[5pt]$$ Plotting of radon levels of different counties. In [121]: ''' Igor Marfin <Unister Gmb, 2014> igor.marfin@unister.de plotting support for discussed models ''' def plot_predicted_radon_level ( models , data , aux_data , selection ): ''' plots predictions ''' sns . set_context ( \"poster\" ) plt . figure ( figsize = ( 15 , 8 )) fig = plt . figure ( figsize = ( 12 , 8 )) data_copy = data . copy () data_copy = pd . merge ( data_copy , aux_data , left_on = 'county' , right_on = 'county_names' , how = 'left' ) data_copy = data_copy . drop ( 'county_names' , 1 ) fig . add_subplot ( '110' ) for i , c in enumerate ( selection ): c_data = data_copy . ix [ data_copy [ \"county\" ] == c ] c_data = c_data . reset_index ( drop = True ) idx = list ( c_data [ 'county_idx' ])[ 0 ] # contains county_idx for the selected county floors = c_data [ 'floor' ] . values for model in models : if type ( model ) == ModelPooled : traces = model . prepare_trace () alpha_trace = traces [ \"alpha\" ] . values [ 500 :: 10 ] beta_trace = traces [ \"beta\" ] . values [ 500 :: 10 ] estimated_radon_level = [] for j in range ( len ( alpha_trace )): estimated_radon_level += ( alpha_trace [ j ] + beta_trace [ j ] * floors ) . tolist () statistics = pd . DataFrame ( estimated_radon_level , columns = [ 'radon_level' ]) . describe () std = statistics [ 'radon_level' ][ 'std' ] mean = statistics [ 'radon_level' ][ 'mean' ] if ( i == 0 ): plot ([ idx , idx ],[ mean - std , mean + std ], 'b-' , label = \"pooled\" ) else : plot ([ idx , idx ],[ mean - std , mean + std ], 'b-' ) scatter ([ idx ],[ mean ], alpha = 1 , color = 'b' , marker = '.' , s = 800 ) elif type ( model ) == ModelUnPooled : traces = model . prepare_trace () alpha_trace = traces [ \"alpha_ %d \" % idx ].values[500::10] beta_trace = traces [ \"beta_ %d \" % idx ].values[500::10] estimated_radon_level = [] for j in range ( len ( alpha_trace )): estimated_radon_level += ( alpha_trace [ j ] + beta_trace [ j ] * floors ) . tolist () statistics = pd . DataFrame ( estimated_radon_level , columns = [ 'radon_level' ]) . describe () std = statistics [ 'radon_level' ][ 'std' ] mean = statistics [ 'radon_level' ][ 'mean' ] if ( i == 0 ): plot ([ idx , idx ],[ mean - std , mean + std ], 'r-' , label = \"unpooled\" ) else : plot ([ idx , idx ],[ mean - std , mean + std ], 'r-' ) scatter ([ idx ],[ mean ], alpha = 1 , color = 'r' , marker = '.' , s = 800 ) elif type ( model ) == ModelHierarchical : traces = model . prepare_trace () alpha_trace = traces [ \"alpha_ %d \" % idx ].values[500::10] beta_trace = traces [ \"beta_ %d \" % idx ].values[500::10] estimated_radon_level = [] for j in range ( len ( alpha_trace )): estimated_radon_level += ( alpha_trace [ j ] + beta_trace [ j ] * floors ) . tolist () statistics = pd . DataFrame ( estimated_radon_level , columns = [ 'radon_level' ]) . describe () std = statistics [ 'radon_level' ][ 'std' ] mean = statistics [ 'radon_level' ][ 'mean' ] if ( i == 0 ): plot ([ idx , idx ],[ mean - std , mean + std ], 'g-' , label = \"hierarchical\" ) else : plot ([ idx , idx ],[ mean - std , mean + std ], 'g-' ) scatter ([ idx ],[ mean ], alpha = 1 , color = 'g' , marker = '.' , s = 800 ) axis = plt . gca () axis . set_title ( \"Estimated radon level per county\" ) axis . legend () axis . set_xlabel ( 'county_idx' ) axis . set_ylabel ( 'Radon Level' ) plt . ylim ([ - 1 , 5 ]) plot_predicted_radon_level ([ pooled_model , unpooled_model , hierarchical_model ], train , county_data , county_data [ 'county_names' ] . values . tolist ()[ 0 :]) <matplotlib.figure.Figure at 0x13bf292c> $$\\\\[5pt]$$ Goodness of the Fit (GoF) The goodness of Fit (GOF) of a statistical model describes how well it fits into a set of observations. GOF indices sum- marize the discrepancy between the observed values and the values expected under a statistical model. GOF statis- tics are GOF indices with known sampling distributions, usually obtained using asymptotic methods, that are used in statistical hypothesis testing. The PyMC proposes a list of the GoF indices: The Geweke statisics is a time-series approach that compares the mean and variance of segments from the beginning and end of a single chain. 95% CI on parameters of the models Gelman-Rubin statistics Discrepency statistics $$\\\\[5pt]$$ The Geweke statisics The geweke statistics is defined as follows $$Z_n=\\frac{\\bar{\\theta}_A-\\bar{\\theta}_B}{\\sqrt{\\frac{1}{n_A}\\hat{S_{\\theta}&#94;A}(0)+\\frac{1}{n_B}\\hat{S_{\\theta}&#94;B}(0)}},$$ where A is the early interval and B the late interval. If the Z-scores (theoretically distributed as standard normal variates) of these two segments are similar, it can provide evidence for convergence. PyMC calculates Z-scores of the difference between various initial segments along the chain, and the last 50% of the remaining chain. If the chain has converged, the majority of points should fall within 2 standard deviations of zero. Here is the geweke statistics for the PooledModel . In [19]: ''' Igor Marfin <Unister Gmb, 2014> igor.marfin@unister.de Geweke statistics for the pooled model. ''' geweke_pooled = pm . geweke ( pooled_model . model ) pm . Matplot . geweke_plot ( geweke_pooled , path = 'plots/pooled_geweke_' ) We can also calculate the Geweke statistics for two other models: UnPooledModel and HierarchicalModel (we will plot only statistics build on the HierarchicalModel ). In [27]: ''' Igor Marfin <Unister Gmb, 2014> igor.marfin@unister.de Geweke statistics for the hierarchical model. ''' geweke_hierarchical = pm . geweke ( hierarchical_model . model ) pm . Matplot . geweke_plot ( geweke_hierarchical , path = 'plots/hierarchical_geweke_' ) As expected, there are several alphas and betas with larger (smaller) z-scores than $+2\\sigma(-2\\sigma)$. They correspond to counties with not enough data points to be good fitted. The betas z-scores show even the worser convergenge than alphas. $$\\\\[5pt]$$ 95% CI on parameters of the models and Gelman-Rubin diagnostic Another diagnostic provided by PyMC is the Gelman-Rubin statistics . This diagnostic uses multiple chains to check for lack of convergence, and is based on the notion that if multiple chains have converged, by definition they should appear very similar to one another; if not, one or more of the chains has failed to converge. The Gelman-Rubin diagnostic uses an analysis of variance approach to assessing convergence. That is, it calculates both the between-chain varaince (B) and within-chain varaince (W), and assesses whether they are different enough to worry about convergence. Assuming m chains, each of length n, quantities are calculated and merged in the estimated covariance matrix V: \\begin{equation} \\hat{V} = \\frac{n-1}{n}W + \\frac{1}{n}B, \\end{equation} Then, one needs to define some scalar metric for the distance between the covariance matrices $\\hat{V},W$. The authors propose \\begin{equation} \\hat{R} = \\max_a \\frac{a&#94;T\\hat{V}a}{a&#94;TWa} = \\frac{n-1}{n} + \\left(\\frac{m+1}{m}\\right)\\lambda_1, \\end{equation} where m is is the chain length. The equality contain $\\lambda_1$ being the largest positive eigenvalue of $W&#94;{−1}\\hat{V}/n$. This $\\hat{R}$ is called the potential scale reduction, since it is an estimate of the potential reduction in the scale of the model parameters as the number of simulations tends to infinity. In practice, we look for values of $\\hat{R}$ close to one (say, less than 1.1) to be confident that a particular estimand has converged. In practice, we need to run MCMC chains several times in order to calculate the Gelman-Rubin statistics. Lets's try to test PooledModel and HierarchicalModel . The PyMC function summary_plot plots not only Gelman-Rubin statistics but also the 95% CI on the parameters. In [31]: ''' Igor Marfin <Unister Gmb, 2014> igor.marfin@unister.de Gelman-Rubin statistics and 95% CI for the pooled/hierarchical models. ''' # How many different MCMC chains do we need for Gelman-Rubin statistics n_traces = 3 for i in range ( n_traces ): pooled_model . model . sample ( 100000 , 5000 , 20 ) hierarchical_model . model . sample ( 20000 , 5000 , 20 ) # create plots pm . Matplot . summary_plot ([ pooled_model . alpha , pooled_model . beta ], path = 'plots/pooled_gr_' , rhat = True ) [-----------------100%-----------------] 20000 of 20000 complete in 9.2 sec In [32]: pm . Matplot . summary_plot ([ hierarchical_model . alpha , hierarchical_model . beta ], path = 'plots/hierarchical_gr_' , rhat = True ) In [30]: # unpooled_model skipped from consideration #from pymc import Matplot, gelman_rubin #unpooled_variables=map(lambda x: unpooled_model.alphas[\"alpha_%d\"%x],county_data['county_idx'].values.tolist()[0:]) #unpooled_variables+=map(lambda x: unpooled_model.betas[\"beta_%d\"%x],county_data['county_idx'].values.tolist()[0:]) #Matplot.summary_plot(unpooled_variables,path='plots/',rhat=True) Rhat values obtained from the plots before, show the convergence to 1.0 and 1.2 for Poolded and Hierarchical models correspondingly. $$\\\\[5pt]$$ 95% CI on parameters of the models and Gelman-Rubin diagnostic One can use the simulated data, the posterior predictive values, to make a discrepancy plot. The discrepancy calculates the difference between data/simulation tot he expected values from the model. A discrepancy plot plots the simulated discrepancy against the observed discrepancy. These should bunch around the 45 degree line. The discrepancy is defined using the the Freeman-Tukey statistic: $$D(x|\\theta) = \\sum_j (\\sqrt{x_j}-\\sqrt{e_j})&#94;2,$$ where the $x_j$ are data and $e_j$ are the corresponding expected values, based on the model. The average difference between the simulated and observed discrepancy ought to be zero. The Bayesian p-value is the proportion of simulated discrepancies that are larger than their corresponding observed discrepancies: $$\\begin{split}p = Pr[ D(x_{\\text{sim}}|\\theta) > D(x_{\\text{obs}}|\\theta) ]\\end{split}.$$ If p is very large or very small the model is not consistent with the data. This is pointing to lack of fit. We try to calculate the Discrepancy statistics for Pooled Model first. To do this, we need to extend the interface of the model, introducing the methods to return expected values of the log_radon and to perform simulations. In [115]: ''' Igor Marfin <Unister Gmb, 2014> igor.marfin@unister.de get_expected() and make_simulations() methods for the pooled model. ''' def _get_expected ( self , burn_in ): ''' returns the traces of the expectations (which are deterministic quantities in PyMC) ''' burn_in = burn_in if len ( self . traces ) >= burn_in else 0 self . expected = {} for var in list ( self . model . deterministics ): # the reason of the these lines see below shape = self . model . trace ( str ( var ))[:] . shape b = np . array ( self . model . trace ( str ( var ))[ - 1 ] . tolist () * ( len ( self . traces ) - burn_in )) c = b . reshape ( len ( self . traces ) - burn_in , shape [ 1 ]) self . expected . update ( # version 1 #{str(var):self.model.trace(str(var))[burn_in:]} # version 2 # instead to use all traces of the deterministic variables, # we are going to use the value from the last MCMC point # plus the noise (noise is Gaussian with large sigma) { str ( var ): c + np . random . normal ( 0 , 100 ,( len ( self . traces ) - burn_in , shape [ 1 ])) } ) return self . expected def _make_simulations ( self , test_data , burn_in = 0 ): \"\"\" returns the simulation of the observed quantity \"\"\" traces = self . prepare_trace () burn_in = burn_in if len ( traces ) >= burn_in else 0 simulations = [] for cnt in range ( burn_in , len ( traces )): alpha = traces . loc [ cnt , \"alpha\" ] beta = traces . loc [ cnt , \"beta\" ] eps = traces . loc [ cnt , \"eps\" ] simulations += [ self . predict ( test_data , alpha , beta , eps )] return np . array ( simulations ) # Update the interface of the ModelPooled ModelPooled . get_expected = _get_expected ModelPooled . make_simulations = _make_simulations expected = pooled_model . get_expected ( burn_in = 4000 )[ 'log_radon' ] simulated = pooled_model . make_simulations ( train , burn_in = 4000 ) # print the calucation of the first two elements of Freeman-Tukey statistic #func_regularization=np.exp #func_regularization=np.abs func_regularization = np . vectorize ( lambda x : x if x > 0 else 0. ) # transform from log levels of the radon to normal levels of the radon expected = func_regularization ( expected ) simulated = func_regularization ( simulated ) data = func_regularization ( train [ \"log_radon\" ] . values ) print \"User defined Freeman-Tukey statistics: \" print sum (( np . sqrt ( data ) - np . sqrt ( expected [ 0 ])) ** 2 ) print sum (( np . sqrt ( simulated [ 0 ]) - np . sqrt ( expected [ 0 ])) ** 2 ) print sum (( np . sqrt ( data ) - np . sqrt ( expected [ 1 ])) ** 2 ) print sum (( np . sqrt ( simulated [ 1 ]) - np . sqrt ( expected [ 1 ])) ** 2 ) D = pm . discrepancy ( data , simulated , expected ) print \"PyMC defined Freeman-Tukey statistics: \" print D [ 0 ][ 0 : 2 ], D [ 1 ][ 0 : 2 ] pm . Matplot . discrepancy_plot ( D , name = 'D' , report_p = True , path = 'plots/pooled_discrepancy_' ) User defined Freeman-Tukey statistics: 25942.7449905 26311.8992629 29211.6401619 30017.0258718 Bayesian p-value: p=0.971 PyMC defined Freeman-Tukey statistics: [ 25942.74499048 29211.64016191] [ 26311.89926291 30017.02587182] Plotting D-gof In [108]: ''' Igor Marfin <Unister Gmb, 2014> igor.marfin@unister.de get_expected() and make_simulations() methods for the hierarchical model. ''' def _get_expected ( self , burn_in ): ''' returns the traces of the expectations (which are deterministic quantities in PyMC) ''' burn_in = burn_in if len ( self . traces ) >= burn_in else 0 self . expected = {} for var in list ( self . model . deterministics ): # the reason of theselines see below shape = self . model . trace ( str ( var ))[:] . shape b = np . array ( self . model . trace ( str ( var ))[ - 1 ] . tolist () * ( len ( self . traces ) - burn_in )) c = b . reshape ( len ( self . traces ) - burn_in , shape [ 1 ]) self . expected . update ( # version 1 #{str(var):self.model.trace(str(var))[burn_in:]} # version 2 # instead to use all traces of the deterministic variables, # we are going to use the value from the last MCMC point # plus the noise (noise is Gaussian with large sigma ) { str ( var ): c + np . random . normal ( 0 , 100 ,( len ( self . traces ) - burn_in , shape [ 1 ])) } ) return self . expected def _make_simulations ( self , test_data , aux_data , burn_in = 0 ): \"\"\" returns the simulation of the observed quantity \"\"\" traces = self . prepare_trace () burn_in = burn_in if len ( traces ) >= burn_in else 0 simulations = [] for cnt in range ( burn_in , len ( traces )): alphas = [] betas = [] for indx in range ( self . alpha . value . shape [ 0 ]): alphas += [ traces . loc [ cnt , \"alpha_ %d \" % indx ]] for indx in range ( self . beta . value . shape [ 0 ]): betas += [ traces . loc [ cnt , \"beta_ %d \" % indx ]] eps = traces . loc [ cnt , \"eps\" ] simulations += [ self . predict ( test_data , aux_data , alphas , betas , eps )] return np . array ( simulations ) # Update the interface of the ModelHierarchical ModelHierarchical . get_expected = _get_expected ModelHierarchical . make_simulations = _make_simulations expected = hierarchical_model . get_expected ( burn_in = 4000 )[ 'log_radon' ] simulated = hierarchical_model . make_simulations ( train , county_data , burn_in = 4000 ) # print the calucation of the first two elements of Freeman-Tukey statistic #func_regularization=np.exp #func_regularization=np.abs func_regularization = np . vectorize ( lambda x : x if x > 0 else 0. ) # transform from log levels of the radon to normal levels of the radon expected = func_regularization ( expected ) simulated = func_regularization ( simulated ) data = func_regularization ( train [ \"log_radon\" ] . values ) print \"User defined Freeman-Tukey statistics: \" print sum (( np . sqrt ( data ) - np . sqrt ( expected [ 0 ])) ** 2 ) print sum (( np . sqrt ( simulated [ 0 ]) - np . sqrt ( expected [ 0 ])) ** 2 ) print sum (( np . sqrt ( data ) - np . sqrt ( expected [ 1 ])) ** 2 ) print sum (( np . sqrt ( simulated [ 1 ]) - np . sqrt ( expected [ 1 ])) ** 2 ) D = pm . discrepancy ( data , simulated , expected ) print \"PyMC defined Freeman-Tukey statistics: \" print D [ 0 ][ 0 : 2 ], D [ 1 ][ 0 : 2 ] pm . Matplot . discrepancy_plot ( D , name = 'D' , report_p = True , path = 'plots/hierarchical_discrepancy_' ) User defined Freeman-Tukey statistics: 28831.5719335 30416.0211203 25476.3089558 26693.4656682 Bayesian p-value: p=0.987 PyMC defined Freeman-Tukey statistics: [ 28831.57193353 25476.30895579] [ 30416.02112029 26693.46566816] Plotting D-gof $$\\\\[5pt]$$ Optional matherial We can extend our partly-pooled model (or hierarchical model) $$radon_{i,c}=\\alpha_c+\\beta_c\\times floor_{i,c}+\\varepsilon_c, $$ to the model where $\\alpha_c$ is modelled by another regression model with a county-level covariate. In the blog [2] , Chris Fonnesbeck suggests to use the uranium level $u_c$, which is thought to be related to radon levels: $$\\alpha_c = \\gamma_0 + \\gamma_1 u_c + \\zeta_c,$$ where $$\\zeta_c \\sim N(0, \\sigma_{\\alpha}&#94;2).$$ Such model is called a model with group-level predictors $(u_c)$. Before we start writting our model, we need to scrape the needed data from Chris's repository, and make cleaning and grouping the obtained information. $$\\\\[5pt]$$ Getting and cleaning the data Let's download Chris's data on uranium levels in different states and counties. In [117]: ! mkdir data ! wget https://raw.githubusercontent.com/fonnesbeck/multilevel_modeling/master/data/cty.dat -O data/cty.dat ! wget https://raw.githubusercontent.com/fonnesbeck/multilevel_modeling/master/data/srrs2.dat -O data/srrs2.dat mkdir: cannot create directory `data': File exists --2015-08-10 15:03:17-- https://raw.githubusercontent.com/fonnesbeck/multilevel_modeling/master/data/cty.dat Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.31.17.133 Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.31.17.133|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 143270 (140K) [text/plain] Saving to: `data/cty.dat' 100%[======================================>] 143,270 556K/s in 0.3s 2015-08-10 15:03:18 (556 KB/s) - `data/cty.dat' saved [143270/143270] --2015-08-10 15:03:18-- https://raw.githubusercontent.com/fonnesbeck/multilevel_modeling/master/data/srrs2.dat Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.31.17.133 Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.31.17.133|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 1650106 (1.6M) [text/plain] Saving to: `data/srrs2.dat' 100%[======================================>] 1,650,106 699K/s in 2.3s 2015-08-10 15:03:22 (699 KB/s) - `data/srrs2.dat' saved [1650106/1650106] Now, let's combine them in one dataframe. All needed steps were taken from the Chris's blog [2] . In [139]: # get radon data and make a dataframe srrs2 = pd . read_csv ( 'data/srrs2.dat' ) srrs2 . columns = srrs2 . columns . map ( str . strip ) srrs_mn = srrs2 [ srrs2 . state == 'MN' ] srrs_mn [ 'fips' ] = srrs_mn . stfips * 1000 + srrs_mn . cntyfips # get urnaium data and make another dataframe cty = pd . read_csv ( 'data/cty.dat' ) cty_mn = cty [ cty . st == 'MN' ] cty_mn [ 'fips' ] = 1000 * cty_mn . stfips + cty_mn . ctfips # merge two dataframes in one srrs_mn = srrs_mn . merge ( cty_mn [[ 'fips' , 'Uppm' ]], on = 'fips' ) srrs_mn = srrs_mn . drop_duplicates ( subset = 'idnum' ) # add log values of the radon and uranium levels radon = srrs_mn . activity srrs_mn [ 'log_radon' ] = log_radon = np . log ( radon + 0.1 ) . values srrs_mn [ 'log_uranium' ] = u = np . log ( srrs_mn . Uppm ) # add county_code srrs_mn . county = srrs_mn . county . map ( str . strip ) mn_counties = srrs_mn . county . unique () county_lookup = dict ( zip ( mn_counties , range ( len ( mn_counties )))) srrs_mn [ 'county_code' ] = county = srrs_mn . county . replace ( county_lookup ) . values # remove uninteresting columns columns_of_interest = [ 'county' , 'log_radon' , 'floor' , 'log_uranium' , 'county_code' ] radon_data_extended = srrs_mn [ columns_of_interest ] print radon_data_extended . head ( 6 ) print radon_data_extended . tail ( 6 ) # make an auxiliary dataframe: county_data county_names = radon_data_extended . county . unique () county_idx = radon_data_extended [ 'county_code' ] . values n_counties = len ( radon_data_extended . county . unique ()) county_data = pd . DataFrame ({ \"county_names\" : county_names , \"county_idx\" : radon_data_extended [ 'county_code' ] . unique () }) print county_data . head ( 6 ) print county_data . tail ( 6 ) county log_radon floor log_uranium county_code 0 AITKIN 0.832909 1 -0.689048 0 1 AITKIN 0.832909 0 -0.689048 0 2 AITKIN 1.098612 0 -0.689048 0 3 AITKIN 0.095310 0 -0.689048 0 4 ANOKA 1.163151 0 -0.847313 1 5 ANOKA 0.955511 0 -0.847313 1 county log_radon floor log_uranium county_code 921 WRIGHT 2.261763 0 -0.090024 83 922 WRIGHT 1.871802 0 -0.090024 83 923 WRIGHT 1.526056 0 -0.090024 83 924 WRIGHT 1.629241 0 -0.090024 83 925 YELLOW MEDICINE 1.335001 0 0.355287 84 926 YELLOW MEDICINE 1.098612 0 0.355287 84 county_idx county_names 0 0 AITKIN 1 1 ANOKA 2 2 BECKER 3 3 BELTRAMI 4 4 BENTON 5 5 BIG STONE county_idx county_names 79 79 WASHINGTON 80 80 WATONWAN 81 81 WILKIN 82 82 WINONA 83 83 WRIGHT 84 84 YELLOW MEDICINE /usr/local/lib/python2.7/dist-packages/IPython/kernel/__main__.py:5: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy /usr/local/lib/python2.7/dist-packages/IPython/kernel/__main__.py:10: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy Now we define our ExtendedHierarchical model with uranium predictors. In [174]: ''' Igor Marfin <Unister Gmb, 2014> igor.marfin@unister.de A Model based on the partly-pooled data ''' #defines the class for the Modeling class ModelExtendedHierarchical ( BaseModel ): \"\"\" A ModelExtendedHierarchical class :param np.ndarray data: The data to use for learning the model. :param np.ndarray aux_data: The data to use for learning the model. \"\"\" def __init__ ( self , data , aux_data ): logging . info ( 'building the model....' ) super ( ModelExtendedHierarchical , self ) . __init__ ( data ) self . aux_data = aux_data # the data contain name of the county and its id self . county_idx = aux_data [ 'county_idx' ] . values alpha_value_init = np . mean ( data . loc [ data [ 'floor' ] == 0 , 'log_radon' ] . values ) beta_value_init = np . mean ( data . loc [ data [ 'floor' ] == 1 , 'log_radon' ] . values ) beta_value_init -= alpha_value_init # County uranium model for slope self . gamma_0 = pm . Normal ( 'gamma_0' , mu = 0. , tau = 0.0001 ) self . gamma_1 = pm . Normal ( 'gamma_1' , mu = 0. , tau = 0.0001 ) # alpha and beta priors self . sigma_alpha = pm . Uniform ( 'sigma_alpha' , lower = 0 , upper = 100 ) self . mu_beta = pm . Normal ( \"mu_beta\" , mu = 0. , #tau=100, tau = 100 **- 1 , value = beta_value_init ) self . sigma_beta = pm . Uniform ( 'sigma_beta' , lower = 0 , upper = 100 ) self . beta = pm . Normal ( \"beta\" , mu = self . mu_beta , tau = self . sigma_beta , size = len ( self . aux_data ), value = np . zeros ( len ( self . aux_data )) ) # error prior self . eps = pm . Uniform ( 'eps' , lower = 0 , upper = 100 ) # new dataframe self . data_update = self . data . copy () self . data_update = pd . merge ( self . data_update , aux_data , left_on = 'county' , right_on = 'county_names' , how = 'left' ) self . data_update = self . data_update . drop ( 'county_names' , 1 ) self . alpha_eps = pm . Normal ( \"alpha_eps\" , mu = 0 , tau = self . sigma_alpha , size = len ( self . aux_data ), value = np . zeros ( len ( self . aux_data )) ) # log of the radon level # Linear model of the log of the radon level @pm.deterministic def log_radon ( alpha_eps = self . alpha_eps , gamma0 = self . gamma_0 , gamma1 = self . gamma_1 , uranium = self . data_update [ \"log_uranium\" ] . values , beta = self . beta , county_idx = self . data_update [ \"county_idx\" ] . values , floor = self . data_update [ \"floor\" ] . values ): return gamma0 + gamma1 * uranium + alpha_eps [ county_idx ] + beta [ county_idx ] * floor # what we observed is self . radon_observed = pm . Normal ( 'radon_observed' , mu = log_radon , tau = self . eps , value = self . data [ \"log_radon\" ] . values , observed = True ) # our model collects all definitions in self.model self . model = pm . MCMC ([ self . gamma_0 , self . gamma_1 , self . alpha_eps , self . sigma_alpha , self . mu_beta , self . sigma_beta , self . beta , self . eps , log_radon , self . radon_observed ]) logging . info ( 'done building the model' ) def _prepare_trace ( self ): ''' prepare a dataframe with traces of the alpha,beta and eps parameters of our model ''' self . traces = { 'sigma_alpha' : self . sigma_alpha . trace (), 'mu_beta' : self . mu_beta . trace (), 'sigma_beta' : self . sigma_beta . trace (), 'eps' : self . eps . trace (), 'gamma_0' : self . gamma_0 . trace (), 'gamma_1' : self . gamma_1 . trace (), } alpha_eps = self . model . trace ( 'alpha_eps' ) . gettrace () betas = self . model . trace ( 'beta' ) . gettrace () for indx in range ( self . alpha_eps . value . shape [ 0 ]): self . traces . update ({ \"alpha_eps_ %d \" % indx :alpha_eps[:,indx]}) for indx in range ( self . beta . value . shape [ 0 ]): self . traces . update ({ \"beta_ %d \" % indx :betas[:,indx]}) self . traces = pd . DataFrame ( self . traces ) return self . traces # Update our Model ModelExtendedHierarchical . prepare_trace = _prepare_trace train , test = split_train_test ( radon_data_extended ) # fix of bug in the old sklearn train , test = pd . DataFrame ( train , columns = columns_of_interest ), pd . DataFrame ( test , columns = columns_of_interest ) train , test = train . convert_objects ( convert_numeric = True ), test . convert_objects ( convert_numeric = True ), extendedhierarchical_model = ModelExtendedHierarchical ( train , county_data ) extendedhierarchical_model . model . sample ( 20000 , 5000 , 20 ) INFO:root:building the model.... INFO:root:done building the model [-----------------100%-----------------] 20000 of 20000 complete in 12.9 sec Let's go further and define the predic method In [176]: def _predict ( self , test_data , aux_data , gamma0 = None , gamma1 = None , alpha_eps = None , betas = None , epses = None ): ''' predicts results on the test data ''' test_data_copy = test_data . copy () test_data_copy = pd . merge ( test_data_copy , aux_data , left_on = 'county' , right_on = 'county_names' , how = 'left' ) test_data_copy = test_data_copy . drop ( 'county_names' , 1 ) #if (alphas is None): alphas = self.alpha.value if ( gamma0 is None ): gamma0 = self . gamma_0 . value if ( gamma1 is None ): gamma1 = self . gamma_1 . value if ( alpha_eps is None ): alpha_eps = self . alpha_eps . value if ( betas is None ): betas = self . beta . value if ( epses is None ): epses = self . eps . value test_data_copy [ 'log_radon_predict' ] = test_data_copy . apply ( lambda x : np . random . normal ( gamma0 + gamma1 * x [ 'log_uranium' ] + alpha_eps [ x [ 'county_idx' ]] + betas [ x [ 'county_idx' ]] * x [ 'floor' ], epses ), axis = 1 ) self . predicted = test_data_copy [ 'log_radon_predict' ] . values return self . predicted # Update the interface of our model ModelExtendedHierarchical . predict = _predict In [177]: extendedhierarchical_model . predict ( train , county_data ) print \"RMSE on train data: \" , extendedhierarchical_model . rmse ( train [ \"log_radon\" ] . values ) extendedhierarchical_model . predict ( test , county_data ) print \"RMSE on test data: \" , extendedhierarchical_model . rmse ( test [ \"log_radon\" ] . values ) RMSE on train data: 2.1042219738 RMSE on test data: 2.13148613522 Finally, let's calculate running RMSE. In [178]: def _running_rmse ( self , test_data , train_data , aux_data , burn_in = 0 , plot = True ): \"\"\"Calculate RMSE for each step of the trace to monitor convergence. \"\"\" traces = self . prepare_trace () burn_in = burn_in if len ( traces ) >= burn_in else 0 results = { 'per-step-train' : [], 'running-train' : [], 'per-step-test' : [], 'running-test' : []} R_train = np . zeros ( len ( train_data )) R_test = np . zeros ( len ( test_data )) for cnt in range ( burn_in , len ( traces )): alpha_eps = [] betas = [] for indx in range ( self . alpha_eps . value . shape [ 0 ]): alpha_eps += [ traces . loc [ cnt , \"alpha_eps_ %d \" % indx ]] for indx in range ( self . beta . value . shape [ 0 ]): betas += [ traces . loc [ cnt , \"beta_ %d \" % indx ]] gamma0 = traces . loc [ cnt , \"gamma_0\" ] gamma1 = traces . loc [ cnt , \"gamma_1\" ] eps = traces . loc [ cnt , \"eps\" ] predicted_train = self . predict ( train_data , aux_data , gamma0 , gamma1 , alpha_eps , betas , eps ) predicted_test = self . predict ( test_data , aux_data , gamma0 , gamma1 , alpha_eps , betas , eps ) R_train += predicted_train R_test += predicted_test running_R_train = R_train / ( cnt + 1 ) running_R_test = R_test / ( cnt + 1 ) results [ 'per-step-train' ] . append ( rmse ( train_data [ \"log_radon\" ] . values , predicted_train )) results [ 'running-train' ] . append ( rmse ( train_data [ \"log_radon\" ] . values , running_R_train )) results [ 'per-step-test' ] . append ( rmse ( test_data [ \"log_radon\" ] . values , predicted_test )) results [ 'running-test' ] . append ( rmse ( test_data [ \"log_radon\" ] . values , running_R_test )) results = pd . DataFrame ( results ) if plot : results . plot ( kind = 'line' , grid = False , figsize = ( 15 , 7 ), title = 'Per-step and Running RMSE From Posterior Predictive' ) # Return the final predictions, and the RMSE calculations return running_R_train , running_R_test , results # Update the interface of ModelExtendedHierarchical ModelExtendedHierarchical . running_rmse = _running_rmse In [179]: _ , _ , results_extendedhierarchical = extendedhierarchical_model . running_rmse ( test , train , county_data , burn_in = 5000 ) In [180]: results_extendedhierarchical . tail ( 3 ) Out[180]: per-step-test per-step-train running-test running-train 747 2.334651 2.036358 0.808198 0.681845 748 2.454388 2.288047 0.808484 0.681869 749 2.353936 2.183702 0.808506 0.681677 After updating the plotting function, we can plot the predictions of the new model. In [199]: ''' Igor Marfin <Unister Gmb, 2014> igor.marfin@unister.de plotting support for discussed models ''' def plot_predictions ( models , data , aux_data , selection ): ''' plots predictions ''' sns . set_context ( \"poster\" ) plt . figure ( figsize = ( 15 , 8 )) fig = plt . figure ( figsize = ( 12 , 8 )) data_copy = data . copy () data_copy = pd . merge ( data_copy , aux_data , left_on = 'county' , right_on = 'county_names' , how = 'left' ) data_copy = data_copy . drop ( 'county_names' , 1 ) xvals = np . linspace ( - 0.2 , 1.2 ) # range on the x-axis for i , c in enumerate ( selection ): c_data = data_copy . ix [ data_copy [ \"county\" ] == c ] c_data = c_data . reset_index ( drop = True ) idx = list ( c_data [ 'county_idx' ])[ 0 ] # contains county_idx for the selected county fig . add_subplot ( str ( len ( selection )) + '1' + str ( i + 1 )) for model in models : if type ( model ) == ModelPooled : traces = model . prepare_trace () alpha_trace = traces [ \"alpha\" ] . values [ 500 :: 10 ] beta_trace = traces [ \"beta\" ] . values [ 500 :: 10 ] for j in range ( len ( alpha_trace )): plot ( xvals , alpha_trace [ j ] + beta_trace [ j ] * xvals , 'b' , alpha =. 1 ) plot ( xvals , alpha_trace . mean () + beta_trace . mean () * xvals , 'b' , label = \"pooled\" ) elif type ( model ) == ModelUnPooled : traces = model . prepare_trace () alpha_trace = traces [ \"alpha_ %d \" % idx ].values[500::10] beta_trace = traces [ \"beta_ %d \" % idx ].values[500::10] #alpha_trace=traces[\"alpha_%d\"%idx].values[50000::300] #beta_trace=traces[\"beta_%d\"%idx].values[50000::300] for j in range ( len ( alpha_trace )): plot ( xvals , alpha_trace [ j ] + beta_trace [ j ] * xvals , 'r' , alpha =. 1 ) plot ( xvals , alpha_trace . mean () + beta_trace . mean () * xvals , 'r' , label = \"unpooled\" ) elif type ( model ) == ModelHierarchical : traces = model . prepare_trace () alpha_trace = traces [ \"alpha_ %d \" % idx ].values[500::10] beta_trace = traces [ \"beta_ %d \" % idx ].values[500::10] for j in range ( len ( alpha_trace )): plot ( xvals , alpha_trace [ j ] + beta_trace [ j ] * xvals , 'g' , alpha =. 1 ) plot ( xvals , alpha_trace . mean () + beta_trace . mean () * xvals , 'g' , label = \"hierarchical\" ) elif type ( model ) == ModelExtendedHierarchical : traces = model . prepare_trace () gamma0_trace = traces [ 'gamma_0' ] . values [ 500 :: 10 ] gamma1_trace = traces [ 'gamma_1' ] . values [ 500 :: 10 ] alpha_eps_trace = traces [ \"alpha_eps_ %d \" % idx ].values[500::10] beta_trace = traces [ \"beta_ %d \" % idx ].values[500::10] uranium = list ( c_data [ 'log_uranium' ])[ 0 ] for j in range ( len ( beta_trace )): plot ( xvals , gamma0_trace [ j ] + gamma1_trace [ j ] * uranium + alpha_eps_trace [ j ] + beta_trace [ j ] * xvals , 'm' , alpha =. 1 ) plot ( xvals , gamma0_trace . mean () + gamma1_trace . mean () * uranium + alpha_eps_trace . mean () + beta_trace . mean () * xvals , 'm' , label = \"extendedhierarchical\" ) # plot the original data scatter ( c_data . floor + np . random . randn ( len ( c_data )) * 0.01 , c_data . log_radon , alpha = 1 , color = 'k' , marker = '.' , s = 80 , label = 'original data' ) axis = plt . gca () axis . set_xticks ([ 0 , 1 ]) axis . set_xticklabels ([ 'basement' , 'no basement' ]) #axis.set_ylim(-0.1, 0.1) axis . set_title ( c ) if not i % 100 : axis . legend () axis . set_ylabel ( 'log radon level' ) plot_predictions ([ unpooled_model , pooled_model , extendedhierarchical_model , hierarchical_model ], train , county_data ,[ 'DAKOTA' , 'ST LOUIS' , 'CROW WING' , 'MARTIN' ]) <matplotlib.figure.Figure at 0xa8e268ec> $$\\\\[5pt]$$ References [1]http://twiecki.github.io/blog/2014/03/17/bayesian-glms-3/ [2]A Primer on Bayesian Methods for Multilevel Modeling In [ ]: if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML'; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: 'center',\" + \" displayIndent: '0em',\" + \" showMathMenu: true,\" + \" tex2jax: { \" + \" inlineMath: [ ['$','$'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }\" + \" } \" + \"}); \"; (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Statistics","title":"Bayesian Hierarchical Linear Regression Model"}]}